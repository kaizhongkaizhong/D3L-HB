{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # 使用第1块GPU（从0开始）\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from math import *\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "from IPython import display\n",
    "import torch.utils.data as Data\n",
    "import torch.nn as nn\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "# %matplotlib notebook\n",
    "from matplotlib import cm\n",
    "from scipy.linalg import block_diag\n",
    "import datetime\n",
    "from torch.nn.utils import *\n",
    "from Network_FDD import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(Nc,N,Nt,B,Nr,L,SNR_dB,K,EPOCH,BATCH_SIZE):#N代表路径数，若N为-1则代表\n",
    "    shoulian = np.zeros(EPOCH)\n",
    "    snr =  10**(SNR_dB/10)/K\n",
    "    parm_set = [Nc,Nt,Nr,snr,B,K]\n",
    "        \n",
    "    H_train_1 = torch.load('data/H_train_UPA'+str(N)+'Lp_1.pt')\n",
    "    H_train_1 = H_train_1[:,0:K,:,:]\n",
    "    H_train_2 = torch.load('data/H_train_UPA'+str(N)+'Lp_2.pt')\n",
    "    H_train_2 = H_train_2[:,0:K,:,:]\n",
    "    H_train = torch.cat([H_train_1,H_train_2],0)\n",
    "    H_train_1 = 0\n",
    "    H_train_2 = 0\n",
    "    print(H_train.shape)\n",
    "    H_test = torch.load('data/H_test_UPA'+str(N)+'Lp.pt')\n",
    "    H_test = H_test[:,0:K,:,:]\n",
    "    \n",
    "    net_US = DNN_US_RF_OFDM(parm_set).cuda()\n",
    "    net_BS = DNN_BS_hyb_OFDM(parm_set).cuda()\n",
    "\n",
    "    print(net_BS)\n",
    "    print(net_US)\n",
    "    optimizer_US = torch.optim.Adam(net_US.parameters(),lr=0.001)\n",
    "    scheduler_US = torch.optim.lr_scheduler.MultiStepLR(optimizer_US,  milestones = [100,150], gamma = 0.6, last_epoch=-1)   \n",
    "\n",
    "    optimizer_BS = torch.optim.Adam(net_BS.parameters(),lr=0.001)\n",
    "    scheduler_BS = torch.optim.lr_scheduler.MultiStepLR(optimizer_BS,  milestones = [100,150], gamma = 0.6, last_epoch=-1)\n",
    "    \n",
    "    loss_func1 = MyLoss_OFDM()\n",
    "    loss_func1 = loss_func1.cuda()\n",
    "\n",
    "    \n",
    "    best_SE = 0\n",
    "    torch_dataset_train = Data.TensorDataset(H_train)\n",
    "    loader_train = Data.DataLoader(\n",
    "        dataset=torch_dataset_train,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,#训练时随机打乱数据\n",
    "        num_workers=0,#每次用两个进程提取\n",
    "    )\n",
    "    \n",
    "    torch_dataset_test = Data.TensorDataset(H_test)\n",
    "    loader_test = Data.DataLoader(\n",
    "        dataset=torch_dataset_test,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,#训练时随机打乱数据\n",
    "        num_workers=0,#每次用两个进程提取\n",
    "    )\n",
    "    start = datetime.datetime.now()\n",
    "    for epoch in range(EPOCH):\n",
    "        train_SE  = 0\n",
    "        num_train = 0\n",
    "        test_SE   = 0\n",
    "        num_test  = 0\n",
    "        for step,[b_x] in enumerate(loader_train):\n",
    "            num_train = num_train + 1\n",
    "            net_US.train() \n",
    "            net_BS.train() #训练模式\n",
    "            b_x = b_x.cuda()\n",
    "            num = b_x.shape[0]\n",
    "            out1 = torch.zeros([num,B*K])\n",
    "            out1 = out1.cuda()\n",
    "            for i in range(K):\n",
    "                out1k = net_US(b_x[:,i,:,:],parm_set)\n",
    "                out1[:,i*B:(i*B+B)] = out1k\n",
    "            out2 = net_BS(out1,parm_set)\n",
    "            loss = loss_func1(b_x,out2,parm_set)\n",
    "            train_SE = train_SE - loss\n",
    "            optimizer_US.zero_grad()\n",
    "            optimizer_BS.zero_grad()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer_US.step()\n",
    "            optimizer_BS.step()\n",
    "        train_SE = train_SE/num_train\n",
    "        scheduler_US.step()\n",
    "        scheduler_BS.step()\n",
    "\n",
    "        net_US.eval() \n",
    "        net_BS.eval() \n",
    "        with torch.no_grad():\n",
    "            for step,[b_x] in enumerate(loader_test):\n",
    "                num_test = num_test + 1\n",
    "                net_US.eval() \n",
    "                net_BS.eval() #验证模式\n",
    "                b_x = b_x.cuda()\n",
    "                num = b_x.shape[0]\n",
    "                out1 = torch.zeros([num,B*K])\n",
    "                out1 = out1.cuda()\n",
    "                for i in range(K):\n",
    "                    out1k = net_US(b_x[:,i,:,:],parm_set)\n",
    "                    out1[:,i*B:(i*B+B)] = out1k\n",
    "                out2 = net_BS(out1,parm_set)\n",
    "                loss = loss_func1(b_x,out2,parm_set)\n",
    "                test_SE = test_SE - loss\n",
    "            test_SE = test_SE/num_test\n",
    "\n",
    "        time0 =  datetime.datetime.now()-start\n",
    "        print('Epoch:',epoch,'time',time0,'train SE %.3f' % train_SE.cpu(),'test SE %.3f' % test_SE.cpu()) \n",
    "        start = datetime.datetime.now()\n",
    "\n",
    "        if test_SE > best_SE:\n",
    "            best_SE = test_SE\n",
    "            torch.save(net_US, './net_US_OFDM_'+str(B)+'B'+str(N)+'Lp'+str(L)+'L'+str(K)+'K'+'_8L_UPA.pth')\n",
    "            torch.save(net_BS, './net_BS_OFDM_'+str(B)+'B'+str(N)+'Lp'+str(L)+'L'+str(K)+'K'+'_8L_UPA.pth')   \n",
    "            print('Model saved!')\n",
    "        shoulian[epoch] = test_SE.cpu()\n",
    "    print('The best SE is: %.3f' % best_SE.cpu())\n",
    "    print(shoulian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(Nc,N_net,N_data,Nt,B,Nr,L,SNR_dB,K,BATCH_SIZE):\n",
    "    #N_net表示网络训练时的路径数，-1代表随即路径 N_data代表测试时的路径\n",
    "    snr =  10**(SNR_dB/10)/K\n",
    "    parm_set = [Nc,Nt,Nr,snr,B,K]\n",
    "    \n",
    "    \n",
    "    H_test = torch.load('data/H_test_UPA'+str(N_data)+'Lp.pt')\n",
    "    H_test = H_test[:,0:K,:,:]\n",
    "    \n",
    "    net_US = DNN_US_RF_OFDM(parm_set).cuda()\n",
    "    net_BS = DNN_BS_hyb_OFDM(parm_set).cuda()\n",
    "    net_US = torch.load('./net_US_OFDM_'+str(B)+'B'+str(N_net)+'Lp'+str(L)+'L'+str(K)+'K'+'_8L_UPA.pth')\n",
    "    net_BS = torch.load('./net_BS_OFDM_'+str(B)+'B'+str(N_net)+'Lp'+str(L)+'L'+str(K)+'K'+'_8L_UPA.pth')\n",
    "#     print(net_BS)\n",
    "#     print(net_US)\n",
    "    \n",
    "    loss_func1 = MyLoss_OFDM()\n",
    "    loss_func1 = loss_func1.cuda()\n",
    "\n",
    "\n",
    "    \n",
    "    torch_dataset_test = Data.TensorDataset(H_test)\n",
    "    loader_test = Data.DataLoader(\n",
    "        dataset=torch_dataset_test,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,#训练时随机打乱数据\n",
    "        num_workers=0,#每次用两个进程提取\n",
    "    )\n",
    "    num_test = 0\n",
    "    with torch.no_grad():\n",
    "        test_SE = 0\n",
    "        for step,[b_x] in enumerate(loader_test):\n",
    "            num_test = num_test + 1\n",
    "            net_US.eval() \n",
    "            net_BS.eval() #验证模式\n",
    "            b_x = b_x.cuda()\n",
    "            num = b_x.shape[0]\n",
    "            out1 = torch.zeros([num,B*K])\n",
    "            out1 = out1.cuda()\n",
    "            for i in range(K):\n",
    "                out1k = net_US(b_x[:,i,:,:],parm_set)\n",
    "                out1[:,i*B:(i*B+B)] = out1k\n",
    "            out2 = net_BS(out1,parm_set)\n",
    "            loss = loss_func1(b_x,out2,parm_set)\n",
    "            test_SE = test_SE - loss\n",
    "        test_SE = test_SE/num_test\n",
    "    print('test SE %.3f' % test_SE.cpu())\n",
    "    return test_SE.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L=8, B=8...128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nc = 32 #number of subcarriers\n",
    "N = 2   # Number of paths\n",
    "Nt = 64 # Number of Antennas at the BS\n",
    "Nr = 1  # Number of Antennas at the UE\n",
    "# B = 30\n",
    "\n",
    "L = 8   # number of pilot OFDM symbols\n",
    "SNR_dB = 10  # SNR\n",
    "K = 2   # number of UEs\n",
    "snr =  10**(SNR_dB/10)/K\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "EPOCH = 180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([204800, 2, 32, 128])\n",
      "DNN_BS_hyb_OFDM(\n",
      "  (DQL): DequantizationLayer()\n",
      "  (FC1): Linear(in_features=2, out_features=2048, bias=True)\n",
      "  (bn1): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (mish1): Mish()\n",
      "  (FC2): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "  (bn2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (mish2): Mish()\n",
      "  (FC3): Linear(in_features=1024, out_features=384, bias=True)\n",
      "  (bn3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (mish3): Mish()\n",
      "  (res1): RES_BLOCK(\n",
      "    (conv1): Sequential(\n",
      "      (0): Conv2d(8, 256, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0))\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Mish()\n",
      "    )\n",
      "    (conv2): Sequential(\n",
      "      (0): Conv2d(256, 512, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0))\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Mish()\n",
      "    )\n",
      "    (conv3): Sequential(\n",
      "      (0): Conv2d(512, 8, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0))\n",
      "      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (conv): Conv2d(8, 8, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0))\n",
      ")\n",
      "DNN_US_RF_OFDM(\n",
      "  (pilot): Linear(in_features=64, out_features=8, bias=False)\n",
      "  (res): RES_BLOCK(\n",
      "    (conv1): Sequential(\n",
      "      (0): Conv2d(16, 256, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0))\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Mish()\n",
      "    )\n",
      "    (conv2): Sequential(\n",
      "      (0): Conv2d(256, 512, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0))\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Mish()\n",
      "    )\n",
      "    (conv3): Sequential(\n",
      "      (0): Conv2d(512, 16, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0))\n",
      "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (FC2): Linear(in_features=512, out_features=1024, bias=True)\n",
      "  (bn2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu2): ReLU()\n",
      "  (FC3): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (bn3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu3): ReLU()\n",
      "  (FC4): Linear(in_features=512, out_features=1, bias=True)\n",
      "  (bn4): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (QL): QuantizationLayer()\n",
      ")\n",
      "Epoch: 0 time 0:01:13.563223 train SE 3.826 test SE 3.782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:256: UserWarning: Couldn't retrieve source code for container of type DNN_US_RF_OFDM. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:256: UserWarning: Couldn't retrieve source code for container of type RES_BLOCK. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:256: UserWarning: Couldn't retrieve source code for container of type Mish. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:256: UserWarning: Couldn't retrieve source code for container of type QuantizationLayer. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:256: UserWarning: Couldn't retrieve source code for container of type DNN_BS_hyb_OFDM. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:256: UserWarning: Couldn't retrieve source code for container of type DequantizationLayer. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved!\n",
      "Epoch: 1 time 0:01:13.319429 train SE 4.189 test SE 3.785\n",
      "Model saved!\n",
      "Epoch: 2 time 0:01:12.986178 train SE 4.070 test SE 4.605\n",
      "Model saved!\n",
      "Epoch: 3 time 0:01:11.549747 train SE 4.339 test SE 4.435\n",
      "Epoch: 4 time 0:01:06.390784 train SE 4.203 test SE 4.119\n",
      "Epoch: 5 time 0:01:08.970144 train SE 4.267 test SE 4.311\n",
      "Epoch: 6 time 0:01:07.061429 train SE 4.100 test SE 4.282\n",
      "Epoch: 7 time 0:01:09.845692 train SE 4.300 test SE 4.200\n",
      "Epoch: 8 time 0:01:13.154563 train SE 3.995 test SE 3.910\n",
      "Epoch: 9 time 0:01:10.319057 train SE 4.202 test SE 4.682\n",
      "Model saved!\n",
      "Epoch: 10 time 0:01:22.018251 train SE 4.677 test SE 4.632\n",
      "Epoch: 11 time 0:01:23.688065 train SE 4.036 test SE 3.876\n",
      "Epoch: 12 time 0:01:23.708071 train SE 4.443 test SE 4.665\n",
      "Epoch: 13 time 0:01:23.957963 train SE 4.630 test SE 4.639\n",
      "Epoch: 14 time 0:01:24.631750 train SE 4.439 test SE 4.503\n",
      "Epoch: 15 time 0:01:22.460855 train SE 4.571 test SE 4.580\n",
      "Epoch: 16 time 0:01:24.745730 train SE 4.533 test SE 4.517\n",
      "Epoch: 17 time 0:01:24.851547 train SE 4.482 test SE 4.484\n",
      "Epoch: 18 time 0:01:24.265041 train SE 4.448 test SE 4.528\n",
      "Epoch: 19 time 0:01:24.575648 train SE 4.542 test SE 4.539\n",
      "Epoch: 20 time 0:01:23.368845 train SE 4.550 test SE 4.391\n",
      "Epoch: 21 time 0:01:23.476129 train SE 4.530 test SE 4.575\n",
      "Epoch: 22 time 0:01:23.729392 train SE 4.493 test SE 3.976\n",
      "Epoch: 23 time 0:01:23.722407 train SE 4.251 test SE 4.419\n",
      "Epoch: 24 time 0:01:23.590764 train SE 4.401 test SE 4.322\n",
      "Epoch: 25 time 0:01:23.993273 train SE 4.484 test SE 4.275\n",
      "Epoch: 26 time 0:01:23.500935 train SE 4.513 test SE 4.382\n",
      "Epoch: 27 time 0:01:23.255359 train SE 4.506 test SE 4.246\n",
      "Epoch: 28 time 0:01:23.139019 train SE 4.531 test SE 4.182\n",
      "Epoch: 29 time 0:01:23.124000 train SE 4.555 test SE 4.261\n",
      "Epoch: 30 time 0:01:23.256118 train SE 4.516 test SE 4.200\n",
      "Epoch: 31 time 0:01:23.811169 train SE 4.531 test SE 4.384\n",
      "Epoch: 32 time 0:01:23.779398 train SE 4.527 test SE 4.210\n",
      "Epoch: 33 time 0:01:23.535380 train SE 4.524 test SE 4.268\n",
      "Epoch: 34 time 0:01:23.118045 train SE 4.566 test SE 4.191\n",
      "Epoch: 35 time 0:01:23.270353 train SE 4.537 test SE 4.370\n",
      "Epoch: 36 time 0:01:22.689212 train SE 4.541 test SE 4.230\n",
      "Epoch: 37 time 0:01:22.231872 train SE 4.561 test SE 4.300\n",
      "Epoch: 38 time 0:01:23.579416 train SE 4.566 test SE 4.277\n",
      "Epoch: 39 time 0:01:24.296348 train SE 4.585 test SE 4.347\n",
      "Epoch: 40 time 0:01:24.275957 train SE 4.553 test SE 4.212\n",
      "Epoch: 41 time 0:01:23.166893 train SE 4.539 test SE 4.254\n",
      "Epoch: 42 time 0:01:23.209247 train SE 4.526 test SE 4.160\n",
      "Epoch: 43 time 0:01:23.377283 train SE 4.565 test SE 4.228\n",
      "Epoch: 44 time 0:01:23.345878 train SE 4.581 test SE 4.299\n",
      "Epoch: 45 time 0:01:23.837151 train SE 4.596 test SE 4.211\n",
      "Epoch: 46 time 0:01:24.027690 train SE 4.602 test SE 4.199\n",
      "Epoch: 47 time 0:01:23.903940 train SE 4.605 test SE 4.518\n",
      "Epoch: 48 time 0:01:23.012923 train SE 4.604 test SE 4.178\n",
      "Epoch: 49 time 0:01:23.690187 train SE 4.598 test SE 4.198\n",
      "Epoch: 50 time 0:01:23.766006 train SE 4.594 test SE 4.238\n",
      "Epoch: 51 time 0:01:23.764306 train SE 4.596 test SE 4.222\n",
      "Epoch: 52 time 0:01:25.387063 train SE 4.518 test SE 4.112\n",
      "Epoch: 53 time 0:01:30.455558 train SE 4.470 test SE 4.128\n",
      "Epoch: 54 time 0:01:24.122290 train SE 4.553 test SE 4.145\n",
      "Epoch: 55 time 0:01:25.025435 train SE 4.540 test SE 4.255\n",
      "Epoch: 56 time 0:01:24.741006 train SE 4.508 test SE 4.246\n",
      "Epoch: 57 time 0:01:22.646760 train SE 4.554 test SE 4.223\n",
      "Epoch: 58 time 0:01:24.571779 train SE 4.553 test SE 4.202\n",
      "Epoch: 59 time 0:01:24.122331 train SE 4.545 test SE 4.108\n",
      "Epoch: 60 time 0:01:24.164822 train SE 4.529 test SE 4.110\n",
      "Epoch: 61 time 0:01:24.073926 train SE 4.460 test SE 4.121\n",
      "Epoch: 62 time 0:01:24.461494 train SE 4.490 test SE 4.097\n",
      "Epoch: 63 time 0:01:24.085511 train SE 4.449 test SE 4.038\n",
      "Epoch: 64 time 0:01:23.016329 train SE 4.443 test SE 4.128\n",
      "Epoch: 65 time 0:01:23.498479 train SE 4.457 test SE 4.216\n",
      "Epoch: 66 time 0:01:23.154037 train SE 4.480 test SE 4.033\n",
      "Epoch: 67 time 0:01:23.728967 train SE 4.380 test SE 3.883\n",
      "Epoch: 68 time 0:01:24.703340 train SE 4.405 test SE 3.846\n",
      "Epoch: 69 time 0:01:27.534250 train SE 4.428 test SE 3.858\n",
      "Epoch: 70 time 0:01:23.000962 train SE 4.453 test SE 4.025\n",
      "Epoch: 71 time 0:01:22.807908 train SE 4.411 test SE 4.386\n",
      "Epoch: 72 time 0:01:20.055703 train SE 4.400 test SE 4.119\n",
      "Epoch: 73 time 0:01:22.228871 train SE 4.462 test SE 4.067\n",
      "Epoch: 74 time 0:01:22.437338 train SE 4.435 test SE 4.106\n",
      "Epoch: 75 time 0:01:23.056785 train SE 4.451 test SE 3.932\n",
      "Epoch: 76 time 0:01:22.822766 train SE 4.459 test SE 3.946\n",
      "Epoch: 77 time 0:01:22.564035 train SE 4.425 test SE 4.001\n",
      "Epoch: 78 time 0:01:22.911804 train SE 4.437 test SE 3.964\n",
      "Epoch: 79 time 0:01:23.115572 train SE 4.451 test SE 4.115\n",
      "Epoch: 80 time 0:01:23.115055 train SE 4.417 test SE 4.124\n",
      "Epoch: 81 time 0:01:22.827874 train SE 4.421 test SE 4.169\n",
      "Epoch: 82 time 0:01:23.059700 train SE 4.392 test SE 4.014\n",
      "Epoch: 83 time 0:01:23.576257 train SE 4.408 test SE 4.043\n",
      "Epoch: 84 time 0:01:23.251691 train SE 4.312 test SE 3.908\n",
      "Epoch: 85 time 0:01:23.363895 train SE 4.315 test SE 4.110\n",
      "Epoch: 86 time 0:01:23.591768 train SE 4.342 test SE 3.939\n",
      "Epoch: 87 time 0:01:23.400209 train SE 4.313 test SE 3.844\n",
      "Epoch: 88 time 0:01:23.257304 train SE 4.345 test SE 3.869\n",
      "Epoch: 89 time 0:01:23.727421 train SE 4.364 test SE 3.825\n",
      "Epoch: 90 time 0:01:23.443686 train SE 4.351 test SE 3.908\n",
      "Epoch: 91 time 0:01:23.292636 train SE 4.365 test SE 3.848\n",
      "Epoch: 92 time 0:01:23.293676 train SE 4.347 test SE 4.082\n",
      "Epoch: 93 time 0:01:24.037752 train SE 4.358 test SE 4.069\n",
      "Epoch: 94 time 0:01:23.058836 train SE 4.363 test SE 3.906\n",
      "Epoch: 95 time 0:01:23.139899 train SE 4.389 test SE 3.907\n",
      "Epoch: 96 time 0:01:22.996346 train SE 4.378 test SE 3.898\n",
      "Epoch: 97 time 0:01:23.128458 train SE 4.394 test SE 3.940\n",
      "Epoch: 98 time 0:01:22.898640 train SE 4.397 test SE 3.966\n",
      "Epoch: 99 time 0:01:23.520996 train SE 4.394 test SE 4.003\n",
      "Epoch: 100 time 0:01:23.698563 train SE 4.383 test SE 3.930\n",
      "Epoch: 101 time 0:01:22.878772 train SE 4.388 test SE 3.827\n",
      "Epoch: 102 time 0:01:22.833799 train SE 4.395 test SE 4.062\n",
      "Epoch: 103 time 0:01:23.025275 train SE 4.393 test SE 3.949\n",
      "Epoch: 104 time 0:01:22.495226 train SE 4.391 test SE 3.902\n",
      "Epoch: 105 time 0:01:23.211748 train SE 4.400 test SE 4.046\n",
      "Epoch: 106 time 0:01:24.437018 train SE 4.407 test SE 4.013\n",
      "Epoch: 107 time 0:01:28.137104 train SE 4.405 test SE 3.686\n",
      "Epoch: 108 time 0:01:23.499531 train SE 4.378 test SE 3.872\n",
      "Epoch: 109 time 0:01:22.604867 train SE 4.399 test SE 4.072\n",
      "Epoch: 110 time 0:01:22.343619 train SE 4.385 test SE 3.968\n",
      "Epoch: 111 time 0:01:22.586581 train SE 4.376 test SE 3.838\n",
      "Epoch: 112 time 0:01:22.439949 train SE 4.358 test SE 3.687\n",
      "Epoch: 113 time 0:01:22.953561 train SE 4.372 test SE 3.870\n",
      "Epoch: 114 time 0:01:22.722269 train SE 4.381 test SE 3.821\n",
      "Epoch: 115 time 0:01:23.578278 train SE 4.408 test SE 3.905\n",
      "Epoch: 116 time 0:01:28.652331 train SE 4.389 test SE 3.748\n",
      "Epoch: 117 time 0:01:27.718802 train SE 4.399 test SE 3.882\n",
      "Epoch: 118 time 0:01:27.117473 train SE 4.392 test SE 3.759\n",
      "Epoch: 119 time 0:01:27.029012 train SE 4.393 test SE 3.911\n",
      "Epoch: 120 time 0:01:26.476557 train SE 4.387 test SE 3.875\n",
      "Epoch: 121 time 0:01:26.007559 train SE 4.410 test SE 3.681\n",
      "Epoch: 122 time 0:01:25.625356 train SE 4.427 test SE 3.890\n",
      "Epoch: 123 time 0:01:22.687704 train SE 4.399 test SE 3.748\n",
      "Epoch: 124 time 0:01:24.832035 train SE 4.380 test SE 3.719\n",
      "Epoch: 125 time 0:01:25.336606 train SE 4.401 test SE 3.990\n",
      "Epoch: 126 time 0:01:25.049332 train SE 4.405 test SE 3.844\n",
      "Epoch: 127 time 0:01:24.451459 train SE 4.402 test SE 3.651\n",
      "Epoch: 128 time 0:01:24.238563 train SE 4.393 test SE 3.678\n",
      "Epoch: 129 time 0:01:24.632462 train SE 4.408 test SE 3.817\n",
      "Epoch: 130 time 0:01:23.925266 train SE 4.403 test SE 3.968\n",
      "Epoch: 131 time 0:01:24.316971 train SE 4.398 test SE 3.905\n",
      "Epoch: 132 time 0:01:23.483596 train SE 4.414 test SE 3.949\n",
      "Epoch: 133 time 0:01:24.807577 train SE 4.405 test SE 3.861\n",
      "Epoch: 134 time 0:01:22.627822 train SE 4.401 test SE 3.819\n",
      "Epoch: 135 time 0:01:24.883443 train SE 4.383 test SE 4.039\n",
      "Epoch: 136 time 0:01:25.120333 train SE 4.388 test SE 3.651\n",
      "Epoch: 137 time 0:01:23.279012 train SE 4.402 test SE 3.927\n",
      "Epoch: 138 time 0:01:24.260094 train SE 4.423 test SE 3.913\n",
      "Epoch: 139 time 0:01:25.291688 train SE 4.424 test SE 4.055\n",
      "Epoch: 140 time 0:01:23.565735 train SE 4.395 test SE 3.770\n",
      "Epoch: 141 time 0:01:22.731034 train SE 4.429 test SE 3.815\n",
      "Epoch: 142 time 0:01:22.713218 train SE 4.422 test SE 3.914\n",
      "Epoch: 143 time 0:01:25.777035 train SE 4.417 test SE 3.839\n",
      "Epoch: 144 time 0:01:22.138088 train SE 4.421 test SE 3.916\n",
      "Epoch: 145 time 0:01:23.266723 train SE 4.411 test SE 3.849\n",
      "Epoch: 146 time 0:01:22.564024 train SE 4.429 test SE 4.109\n",
      "Epoch: 147 time 0:01:22.821800 train SE 4.408 test SE 3.757\n",
      "Epoch: 148 time 0:01:22.719160 train SE 4.390 test SE 3.919\n",
      "Epoch: 149 time 0:01:22.744617 train SE 4.401 test SE 3.629\n",
      "Epoch: 150 time 0:01:22.852821 train SE 4.413 test SE 3.731\n",
      "Epoch: 151 time 0:01:22.388999 train SE 4.423 test SE 3.969\n",
      "Epoch: 152 time 0:01:23.335059 train SE 4.417 test SE 3.783\n",
      "Epoch: 153 time 0:01:23.619725 train SE 4.431 test SE 3.834\n",
      "Epoch: 154 time 0:01:23.899999 train SE 4.445 test SE 3.731\n",
      "Epoch: 155 time 0:01:24.002167 train SE 4.428 test SE 3.711\n",
      "Epoch: 156 time 0:01:22.690282 train SE 4.422 test SE 3.995\n",
      "Epoch: 157 time 0:01:23.326899 train SE 4.410 test SE 3.971\n",
      "Epoch: 158 time 0:01:22.938459 train SE 4.417 test SE 4.102\n",
      "Epoch: 159 time 0:01:23.390214 train SE 4.411 test SE 4.007\n",
      "Epoch: 160 time 0:01:22.854789 train SE 4.410 test SE 3.823\n",
      "Epoch: 161 time 0:01:23.251737 train SE 4.412 test SE 3.711\n",
      "Epoch: 162 time 0:01:23.426107 train SE 4.403 test SE 3.968\n",
      "Epoch: 163 time 0:01:23.094650 train SE 4.421 test SE 4.023\n",
      "Epoch: 164 time 0:01:22.792671 train SE 4.413 test SE 4.051\n",
      "Epoch: 165 time 0:01:22.644286 train SE 4.424 test SE 3.836\n",
      "Epoch: 166 time 0:01:25.666384 train SE 4.435 test SE 3.846\n",
      "Epoch: 167 time 0:01:23.237728 train SE 4.429 test SE 3.739\n",
      "Epoch: 168 time 0:01:24.125424 train SE 4.433 test SE 3.815\n",
      "Epoch: 169 time 0:01:23.344295 train SE 4.422 test SE 4.154\n",
      "Epoch: 170 time 0:01:23.236735 train SE 4.433 test SE 4.019\n",
      "Epoch: 171 time 0:01:22.815931 train SE 4.433 test SE 3.750\n",
      "Epoch: 172 time 0:01:22.845224 train SE 4.417 test SE 3.915\n",
      "Epoch: 173 time 0:01:23.161838 train SE 4.424 test SE 3.804\n",
      "Epoch: 174 time 0:01:23.114109 train SE 4.420 test SE 3.756\n",
      "Epoch: 175 time 0:01:23.343483 train SE 4.416 test SE 3.900\n",
      "Epoch: 176 time 0:01:22.983857 train SE 4.404 test SE 3.788\n",
      "Epoch: 177 time 0:01:23.301621 train SE 4.421 test SE 3.923\n",
      "Epoch: 178 time 0:01:23.365949 train SE 4.424 test SE 3.991\n",
      "Epoch: 179 time 0:01:23.812789 train SE 4.432 test SE 3.808\n",
      "The best SE is: 4.682\n",
      "[3.78184104 3.78518558 4.60490704 4.43456793 4.11883116 4.3114109\n",
      " 4.28190374 4.19994497 3.90977359 4.68176126 4.63172007 3.8757782\n",
      " 4.66498375 4.6394434  4.50348425 4.58043957 4.51660728 4.48421574\n",
      " 4.52753067 4.53923798 4.39093065 4.57516432 3.97649884 4.41947889\n",
      " 4.32192755 4.27504826 4.3816781  4.24625874 4.18243313 4.26145744\n",
      " 4.20000219 4.38384008 4.20996094 4.26833963 4.19091415 4.3702426\n",
      " 4.2300334  4.30037498 4.27696896 4.3468442  4.21154928 4.25372744\n",
      " 4.15997553 4.2277503  4.29891729 4.21110916 4.19856977 4.51773167\n",
      " 4.17833424 4.19801998 4.23760796 4.22203255 4.11208868 4.1284647\n",
      " 4.14498234 4.2552042  4.24614859 4.2231102  4.20158434 4.1080451\n",
      " 4.1095252  4.12105989 4.09726191 4.03811741 4.12830305 4.21626425\n",
      " 4.03308916 3.88301468 3.84585047 3.85780025 4.02523279 4.38591146\n",
      " 4.1190896  4.06668234 4.10584068 3.93187523 3.94612002 4.00064802\n",
      " 3.96449399 4.1151042  4.12398863 4.16932869 4.0141921  4.04288244\n",
      " 3.90760541 4.1095252  3.93883491 3.84432387 3.86889434 3.82544827\n",
      " 3.90838242 3.84767699 4.08158016 4.06941938 3.90573668 3.9066205\n",
      " 3.89846158 3.94049883 3.96634936 4.00293303 3.93031573 3.82693219\n",
      " 4.06207371 3.94850469 3.90179372 4.04559517 4.01296616 3.68571782\n",
      " 3.87160683 4.07229185 3.96802902 3.83797956 3.68672991 3.86994433\n",
      " 3.82105565 3.90473247 3.74800658 3.88246131 3.75865793 3.91120982\n",
      " 3.87528968 3.68123674 3.89043689 3.74839592 3.71893549 3.98965335\n",
      " 3.84418178 3.65087867 3.67839026 3.81675124 3.96785474 3.90510798\n",
      " 3.94894457 3.86084056 3.81918955 4.03929043 3.6508503  3.92657161\n",
      " 3.91349912 4.05489254 3.77033973 3.8147409  3.91365242 3.83854175\n",
      " 3.91633868 3.84888077 4.1094327  3.7569139  3.91852236 3.62927699\n",
      " 3.73128557 3.96892023 3.78279567 3.83438182 3.7306869  3.71081901\n",
      " 3.99461746 3.97118735 4.10210752 4.00716162 3.82257819 3.71092391\n",
      " 3.96779227 4.02331877 4.05086851 3.83627963 3.84643626 3.73942924\n",
      " 3.81519961 4.15420628 4.0189743  3.74971318 3.91530681 3.80405927\n",
      " 3.75647354 3.89998865 3.78838277 3.92316937 3.99138713 3.80792809]\n"
     ]
    }
   ],
   "source": [
    "B = 1  #feedback bits\n",
    "train(Nc,N,Nt,B,Nr,L,SNR_dB,K,EPOCH,BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([204800, 2, 32, 128])\n",
      "DNN_BS_hyb_OFDM(\n",
      "  (DQL): DequantizationLayer()\n",
      "  (FC1): Linear(in_features=6, out_features=2048, bias=True)\n",
      "  (bn1): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (mish1): Mish()\n",
      "  (FC2): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "  (bn2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (mish2): Mish()\n",
      "  (FC3): Linear(in_features=1024, out_features=384, bias=True)\n",
      "  (bn3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (mish3): Mish()\n",
      "  (res1): RES_BLOCK(\n",
      "    (conv1): Sequential(\n",
      "      (0): Conv2d(8, 256, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0))\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Mish()\n",
      "    )\n",
      "    (conv2): Sequential(\n",
      "      (0): Conv2d(256, 512, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0))\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Mish()\n",
      "    )\n",
      "    (conv3): Sequential(\n",
      "      (0): Conv2d(512, 8, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0))\n",
      "      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (conv): Conv2d(8, 8, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0))\n",
      ")\n",
      "DNN_US_RF_OFDM(\n",
      "  (pilot): Linear(in_features=64, out_features=8, bias=False)\n",
      "  (res): RES_BLOCK(\n",
      "    (conv1): Sequential(\n",
      "      (0): Conv2d(16, 256, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0))\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Mish()\n",
      "    )\n",
      "    (conv2): Sequential(\n",
      "      (0): Conv2d(256, 512, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0))\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Mish()\n",
      "    )\n",
      "    (conv3): Sequential(\n",
      "      (0): Conv2d(512, 16, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0))\n",
      "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (FC2): Linear(in_features=512, out_features=1024, bias=True)\n",
      "  (bn2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu2): ReLU()\n",
      "  (FC3): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (bn3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu3): ReLU()\n",
      "  (FC4): Linear(in_features=512, out_features=3, bias=True)\n",
      "  (bn4): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (QL): QuantizationLayer()\n",
      ")\n",
      "Epoch: 0 time 0:01:23.526982 train SE 3.997 test SE 4.727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:256: UserWarning: Couldn't retrieve source code for container of type DNN_US_RF_OFDM. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:256: UserWarning: Couldn't retrieve source code for container of type RES_BLOCK. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:256: UserWarning: Couldn't retrieve source code for container of type Mish. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:256: UserWarning: Couldn't retrieve source code for container of type QuantizationLayer. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:256: UserWarning: Couldn't retrieve source code for container of type DNN_BS_hyb_OFDM. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:256: UserWarning: Couldn't retrieve source code for container of type DequantizationLayer. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved!\n",
      "Epoch: 1 time 0:01:24.386680 train SE 4.928 test SE 4.491\n",
      "Epoch: 2 time 0:01:24.103874 train SE 4.976 test SE 4.594\n",
      "Epoch: 3 time 0:01:24.949124 train SE 4.739 test SE 5.440\n",
      "Model saved!\n",
      "Epoch: 4 time 0:01:24.297893 train SE 4.527 test SE 4.692\n",
      "Epoch: 5 time 0:01:24.041667 train SE 4.678 test SE 4.804\n",
      "Epoch: 6 time 0:01:22.382077 train SE 4.928 test SE 5.069\n",
      "Epoch: 7 time 0:01:23.421213 train SE 5.481 test SE 5.751\n",
      "Model saved!\n",
      "Epoch: 8 time 0:01:23.257191 train SE 4.805 test SE 4.904\n",
      "Epoch: 9 time 0:01:26.232788 train SE 4.991 test SE 3.899\n",
      "Epoch: 10 time 0:01:25.270723 train SE 5.356 test SE 5.389\n",
      "Epoch: 11 time 0:01:25.477251 train SE 4.572 test SE 4.285\n",
      "Epoch: 12 time 0:01:24.930193 train SE 4.281 test SE 4.896\n",
      "Epoch: 13 time 0:01:25.236519 train SE 4.795 test SE 5.015\n",
      "Epoch: 14 time 0:01:23.131937 train SE 5.039 test SE 4.926\n",
      "Epoch: 15 time 0:01:23.742353 train SE 4.981 test SE 5.071\n",
      "Epoch: 16 time 0:01:24.561643 train SE 5.011 test SE 5.161\n",
      "Epoch: 17 time 0:01:24.952687 train SE 4.951 test SE 4.707\n",
      "Epoch: 18 time 0:01:23.470081 train SE 4.772 test SE 4.637\n",
      "Epoch: 19 time 0:01:24.395099 train SE 4.907 test SE 5.623\n",
      "Epoch: 20 time 0:01:24.862883 train SE 5.630 test SE 5.849\n",
      "Model saved!\n",
      "Epoch: 21 time 0:01:24.766124 train SE 5.882 test SE 6.058\n",
      "Model saved!\n",
      "Epoch: 22 time 0:01:23.563865 train SE 5.762 test SE 5.429\n",
      "Epoch: 23 time 0:01:24.991595 train SE 5.755 test SE 5.857\n",
      "Epoch: 24 time 0:01:24.451007 train SE 5.714 test SE 4.733\n",
      "Epoch: 25 time 0:01:23.448614 train SE 4.949 test SE 4.553\n",
      "Epoch: 26 time 0:01:24.223726 train SE 4.835 test SE 5.190\n",
      "Epoch: 27 time 0:01:23.206332 train SE 5.204 test SE 5.079\n",
      "Epoch: 28 time 0:01:24.453470 train SE 5.276 test SE 5.398\n",
      "Epoch: 29 time 0:01:24.726215 train SE 5.154 test SE 4.467\n",
      "Epoch: 30 time 0:01:24.255469 train SE 5.135 test SE 5.455\n",
      "Epoch: 31 time 0:01:23.805778 train SE 5.245 test SE 5.085\n",
      "Epoch: 32 time 0:01:24.265926 train SE 4.773 test SE 5.275\n",
      "Epoch: 33 time 0:01:23.626050 train SE 5.060 test SE 5.412\n",
      "Epoch: 34 time 0:01:23.476690 train SE 4.870 test SE 4.388\n",
      "Epoch: 35 time 0:01:23.670152 train SE 4.422 test SE 4.717\n",
      "Epoch: 36 time 0:01:23.821080 train SE 4.805 test SE 4.744\n",
      "Epoch: 37 time 0:01:23.727008 train SE 4.735 test SE 4.856\n",
      "Epoch: 38 time 0:01:24.369338 train SE 4.790 test SE 5.173\n",
      "Epoch: 39 time 0:01:23.025266 train SE 5.254 test SE 5.274\n",
      "Epoch: 40 time 0:01:23.075229 train SE 5.351 test SE 5.482\n",
      "Epoch: 41 time 0:01:23.794717 train SE 5.401 test SE 5.604\n",
      "Epoch: 42 time 0:01:23.296101 train SE 5.474 test SE 5.487\n",
      "Epoch: 43 time 0:01:23.329166 train SE 5.439 test SE 5.883\n",
      "Epoch: 44 time 0:01:23.216899 train SE 5.451 test SE 5.793\n",
      "Epoch: 45 time 0:01:24.346297 train SE 5.443 test SE 5.346\n",
      "Epoch: 46 time 0:01:24.032578 train SE 5.245 test SE 5.518\n",
      "Epoch: 47 time 0:01:24.153747 train SE 5.508 test SE 5.654\n",
      "Epoch: 48 time 0:01:23.736589 train SE 5.443 test SE 5.524\n",
      "Epoch: 49 time 0:01:22.773419 train SE 5.140 test SE 5.070\n",
      "Epoch: 50 time 0:01:24.484452 train SE 5.310 test SE 5.452\n",
      "Epoch: 51 time 0:01:23.617737 train SE 5.541 test SE 5.799\n",
      "Epoch: 52 time 0:01:24.330908 train SE 5.331 test SE 5.333\n",
      "Epoch: 53 time 0:01:23.570324 train SE 5.317 test SE 5.441\n",
      "Epoch: 54 time 0:01:24.252089 train SE 4.858 test SE 5.635\n",
      "Epoch: 55 time 0:01:23.989281 train SE 5.031 test SE 5.302\n",
      "Epoch: 56 time 0:01:23.707037 train SE 5.192 test SE 5.342\n",
      "Epoch: 57 time 0:01:23.783278 train SE 5.297 test SE 5.294\n",
      "Epoch: 58 time 0:01:23.242299 train SE 5.390 test SE 5.723\n",
      "Epoch: 59 time 0:01:23.456607 train SE 4.503 test SE 4.870\n",
      "Epoch: 60 time 0:01:24.323830 train SE 4.812 test SE 5.157\n",
      "Epoch: 61 time 0:01:24.087455 train SE 5.027 test SE 5.406\n",
      "Epoch: 62 time 0:01:23.657572 train SE 5.162 test SE 5.290\n",
      "Epoch: 63 time 0:01:24.092449 train SE 5.112 test SE 5.350\n",
      "Epoch: 64 time 0:01:24.787003 train SE 5.129 test SE 5.385\n",
      "Epoch: 65 time 0:01:24.525695 train SE 5.077 test SE 5.162\n",
      "Epoch: 66 time 0:01:29.858544 train SE 5.048 test SE 5.356\n",
      "Epoch: 67 time 0:01:29.236653 train SE 5.099 test SE 4.541\n",
      "Epoch: 68 time 0:01:23.920934 train SE 5.161 test SE 4.408\n",
      "Epoch: 69 time 0:01:23.491145 train SE 5.300 test SE 5.454\n",
      "Epoch: 70 time 0:01:24.306163 train SE 5.302 test SE 5.470\n",
      "Epoch: 71 time 0:01:28.157073 train SE 5.108 test SE 4.955\n",
      "Epoch: 72 time 0:01:27.767180 train SE 5.056 test SE 5.299\n",
      "Epoch: 73 time 0:01:23.464630 train SE 5.201 test SE 5.053\n",
      "Epoch: 74 time 0:01:23.477672 train SE 5.249 test SE 5.458\n",
      "Epoch: 75 time 0:01:27.106431 train SE 5.298 test SE 5.386\n",
      "Epoch: 76 time 0:01:23.227278 train SE 5.413 test SE 5.479\n",
      "Epoch: 77 time 0:01:23.471630 train SE 5.339 test SE 5.518\n",
      "Epoch: 78 time 0:01:27.237562 train SE 5.346 test SE 5.595\n",
      "Epoch: 79 time 0:01:23.676982 train SE 5.450 test SE 5.767\n",
      "Epoch: 80 time 0:01:23.569936 train SE 5.388 test SE 5.143\n",
      "Epoch: 81 time 0:01:23.506079 train SE 5.116 test SE 5.384\n",
      "Epoch: 82 time 0:01:24.461084 train SE 5.294 test SE 5.318\n",
      "Epoch: 83 time 0:01:25.957463 train SE 5.364 test SE 5.265\n",
      "Epoch: 84 time 0:01:25.537088 train SE 5.347 test SE 5.164\n",
      "Epoch: 85 time 0:01:25.685355 train SE 5.365 test SE 5.019\n",
      "Epoch: 86 time 0:01:23.322063 train SE 5.388 test SE 5.248\n",
      "Epoch: 87 time 0:01:23.370909 train SE 5.340 test SE 5.098\n",
      "Epoch: 88 time 0:01:25.607378 train SE 5.397 test SE 5.398\n",
      "Epoch: 89 time 0:01:24.126394 train SE 5.391 test SE 5.382\n",
      "Epoch: 90 time 0:01:25.402651 train SE 5.396 test SE 5.003\n",
      "Epoch: 91 time 0:01:24.886423 train SE 5.365 test SE 5.158\n",
      "Epoch: 92 time 0:01:23.026853 train SE 5.404 test SE 5.207\n",
      "Epoch: 93 time 0:01:22.999812 train SE 5.431 test SE 5.677\n",
      "Epoch: 94 time 0:01:24.548792 train SE 5.451 test SE 5.580\n",
      "Epoch: 95 time 0:01:23.618780 train SE 5.438 test SE 5.566\n",
      "Epoch: 96 time 0:01:24.380132 train SE 5.443 test SE 5.710\n",
      "Epoch: 97 time 0:01:24.533249 train SE 5.376 test SE 4.852\n",
      "Epoch: 98 time 0:01:24.722737 train SE 5.380 test SE 4.970\n",
      "Epoch: 99 time 0:01:26.726980 train SE 5.381 test SE 5.416\n",
      "Epoch: 100 time 0:01:23.752279 train SE 5.389 test SE 5.208\n",
      "Epoch: 101 time 0:01:22.844722 train SE 5.416 test SE 5.082\n",
      "Epoch: 102 time 0:01:23.536437 train SE 5.424 test SE 4.850\n",
      "Epoch: 103 time 0:01:23.239190 train SE 5.391 test SE 5.221\n",
      "Epoch: 104 time 0:01:22.902078 train SE 5.420 test SE 5.292\n",
      "Epoch: 105 time 0:01:23.673135 train SE 5.458 test SE 5.278\n",
      "Epoch: 106 time 0:01:24.221085 train SE 5.442 test SE 5.089\n",
      "Epoch: 107 time 0:01:25.470234 train SE 5.448 test SE 5.217\n",
      "Epoch: 108 time 0:01:24.001687 train SE 5.444 test SE 5.112\n",
      "Epoch: 109 time 0:01:24.129483 train SE 5.379 test SE 5.031\n",
      "Epoch: 110 time 0:01:23.532862 train SE 5.403 test SE 5.130\n",
      "Epoch: 111 time 0:01:23.455628 train SE 5.424 test SE 5.404\n",
      "Epoch: 112 time 0:01:24.764305 train SE 5.404 test SE 5.020\n",
      "Epoch: 113 time 0:01:23.265632 train SE 5.367 test SE 5.133\n",
      "Epoch: 114 time 0:01:23.213705 train SE 5.365 test SE 5.137\n",
      "Epoch: 115 time 0:01:23.450291 train SE 5.372 test SE 5.400\n",
      "Epoch: 116 time 0:01:23.128465 train SE 5.360 test SE 5.081\n",
      "Epoch: 117 time 0:01:23.819662 train SE 5.353 test SE 5.148\n",
      "Epoch: 118 time 0:01:23.187826 train SE 5.357 test SE 5.118\n",
      "Epoch: 119 time 0:01:23.821716 train SE 5.368 test SE 5.443\n",
      "Epoch: 120 time 0:01:23.321489 train SE 5.347 test SE 5.103\n",
      "Epoch: 121 time 0:01:23.266633 train SE 5.318 test SE 5.232\n",
      "Epoch: 122 time 0:01:23.815677 train SE 5.318 test SE 5.140\n",
      "Epoch: 123 time 0:01:23.555334 train SE 5.321 test SE 5.230\n",
      "Epoch: 124 time 0:01:23.364944 train SE 5.286 test SE 5.189\n",
      "Epoch: 125 time 0:01:23.635782 train SE 5.282 test SE 5.204\n",
      "Epoch: 126 time 0:01:24.275968 train SE 5.302 test SE 5.163\n",
      "Epoch: 127 time 0:01:23.940380 train SE 5.253 test SE 5.098\n",
      "Epoch: 128 time 0:01:23.876445 train SE 5.264 test SE 5.070\n",
      "Epoch: 129 time 0:01:23.388518 train SE 5.193 test SE 5.116\n",
      "Epoch: 130 time 0:01:23.624239 train SE 5.209 test SE 5.062\n",
      "Epoch: 131 time 0:01:23.492549 train SE 5.178 test SE 4.932\n",
      "Epoch: 132 time 0:01:23.662038 train SE 5.110 test SE 4.797\n",
      "Epoch: 133 time 0:01:23.683590 train SE 5.058 test SE 4.966\n",
      "Epoch: 134 time 0:01:23.953457 train SE 5.101 test SE 4.879\n",
      "Epoch: 135 time 0:01:24.236605 train SE 5.121 test SE 5.080\n",
      "Epoch: 136 time 0:01:24.154247 train SE 5.090 test SE 4.716\n",
      "Epoch: 137 time 0:01:23.270095 train SE 4.991 test SE 4.941\n",
      "Epoch: 138 time 0:01:23.843147 train SE 5.064 test SE 5.336\n",
      "Epoch: 139 time 0:01:23.742291 train SE 5.025 test SE 5.023\n",
      "Epoch: 140 time 0:01:23.402313 train SE 5.027 test SE 5.181\n",
      "Epoch: 141 time 0:01:23.557478 train SE 5.105 test SE 5.074\n",
      "Epoch: 142 time 0:01:23.292684 train SE 5.095 test SE 4.878\n",
      "Epoch: 143 time 0:01:24.250103 train SE 4.978 test SE 4.646\n",
      "Epoch: 144 time 0:01:23.979436 train SE 4.991 test SE 4.440\n",
      "Epoch: 145 time 0:01:24.180355 train SE 5.047 test SE 4.626\n",
      "Epoch: 146 time 0:01:24.244559 train SE 5.064 test SE 4.592\n",
      "Epoch: 147 time 0:01:32.090636 train SE 5.079 test SE 4.605\n",
      "Epoch: 148 time 0:01:24.575198 train SE 5.096 test SE 4.721\n",
      "Epoch: 149 time 0:01:25.049888 train SE 5.130 test SE 4.534\n",
      "Epoch: 150 time 0:01:24.865280 train SE 5.134 test SE 4.883\n",
      "Epoch: 151 time 0:01:24.310446 train SE 5.114 test SE 4.828\n",
      "Epoch: 152 time 0:01:23.597825 train SE 5.107 test SE 4.877\n",
      "Epoch: 153 time 0:01:23.699828 train SE 5.076 test SE 4.913\n",
      "Epoch: 154 time 0:01:24.606542 train SE 5.065 test SE 4.726\n",
      "Epoch: 155 time 0:01:23.301806 train SE 5.093 test SE 4.982\n",
      "Epoch: 156 time 0:01:26.027284 train SE 5.107 test SE 5.048\n",
      "Epoch: 157 time 0:01:25.037961 train SE 5.111 test SE 4.672\n",
      "Epoch: 158 time 0:01:25.222469 train SE 5.127 test SE 5.098\n",
      "Epoch: 159 time 0:01:24.590608 train SE 5.113 test SE 4.954\n",
      "Epoch: 160 time 0:01:24.095933 train SE 5.135 test SE 4.914\n",
      "Epoch: 161 time 0:01:24.013603 train SE 5.130 test SE 4.881\n",
      "Epoch: 162 time 0:01:23.286670 train SE 5.136 test SE 4.834\n",
      "Epoch: 163 time 0:01:23.658063 train SE 5.138 test SE 5.107\n",
      "Epoch: 164 time 0:01:23.903898 train SE 5.118 test SE 5.176\n",
      "Epoch: 165 time 0:01:23.784327 train SE 5.167 test SE 4.770\n",
      "Epoch: 166 time 0:01:24.250532 train SE 5.168 test SE 5.158\n",
      "Epoch: 167 time 0:01:22.837365 train SE 5.181 test SE 5.245\n",
      "Epoch: 168 time 0:01:23.186282 train SE 5.209 test SE 5.218\n",
      "Epoch: 169 time 0:01:23.883264 train SE 5.192 test SE 5.139\n",
      "Epoch: 170 time 0:01:23.439281 train SE 5.204 test SE 5.166\n",
      "Epoch: 171 time 0:01:24.559216 train SE 5.153 test SE 5.089\n",
      "Epoch: 172 time 0:01:24.236803 train SE 5.183 test SE 5.020\n",
      "Epoch: 173 time 0:01:23.803261 train SE 5.181 test SE 5.096\n",
      "Epoch: 174 time 0:01:24.748139 train SE 5.197 test SE 5.145\n",
      "Epoch: 175 time 0:01:23.258701 train SE 5.201 test SE 5.103\n",
      "Epoch: 176 time 0:01:24.441066 train SE 5.183 test SE 5.118\n",
      "Epoch: 177 time 0:01:22.575010 train SE 5.205 test SE 4.970\n",
      "Epoch: 178 time 0:01:23.669016 train SE 5.210 test SE 5.102\n",
      "Epoch: 179 time 0:01:23.148426 train SE 5.214 test SE 5.166\n",
      "The best SE is: 6.058\n",
      "[4.72696924 4.49148273 4.5943861  5.43953562 4.69201994 4.80383873\n",
      " 5.06886578 5.75058222 4.90355206 3.89860511 5.38948679 4.28487921\n",
      " 4.89629793 5.01482773 4.92626286 5.0708828  5.16076851 4.70734549\n",
      " 4.63656807 5.62289858 5.84907484 6.05809021 5.42873335 5.85684967\n",
      " 4.7326827  4.55322218 5.19009924 5.07889462 5.39758253 4.46679163\n",
      " 5.45536184 5.08473778 5.27455759 5.4115653  4.38794184 4.71654701\n",
      " 4.74425507 4.85570908 5.1732111  5.27404833 5.48219776 5.60414076\n",
      " 5.48728371 5.88334656 5.79333639 5.34587622 5.51806259 5.65396404\n",
      " 5.52394199 5.07023525 5.45230484 5.79854345 5.33303118 5.44056892\n",
      " 5.63505316 5.30220985 5.34172773 5.29444647 5.72286367 4.86972046\n",
      " 5.15731382 5.40566826 5.29040813 5.34993124 5.38529491 5.16173744\n",
      " 5.35550547 4.541049   4.40812445 5.45393419 5.47034502 4.9545083\n",
      " 5.29891396 5.05327559 5.45836115 5.38560057 5.47909546 5.51822567\n",
      " 5.59474659 5.76654482 5.14340925 5.38371038 5.31761599 5.2653532\n",
      " 5.16376734 5.01929617 5.24779558 5.09762573 5.39769793 5.38189507\n",
      " 5.00328302 5.15794516 5.20745087 5.67713881 5.58049154 5.56588697\n",
      " 5.70996714 4.85208941 4.97001648 5.41617346 5.20815468 5.0819664\n",
      " 4.849895   5.22098827 5.29177046 5.27833796 5.08896017 5.2174077\n",
      " 5.11225271 5.03123665 5.13017988 5.40360165 5.01979351 5.13334608\n",
      " 5.13714886 5.3995719  5.08149624 5.14766455 5.11755753 5.44345093\n",
      " 5.10298109 5.23164845 5.13993073 5.22989845 5.18910694 5.20403957\n",
      " 5.16335726 5.09804535 5.0703845  5.11627531 5.06160641 4.93207788\n",
      " 4.79661703 4.96574926 4.87880754 5.08045149 4.71568537 4.94098377\n",
      " 5.33612919 5.02268553 5.18100262 5.07366419 4.87837934 4.64636087\n",
      " 4.44010878 4.6260376  4.59249878 4.60471821 4.7206459  4.5342145\n",
      " 4.88251591 4.82782936 4.87677288 4.91321039 4.72600079 4.9822979\n",
      " 5.04810572 4.6717782  5.09809256 4.95384836 4.91376829 4.88055944\n",
      " 4.83375216 5.10727262 5.17591047 4.76988268 5.15848207 5.24505091\n",
      " 5.21791029 5.13916492 5.16639662 5.08896542 5.02030087 5.09559679\n",
      " 5.14514971 5.10336161 5.11783981 4.96995163 5.10234022 5.16618586]\n"
     ]
    }
   ],
   "source": [
    "B = 3\n",
    "train(Nc,N,Nt,B,Nr,L,SNR_dB,K,EPOCH,BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([204800, 2, 32, 128])\n",
      "DNN_BS_hyb_OFDM(\n",
      "  (DQL): DequantizationLayer()\n",
      "  (FC1): Linear(in_features=10, out_features=2048, bias=True)\n",
      "  (bn1): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (mish1): Mish()\n",
      "  (FC2): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "  (bn2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (mish2): Mish()\n",
      "  (FC3): Linear(in_features=1024, out_features=384, bias=True)\n",
      "  (bn3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (mish3): Mish()\n",
      "  (res1): RES_BLOCK(\n",
      "    (conv1): Sequential(\n",
      "      (0): Conv2d(8, 256, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0))\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Mish()\n",
      "    )\n",
      "    (conv2): Sequential(\n",
      "      (0): Conv2d(256, 512, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0))\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Mish()\n",
      "    )\n",
      "    (conv3): Sequential(\n",
      "      (0): Conv2d(512, 8, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0))\n",
      "      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (conv): Conv2d(8, 8, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0))\n",
      ")\n",
      "DNN_US_RF_OFDM(\n",
      "  (pilot): Linear(in_features=64, out_features=8, bias=False)\n",
      "  (res): RES_BLOCK(\n",
      "    (conv1): Sequential(\n",
      "      (0): Conv2d(16, 256, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0))\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Mish()\n",
      "    )\n",
      "    (conv2): Sequential(\n",
      "      (0): Conv2d(256, 512, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0))\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Mish()\n",
      "    )\n",
      "    (conv3): Sequential(\n",
      "      (0): Conv2d(512, 16, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0))\n",
      "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (FC2): Linear(in_features=512, out_features=1024, bias=True)\n",
      "  (bn2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu2): ReLU()\n",
      "  (FC3): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (bn3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu3): ReLU()\n",
      "  (FC4): Linear(in_features=512, out_features=5, bias=True)\n",
      "  (bn4): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (QL): QuantizationLayer()\n",
      ")\n",
      "Epoch: 0 time 0:01:25.013044 train SE 3.989 test SE 4.474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:256: UserWarning: Couldn't retrieve source code for container of type DNN_US_RF_OFDM. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:256: UserWarning: Couldn't retrieve source code for container of type RES_BLOCK. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:256: UserWarning: Couldn't retrieve source code for container of type Mish. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:256: UserWarning: Couldn't retrieve source code for container of type QuantizationLayer. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:256: UserWarning: Couldn't retrieve source code for container of type DNN_BS_hyb_OFDM. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:256: UserWarning: Couldn't retrieve source code for container of type DequantizationLayer. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved!\n",
      "Epoch: 1 time 0:01:27.061063 train SE 5.516 test SE 6.243\n",
      "Model saved!\n",
      "Epoch: 2 time 0:01:23.472120 train SE 6.421 test SE 6.585\n",
      "Model saved!\n",
      "Epoch: 3 time 0:01:27.385652 train SE 6.732 test SE 7.008\n",
      "Model saved!\n",
      "Epoch: 4 time 0:01:23.105067 train SE 7.088 test SE 7.230\n",
      "Model saved!\n",
      "Epoch: 5 time 0:01:26.819072 train SE 7.040 test SE 7.107\n",
      "Epoch: 6 time 0:01:27.670501 train SE 6.951 test SE 7.069\n",
      "Epoch: 7 time 0:01:24.903476 train SE 7.026 test SE 7.178\n",
      "Epoch: 8 time 0:01:23.263196 train SE 6.724 test SE 6.098\n",
      "Epoch: 9 time 0:01:26.747427 train SE 6.217 test SE 6.419\n",
      "Epoch: 10 time 0:01:27.301478 train SE 6.675 test SE 6.582\n",
      "Epoch: 11 time 0:01:26.368295 train SE 6.701 test SE 6.765\n",
      "Epoch: 12 time 0:01:25.371853 train SE 6.167 test SE 6.387\n",
      "Epoch: 13 time 0:01:23.203911 train SE 6.089 test SE 6.045\n",
      "Epoch: 14 time 0:01:26.239257 train SE 5.825 test SE 5.680\n",
      "Epoch: 15 time 0:01:23.135960 train SE 6.014 test SE 6.342\n",
      "Epoch: 16 time 0:01:24.793034 train SE 6.377 test SE 5.908\n",
      "Epoch: 17 time 0:01:24.540242 train SE 5.620 test SE 5.216\n",
      "Epoch: 18 time 0:01:24.256635 train SE 5.749 test SE 5.935\n",
      "Epoch: 19 time 0:01:25.011382 train SE 6.146 test SE 6.276\n",
      "Epoch: 20 time 0:01:25.164010 train SE 5.784 test SE 5.987\n",
      "Epoch: 21 time 0:01:24.336272 train SE 5.745 test SE 5.958\n",
      "Epoch: 22 time 0:01:24.501261 train SE 5.973 test SE 6.048\n",
      "Epoch: 23 time 0:01:24.327310 train SE 5.813 test SE 6.064\n",
      "Epoch: 24 time 0:01:23.420722 train SE 6.356 test SE 6.519\n",
      "Epoch: 25 time 0:01:23.798743 train SE 5.970 test SE 5.890\n",
      "Epoch: 26 time 0:01:24.276947 train SE 6.087 test SE 6.427\n",
      "Epoch: 27 time 0:01:23.408805 train SE 6.212 test SE 6.113\n",
      "Epoch: 28 time 0:01:23.690491 train SE 5.816 test SE 5.808\n",
      "Epoch: 29 time 0:01:23.812702 train SE 5.670 test SE 5.710\n",
      "Epoch: 30 time 0:01:23.725913 train SE 5.964 test SE 6.181\n",
      "Epoch: 31 time 0:01:23.523961 train SE 5.567 test SE 5.605\n",
      "Epoch: 32 time 0:01:23.511032 train SE 5.284 test SE 5.237\n",
      "Epoch: 33 time 0:01:23.571842 train SE 5.527 test SE 5.414\n",
      "Epoch: 34 time 0:01:23.305145 train SE 6.005 test SE 6.214\n",
      "Epoch: 35 time 0:01:23.941486 train SE 5.977 test SE 5.958\n",
      "Epoch: 36 time 0:01:24.011223 train SE 5.425 test SE 5.776\n",
      "Epoch: 37 time 0:01:23.081087 train SE 5.678 test SE 6.037\n",
      "Epoch: 38 time 0:01:23.689992 train SE 5.820 test SE 5.303\n",
      "Epoch: 39 time 0:01:23.053284 train SE 5.589 test SE 5.790\n",
      "Epoch: 40 time 0:01:24.222134 train SE 5.777 test SE 5.987\n",
      "Epoch: 41 time 0:01:23.843082 train SE 5.726 test SE 5.985\n",
      "Epoch: 42 time 0:01:23.313989 train SE 5.923 test SE 6.079\n",
      "Epoch: 43 time 0:01:23.556910 train SE 6.051 test SE 6.281\n",
      "Epoch: 44 time 0:01:24.056473 train SE 5.918 test SE 5.651\n",
      "Epoch: 45 time 0:01:23.532878 train SE 5.557 test SE 5.843\n",
      "Epoch: 46 time 0:01:24.393718 train SE 5.679 test SE 5.826\n",
      "Epoch: 47 time 0:01:23.096579 train SE 5.720 test SE 5.398\n",
      "Epoch: 48 time 0:01:24.054975 train SE 5.389 test SE 5.838\n",
      "Epoch: 49 time 0:01:23.627731 train SE 5.560 test SE 5.797\n",
      "Epoch: 50 time 0:01:23.953896 train SE 5.445 test SE 5.726\n",
      "Epoch: 51 time 0:01:23.535898 train SE 5.648 test SE 6.040\n",
      "Epoch: 52 time 0:01:23.501011 train SE 5.925 test SE 6.508\n",
      "Epoch: 53 time 0:01:23.441306 train SE 5.916 test SE 5.989\n",
      "Epoch: 54 time 0:01:23.591731 train SE 5.653 test SE 5.722\n",
      "Epoch: 55 time 0:01:23.502396 train SE 5.818 test SE 6.035\n",
      "Epoch: 56 time 0:01:24.027675 train SE 5.631 test SE 5.778\n",
      "Epoch: 57 time 0:01:23.795294 train SE 5.688 test SE 5.828\n",
      "Epoch: 58 time 0:01:23.567802 train SE 5.592 test SE 5.489\n",
      "Epoch: 59 time 0:01:23.522890 train SE 5.316 test SE 5.644\n",
      "Epoch: 60 time 0:01:23.203337 train SE 5.427 test SE 5.490\n",
      "Epoch: 61 time 0:01:23.974961 train SE 5.217 test SE 5.572\n",
      "Epoch: 62 time 0:01:23.397954 train SE 5.652 test SE 5.987\n",
      "Epoch: 63 time 0:01:23.279231 train SE 5.970 test SE 6.103\n",
      "Epoch: 64 time 0:01:23.712458 train SE 5.509 test SE 5.294\n",
      "Epoch: 65 time 0:01:22.985857 train SE 5.144 test SE 5.587\n",
      "Epoch: 66 time 0:01:23.782371 train SE 5.524 test SE 5.592\n",
      "Epoch: 67 time 0:01:24.231020 train SE 5.299 test SE 5.662\n",
      "Epoch: 68 time 0:01:24.007167 train SE 5.380 test SE 5.829\n",
      "Epoch: 69 time 0:01:24.060520 train SE 5.746 test SE 6.084\n",
      "Epoch: 70 time 0:01:23.927310 train SE 5.467 test SE 5.353\n",
      "Epoch: 71 time 0:01:23.848107 train SE 5.251 test SE 5.403\n",
      "Epoch: 72 time 0:01:23.217738 train SE 5.527 test SE 5.881\n",
      "Epoch: 73 time 0:01:23.790282 train SE 5.644 test SE 5.523\n",
      "Epoch: 74 time 0:01:23.932358 train SE 5.265 test SE 5.161\n",
      "Epoch: 75 time 0:01:23.149444 train SE 5.177 test SE 5.650\n",
      "Epoch: 76 time 0:01:23.698597 train SE 5.496 test SE 5.579\n",
      "Epoch: 77 time 0:01:23.127987 train SE 5.633 test SE 5.396\n",
      "Epoch: 78 time 0:01:23.756572 train SE 5.638 test SE 5.685\n",
      "Epoch: 79 time 0:01:23.185855 train SE 5.311 test SE 5.501\n",
      "Epoch: 80 time 0:01:23.650580 train SE 5.421 test SE 5.728\n",
      "Epoch: 81 time 0:01:23.800132 train SE 5.404 test SE 5.729\n",
      "Epoch: 82 time 0:01:23.663700 train SE 5.785 test SE 5.830\n",
      "Epoch: 83 time 0:01:23.705137 train SE 5.630 test SE 5.530\n",
      "Epoch: 84 time 0:01:23.958737 train SE 5.538 test SE 5.736\n",
      "Epoch: 85 time 0:01:24.342246 train SE 5.624 test SE 5.985\n",
      "Epoch: 86 time 0:01:23.403345 train SE 5.764 test SE 5.945\n",
      "Epoch: 87 time 0:01:23.837533 train SE 5.757 test SE 5.680\n",
      "Epoch: 88 time 0:01:23.901909 train SE 5.649 test SE 5.578\n",
      "Epoch: 89 time 0:01:23.090152 train SE 5.423 test SE 5.371\n",
      "Epoch: 90 time 0:01:23.279704 train SE 5.386 test SE 5.670\n",
      "Epoch: 91 time 0:01:24.354175 train SE 5.744 test SE 5.933\n",
      "Epoch: 92 time 0:01:24.012213 train SE 5.611 test SE 5.603\n",
      "Epoch: 93 time 0:01:23.405806 train SE 5.660 test SE 5.815\n",
      "Epoch: 94 time 0:01:23.271610 train SE 5.620 test SE 5.002\n",
      "Epoch: 95 time 0:01:23.712930 train SE 5.491 test SE 5.333\n",
      "Epoch: 96 time 0:01:23.135086 train SE 5.494 test SE 5.666\n",
      "Epoch: 97 time 0:01:23.425570 train SE 5.688 test SE 5.793\n",
      "Epoch: 98 time 0:01:23.372758 train SE 5.531 test SE 4.888\n",
      "Epoch: 99 time 0:01:23.245754 train SE 5.452 test SE 5.779\n",
      "Epoch: 100 time 0:01:23.551967 train SE 5.645 test SE 5.779\n",
      "Epoch: 101 time 0:01:23.451174 train SE 5.606 test SE 5.683\n",
      "Epoch: 102 time 0:01:23.673024 train SE 5.459 test SE 5.681\n",
      "Epoch: 103 time 0:01:23.724442 train SE 5.706 test SE 5.827\n",
      "Epoch: 104 time 0:01:23.065328 train SE 5.642 test SE 5.656\n",
      "Epoch: 105 time 0:01:24.131314 train SE 5.453 test SE 5.577\n",
      "Epoch: 106 time 0:01:24.145344 train SE 5.445 test SE 5.612\n",
      "Epoch: 107 time 0:01:24.374665 train SE 5.426 test SE 5.545\n",
      "Epoch: 108 time 0:01:24.281463 train SE 5.609 test SE 5.855\n",
      "Epoch: 109 time 0:01:24.358239 train SE 5.769 test SE 5.843\n",
      "Epoch: 110 time 0:01:23.880907 train SE 5.486 test SE 5.639\n",
      "Epoch: 111 time 0:01:24.473396 train SE 5.557 test SE 6.016\n",
      "Epoch: 112 time 0:01:23.452218 train SE 5.799 test SE 6.028\n",
      "Epoch: 113 time 0:01:23.373846 train SE 5.719 test SE 5.686\n",
      "Epoch: 114 time 0:01:23.299650 train SE 5.761 test SE 5.726\n",
      "Epoch: 115 time 0:01:23.100017 train SE 5.543 test SE 5.732\n",
      "Epoch: 116 time 0:01:23.758342 train SE 5.625 test SE 5.322\n",
      "Epoch: 117 time 0:01:23.004742 train SE 5.580 test SE 5.786\n",
      "Epoch: 118 time 0:01:26.525885 train SE 5.597 test SE 5.258\n",
      "Epoch: 119 time 0:01:23.671637 train SE 5.182 test SE 5.555\n",
      "Epoch: 120 time 0:01:19.542715 train SE 5.379 test SE 5.564\n",
      "Epoch: 121 time 0:01:23.805459 train SE 5.409 test SE 5.419\n",
      "Epoch: 122 time 0:01:23.490619 train SE 5.538 test SE 5.537\n",
      "Epoch: 123 time 0:01:23.198293 train SE 5.563 test SE 5.726\n",
      "Epoch: 124 time 0:01:23.734807 train SE 5.522 test SE 5.679\n",
      "Epoch: 125 time 0:01:23.468210 train SE 5.299 test SE 5.364\n",
      "Epoch: 126 time 0:01:24.329382 train SE 5.034 test SE 5.166\n",
      "Epoch: 127 time 0:01:23.110001 train SE 5.243 test SE 5.557\n",
      "Epoch: 128 time 0:01:23.470596 train SE 5.438 test SE 5.282\n",
      "Epoch: 129 time 0:01:23.757332 train SE 5.280 test SE 5.439\n",
      "Epoch: 130 time 0:01:23.245194 train SE 5.463 test SE 5.775\n",
      "Epoch: 131 time 0:01:23.929907 train SE 5.634 test SE 5.669\n",
      "Epoch: 132 time 0:01:23.281792 train SE 5.673 test SE 5.969\n",
      "Epoch: 133 time 0:01:23.756800 train SE 5.813 test SE 5.995\n",
      "Epoch: 134 time 0:01:23.360407 train SE 6.043 test SE 5.984\n",
      "Epoch: 135 time 0:01:23.825256 train SE 6.026 test SE 6.042\n",
      "Epoch: 136 time 0:01:23.475063 train SE 5.992 test SE 6.081\n",
      "Epoch: 137 time 0:01:24.036577 train SE 5.811 test SE 5.951\n",
      "Epoch: 138 time 0:01:23.913387 train SE 5.835 test SE 5.802\n",
      "Epoch: 139 time 0:01:23.319524 train SE 5.697 test SE 5.604\n",
      "Epoch: 140 time 0:01:23.716354 train SE 5.570 test SE 5.686\n",
      "Epoch: 141 time 0:01:23.627074 train SE 5.237 test SE 5.299\n",
      "Epoch: 142 time 0:01:31.228436 train SE 5.317 test SE 5.716\n",
      "Epoch: 143 time 0:01:25.613390 train SE 5.622 test SE 5.684\n",
      "Epoch: 144 time 0:01:23.235625 train SE 5.457 test SE 5.631\n",
      "Epoch: 145 time 0:01:23.933784 train SE 5.525 test SE 5.734\n",
      "Epoch: 146 time 0:01:24.921775 train SE 5.614 test SE 5.819\n",
      "Epoch: 147 time 0:01:22.938522 train SE 5.614 test SE 5.569\n",
      "Epoch: 148 time 0:01:25.696162 train SE 5.592 test SE 5.629\n",
      "Epoch: 149 time 0:01:24.691407 train SE 5.501 test SE 5.739\n",
      "Epoch: 150 time 0:01:23.828586 train SE 5.584 test SE 5.487\n",
      "Epoch: 151 time 0:01:24.727214 train SE 5.254 test SE 5.389\n",
      "Epoch: 152 time 0:01:24.482358 train SE 5.361 test SE 5.507\n",
      "Epoch: 153 time 0:01:24.299866 train SE 5.310 test SE 5.556\n",
      "Epoch: 154 time 0:01:23.755892 train SE 5.260 test SE 5.107\n",
      "Epoch: 155 time 0:01:23.649153 train SE 5.032 test SE 5.495\n",
      "Epoch: 156 time 0:01:23.211344 train SE 5.594 test SE 5.836\n",
      "Epoch: 157 time 0:01:22.678508 train SE 5.658 test SE 5.652\n",
      "Epoch: 158 time 0:01:21.652055 train SE 5.486 test SE 5.609\n",
      "Epoch: 159 time 0:01:24.911353 train SE 5.348 test SE 5.336\n",
      "Epoch: 160 time 0:01:23.494051 train SE 5.096 test SE 5.182\n",
      "Epoch: 161 time 0:01:24.068620 train SE 5.491 test SE 5.813\n",
      "Epoch: 162 time 0:01:25.117800 train SE 5.759 test SE 5.884\n",
      "Epoch: 163 time 0:01:24.558181 train SE 5.866 test SE 5.889\n",
      "Epoch: 164 time 0:01:24.289039 train SE 5.352 test SE 5.058\n",
      "Epoch: 165 time 0:01:29.521501 train SE 5.276 test SE 5.499\n",
      "Epoch: 166 time 0:01:23.563884 train SE 5.488 test SE 5.586\n",
      "Epoch: 167 time 0:01:24.087857 train SE 5.400 test SE 5.616\n",
      "Epoch: 168 time 0:01:24.459146 train SE 5.576 test SE 5.779\n",
      "Epoch: 169 time 0:01:23.643228 train SE 5.643 test SE 5.935\n",
      "Epoch: 170 time 0:01:23.483524 train SE 5.960 test SE 6.188\n",
      "Epoch: 171 time 0:01:24.681876 train SE 6.003 test SE 6.102\n",
      "Epoch: 172 time 0:01:23.718516 train SE 5.884 test SE 5.973\n",
      "Epoch: 173 time 0:01:23.875087 train SE 5.994 test SE 6.185\n",
      "Epoch: 174 time 0:01:20.745407 train SE 6.134 test SE 6.239\n",
      "Epoch: 175 time 0:01:24.189297 train SE 6.074 test SE 6.049\n",
      "Epoch: 176 time 0:01:23.587266 train SE 5.794 test SE 5.797\n",
      "Epoch: 177 time 0:01:23.419719 train SE 5.449 test SE 5.517\n",
      "Epoch: 178 time 0:01:23.260216 train SE 5.269 test SE 5.516\n",
      "Epoch: 179 time 0:01:24.059179 train SE 5.452 test SE 5.718\n",
      "The best SE is: 7.230\n",
      "[4.47422552 6.2431531  6.58537292 7.0076189  7.22976255 7.10725737\n",
      " 7.06927109 7.17797709 6.09795141 6.41903067 6.58181858 6.76537704\n",
      " 6.38674498 6.04526424 5.67978954 6.34181499 5.90790939 5.21638489\n",
      " 5.93544531 6.27564907 5.98713255 5.95761395 6.0484004  6.06407404\n",
      " 6.51913023 5.89044857 6.42715454 6.11341    5.80803299 5.71042728\n",
      " 6.18138409 5.60510874 5.23650694 5.4143939  6.21375036 5.95761251\n",
      " 5.77617168 6.03742886 5.30311441 5.79032898 5.98651075 5.98523188\n",
      " 6.07888937 6.28094959 5.65126419 5.84341526 5.82584143 5.39758444\n",
      " 5.83807659 5.79728031 5.72611952 6.03998995 6.50795746 5.98922873\n",
      " 5.72187567 6.03452539 5.7781024  5.82838345 5.48891306 5.64374304\n",
      " 5.49033785 5.57170343 5.98658323 6.10263062 5.29408312 5.5871253\n",
      " 5.59207344 5.66185713 5.82868052 6.08438349 5.35298729 5.40314531\n",
      " 5.88144159 5.52278376 5.16149473 5.64992428 5.5793004  5.39647102\n",
      " 5.68539715 5.50145483 5.72781086 5.7287612  5.83040142 5.52973795\n",
      " 5.73612118 5.98505592 5.94494104 5.67986298 5.57807159 5.37107182\n",
      " 5.67035437 5.93253279 5.60252714 5.81467962 5.00198269 5.33341122\n",
      " 5.66609383 5.79259491 4.88813782 5.7789917  5.77919245 5.68303585\n",
      " 5.68082714 5.82748842 5.65606833 5.57657146 5.61166143 5.54470825\n",
      " 5.85523605 5.84310198 5.63857603 6.01637077 6.02794981 5.68611145\n",
      " 5.72565079 5.73150587 5.32216692 5.78578281 5.25827312 5.55456352\n",
      " 5.56380033 5.41892958 5.53655767 5.72608566 5.67888021 5.36422729\n",
      " 5.16642952 5.55725765 5.28187704 5.43882132 5.7753973  5.66936588\n",
      " 5.96918774 5.99532318 5.9835062  6.04212809 6.08149576 5.95142126\n",
      " 5.80203676 5.60425138 5.68588114 5.29857206 5.71637249 5.68449545\n",
      " 5.63069487 5.73412848 5.81911993 5.56883574 5.62891006 5.738904\n",
      " 5.48662329 5.3890872  5.50733185 5.55627251 5.10714817 5.49519348\n",
      " 5.83586359 5.651999   5.6088438  5.33573294 5.18176556 5.81303215\n",
      " 5.88416624 5.88870764 5.05800962 5.49924278 5.58603144 5.6162982\n",
      " 5.77921343 5.93478966 6.18815184 6.10222626 5.97286558 6.18508482\n",
      " 6.23896599 6.04876852 5.79685926 5.51685476 5.51573896 5.71782446]\n"
     ]
    }
   ],
   "source": [
    "B = 5\n",
    "train(Nc,N,Nt,B,Nr,L,SNR_dB,K,EPOCH,BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([204800, 2, 32, 128])\n",
      "DNN_BS_hyb_OFDM(\n",
      "  (DQL): DequantizationLayer()\n",
      "  (FC1): Linear(in_features=16, out_features=2048, bias=True)\n",
      "  (bn1): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (mish1): Mish()\n",
      "  (FC2): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "  (bn2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (mish2): Mish()\n",
      "  (FC3): Linear(in_features=1024, out_features=384, bias=True)\n",
      "  (bn3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (mish3): Mish()\n",
      "  (res1): RES_BLOCK(\n",
      "    (conv1): Sequential(\n",
      "      (0): Conv2d(8, 256, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0))\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Mish()\n",
      "    )\n",
      "    (conv2): Sequential(\n",
      "      (0): Conv2d(256, 512, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0))\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Mish()\n",
      "    )\n",
      "    (conv3): Sequential(\n",
      "      (0): Conv2d(512, 8, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0))\n",
      "      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (conv): Conv2d(8, 8, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0))\n",
      ")\n",
      "DNN_US_RF_OFDM(\n",
      "  (pilot): Linear(in_features=64, out_features=8, bias=False)\n",
      "  (res): RES_BLOCK(\n",
      "    (conv1): Sequential(\n",
      "      (0): Conv2d(16, 256, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0))\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Mish()\n",
      "    )\n",
      "    (conv2): Sequential(\n",
      "      (0): Conv2d(256, 512, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0))\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Mish()\n",
      "    )\n",
      "    (conv3): Sequential(\n",
      "      (0): Conv2d(512, 16, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0))\n",
      "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (FC2): Linear(in_features=512, out_features=1024, bias=True)\n",
      "  (bn2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu2): ReLU()\n",
      "  (FC3): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (bn3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu3): ReLU()\n",
      "  (FC4): Linear(in_features=512, out_features=8, bias=True)\n",
      "  (bn4): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (QL): QuantizationLayer()\n",
      ")\n",
      "Epoch: 0 time 0:01:25.365547 train SE 4.147 test SE 4.501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:256: UserWarning: Couldn't retrieve source code for container of type DNN_US_RF_OFDM. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:256: UserWarning: Couldn't retrieve source code for container of type RES_BLOCK. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:256: UserWarning: Couldn't retrieve source code for container of type Mish. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:256: UserWarning: Couldn't retrieve source code for container of type QuantizationLayer. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:256: UserWarning: Couldn't retrieve source code for container of type DNN_BS_hyb_OFDM. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:256: UserWarning: Couldn't retrieve source code for container of type DequantizationLayer. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved!\n",
      "Epoch: 1 time 0:01:26.734897 train SE 5.037 test SE 5.578\n",
      "Model saved!\n",
      "Epoch: 2 time 0:01:26.787290 train SE 6.491 test SE 6.891\n",
      "Model saved!\n",
      "Epoch: 3 time 0:01:26.447252 train SE 7.032 test SE 7.058\n",
      "Model saved!\n",
      "Epoch: 4 time 0:01:26.264545 train SE 7.055 test SE 7.112\n",
      "Model saved!\n",
      "Epoch: 5 time 0:01:25.455357 train SE 7.081 test SE 7.243\n",
      "Model saved!\n",
      "Epoch: 6 time 0:01:23.314053 train SE 7.197 test SE 7.210\n",
      "Epoch: 7 time 0:01:23.404806 train SE 7.401 test SE 7.643\n",
      "Model saved!\n",
      "Epoch: 8 time 0:01:23.138498 train SE 7.776 test SE 7.816\n",
      "Model saved!\n",
      "Epoch: 9 time 0:01:24.176828 train SE 7.890 test SE 7.916\n",
      "Model saved!\n",
      "Epoch: 10 time 0:01:23.452696 train SE 8.035 test SE 8.009\n",
      "Model saved!\n",
      "Epoch: 11 time 0:01:26.855123 train SE 8.077 test SE 7.989\n",
      "Epoch: 12 time 0:01:23.440706 train SE 8.201 test SE 8.104\n",
      "Model saved!\n",
      "Epoch: 13 time 0:01:26.199922 train SE 8.240 test SE 8.239\n",
      "Model saved!\n",
      "Epoch: 14 time 0:01:23.257612 train SE 8.320 test SE 8.304\n",
      "Model saved!\n",
      "Epoch: 15 time 0:01:22.875629 train SE 8.326 test SE 8.284\n",
      "Epoch: 16 time 0:01:23.123559 train SE 8.343 test SE 8.497\n",
      "Model saved!\n",
      "Epoch: 17 time 0:01:25.942965 train SE 8.369 test SE 8.447\n",
      "Epoch: 18 time 0:01:23.400263 train SE 8.404 test SE 8.514\n",
      "Model saved!\n",
      "Epoch: 19 time 0:01:26.411243 train SE 8.465 test SE 8.504\n",
      "Epoch: 20 time 0:01:27.790610 train SE 8.447 test SE 8.332\n",
      "Epoch: 21 time 0:01:25.856187 train SE 8.504 test SE 8.551\n",
      "Model saved!\n",
      "Epoch: 22 time 0:01:24.643583 train SE 8.478 test SE 8.476\n",
      "Epoch: 23 time 0:01:24.562762 train SE 8.533 test SE 8.578\n",
      "Model saved!\n",
      "Epoch: 24 time 0:01:26.376386 train SE 8.499 test SE 8.556\n",
      "Epoch: 25 time 0:01:24.784136 train SE 8.496 test SE 8.582\n",
      "Model saved!\n",
      "Epoch: 26 time 0:01:23.683621 train SE 8.534 test SE 8.491\n",
      "Epoch: 27 time 0:01:24.666774 train SE 8.568 test SE 8.572\n",
      "Epoch: 28 time 0:01:24.248060 train SE 8.583 test SE 8.621\n",
      "Model saved!\n",
      "Epoch: 29 time 0:01:24.085342 train SE 8.655 test SE 8.699\n",
      "Model saved!\n",
      "Epoch: 30 time 0:01:23.899153 train SE 8.725 test SE 8.695\n",
      "Epoch: 31 time 0:01:24.014704 train SE 8.714 test SE 8.673\n",
      "Epoch: 32 time 0:01:23.909884 train SE 8.609 test SE 8.556\n",
      "Epoch: 33 time 0:01:24.211169 train SE 8.565 test SE 8.677\n",
      "Epoch: 34 time 0:01:25.187076 train SE 8.589 test SE 8.660\n",
      "Epoch: 35 time 0:01:24.809004 train SE 8.568 test SE 8.610\n",
      "Epoch: 36 time 0:01:24.351819 train SE 8.547 test SE 8.553\n",
      "Epoch: 37 time 0:01:24.565337 train SE 8.603 test SE 8.686\n",
      "Epoch: 38 time 0:01:23.103059 train SE 8.572 test SE 8.537\n",
      "Epoch: 39 time 0:01:23.004383 train SE 8.596 test SE 8.622\n",
      "Epoch: 40 time 0:01:24.235744 train SE 8.558 test SE 8.658\n",
      "Epoch: 41 time 0:01:23.474610 train SE 8.639 test SE 8.675\n",
      "Epoch: 42 time 0:01:24.477949 train SE 8.562 test SE 8.628\n",
      "Epoch: 43 time 0:01:24.606216 train SE 8.614 test SE 8.618\n",
      "Epoch: 44 time 0:01:23.809760 train SE 8.672 test SE 8.710\n",
      "Model saved!\n",
      "Epoch: 45 time 0:01:24.253498 train SE 8.701 test SE 8.758\n",
      "Model saved!\n",
      "Epoch: 46 time 0:01:23.782222 train SE 8.624 test SE 8.574\n",
      "Epoch: 47 time 0:01:23.665625 train SE 8.650 test SE 8.674\n",
      "Epoch: 48 time 0:01:23.490546 train SE 8.689 test SE 8.638\n",
      "Epoch: 49 time 0:01:24.223025 train SE 8.702 test SE 8.796\n",
      "Model saved!\n",
      "Epoch: 50 time 0:01:24.067527 train SE 8.693 test SE 8.720\n",
      "Epoch: 51 time 0:01:23.335510 train SE 8.664 test SE 8.720\n",
      "Epoch: 52 time 0:01:24.336851 train SE 8.719 test SE 8.765\n",
      "Epoch: 53 time 0:01:24.761325 train SE 8.644 test SE 8.773\n",
      "Epoch: 54 time 0:01:22.817184 train SE 8.646 test SE 8.672\n",
      "Epoch: 55 time 0:01:26.201754 train SE 8.616 test SE 8.699\n",
      "Epoch: 56 time 0:01:23.599289 train SE 8.707 test SE 8.787\n",
      "Epoch: 57 time 0:01:23.170045 train SE 8.751 test SE 8.874\n",
      "Model saved!\n",
      "Epoch: 58 time 0:01:23.676523 train SE 8.779 test SE 8.954\n",
      "Model saved!\n",
      "Epoch: 59 time 0:01:23.291149 train SE 8.810 test SE 8.955\n",
      "Model saved!\n",
      "Epoch: 60 time 0:01:23.825719 train SE 8.884 test SE 9.059\n",
      "Model saved!\n",
      "Epoch: 61 time 0:01:23.888039 train SE 8.952 test SE 9.031\n",
      "Epoch: 62 time 0:01:23.703025 train SE 8.979 test SE 9.058\n",
      "Epoch: 63 time 0:01:24.033163 train SE 8.997 test SE 9.135\n",
      "Model saved!\n",
      "Epoch: 64 time 0:01:24.406670 train SE 8.938 test SE 9.125\n",
      "Epoch: 65 time 0:01:23.661585 train SE 8.950 test SE 9.134\n",
      "Epoch: 66 time 0:01:23.734883 train SE 8.917 test SE 9.044\n",
      "Epoch: 67 time 0:01:23.789723 train SE 8.922 test SE 9.125\n",
      "Epoch: 68 time 0:01:24.484928 train SE 8.945 test SE 9.030\n",
      "Epoch: 69 time 0:01:24.696890 train SE 8.915 test SE 8.950\n",
      "Epoch: 70 time 0:01:23.548058 train SE 8.971 test SE 9.007\n",
      "Epoch: 71 time 0:01:23.605315 train SE 8.877 test SE 9.003\n",
      "Epoch: 72 time 0:01:24.295947 train SE 8.884 test SE 9.041\n",
      "Epoch: 73 time 0:01:23.657112 train SE 8.903 test SE 8.923\n",
      "Epoch: 74 time 0:01:23.345180 train SE 8.900 test SE 9.001\n",
      "Epoch: 75 time 0:01:23.292543 train SE 8.917 test SE 9.059\n",
      "Epoch: 76 time 0:01:23.042747 train SE 8.914 test SE 8.981\n",
      "Epoch: 77 time 0:01:19.483270 train SE 8.888 test SE 8.859\n",
      "Epoch: 78 time 0:01:23.496985 train SE 8.884 test SE 8.898\n",
      "Epoch: 79 time 0:01:24.080913 train SE 8.863 test SE 8.850\n",
      "Epoch: 80 time 0:01:24.102025 train SE 8.789 test SE 8.915\n",
      "Epoch: 81 time 0:01:23.801217 train SE 8.749 test SE 8.901\n",
      "Epoch: 82 time 0:01:23.797187 train SE 8.752 test SE 8.898\n",
      "Epoch: 83 time 0:01:23.491192 train SE 8.744 test SE 8.774\n",
      "Epoch: 84 time 0:01:24.185717 train SE 8.734 test SE 8.995\n",
      "Epoch: 85 time 0:01:23.318106 train SE 8.766 test SE 8.913\n",
      "Epoch: 86 time 0:01:24.013726 train SE 8.713 test SE 8.869\n",
      "Epoch: 87 time 0:01:23.171893 train SE 8.611 test SE 8.765\n",
      "Epoch: 88 time 0:01:23.881490 train SE 8.624 test SE 8.708\n",
      "Epoch: 89 time 0:01:23.481601 train SE 8.567 test SE 8.746\n",
      "Epoch: 90 time 0:01:23.882036 train SE 8.626 test SE 8.708\n",
      "Epoch: 91 time 0:01:23.229710 train SE 8.635 test SE 8.750\n",
      "Epoch: 92 time 0:01:24.409501 train SE 8.654 test SE 8.725\n",
      "Epoch: 93 time 0:01:24.092929 train SE 8.529 test SE 8.629\n",
      "Epoch: 94 time 0:01:23.117212 train SE 8.473 test SE 8.557\n",
      "Epoch: 95 time 0:01:23.714478 train SE 8.495 test SE 8.698\n",
      "Epoch: 96 time 0:01:23.791790 train SE 8.493 test SE 8.720\n",
      "Epoch: 97 time 0:01:24.637012 train SE 8.502 test SE 8.538\n",
      "Epoch: 98 time 0:01:23.660626 train SE 8.479 test SE 8.714\n",
      "Epoch: 99 time 0:01:23.877484 train SE 8.464 test SE 8.675\n",
      "Epoch: 100 time 0:01:23.793284 train SE 8.531 test SE 8.697\n",
      "Epoch: 101 time 0:01:23.112918 train SE 8.548 test SE 8.671\n",
      "Epoch: 102 time 0:01:22.876762 train SE 8.574 test SE 8.739\n",
      "Epoch: 103 time 0:01:23.601950 train SE 8.563 test SE 8.743\n",
      "Epoch: 104 time 0:01:23.455695 train SE 8.569 test SE 8.792\n",
      "Epoch: 105 time 0:01:23.044764 train SE 8.597 test SE 8.806\n",
      "Epoch: 106 time 0:01:23.470706 train SE 8.612 test SE 8.759\n",
      "Epoch: 107 time 0:01:23.656662 train SE 8.675 test SE 8.793\n",
      "Epoch: 108 time 0:01:23.908483 train SE 8.656 test SE 8.760\n",
      "Epoch: 109 time 0:01:23.831139 train SE 8.648 test SE 8.839\n",
      "Epoch: 110 time 0:01:23.195294 train SE 8.663 test SE 8.676\n",
      "Epoch: 111 time 0:01:24.042114 train SE 8.573 test SE 8.759\n",
      "Epoch: 112 time 0:01:23.583800 train SE 8.565 test SE 8.703\n",
      "Epoch: 113 time 0:01:23.574919 train SE 8.627 test SE 8.839\n",
      "Epoch: 114 time 0:01:23.710080 train SE 8.671 test SE 8.842\n",
      "Epoch: 115 time 0:01:23.097141 train SE 8.654 test SE 8.924\n",
      "Epoch: 116 time 0:01:23.599281 train SE 8.666 test SE 8.921\n",
      "Epoch: 117 time 0:01:24.003636 train SE 8.671 test SE 8.832\n",
      "Epoch: 118 time 0:01:23.631684 train SE 8.660 test SE 8.747\n",
      "Epoch: 119 time 0:01:23.292605 train SE 8.712 test SE 8.827\n",
      "Epoch: 120 time 0:01:23.707593 train SE 8.712 test SE 8.920\n",
      "Epoch: 121 time 0:01:22.568902 train SE 8.730 test SE 8.937\n",
      "Epoch: 122 time 0:01:24.425574 train SE 8.719 test SE 8.974\n",
      "Epoch: 123 time 0:01:23.354017 train SE 8.722 test SE 8.953\n",
      "Epoch: 124 time 0:01:24.616976 train SE 8.671 test SE 8.887\n",
      "Epoch: 125 time 0:01:25.144141 train SE 8.727 test SE 8.978\n",
      "Epoch: 126 time 0:01:24.598642 train SE 8.706 test SE 8.945\n",
      "Epoch: 127 time 0:01:24.701314 train SE 8.676 test SE 9.005\n",
      "Epoch: 128 time 0:01:24.601098 train SE 8.683 test SE 8.758\n",
      "Epoch: 129 time 0:01:24.024605 train SE 8.680 test SE 8.923\n",
      "Epoch: 130 time 0:01:23.611768 train SE 8.682 test SE 8.772\n",
      "Epoch: 131 time 0:01:24.131777 train SE 8.729 test SE 8.822\n",
      "Epoch: 132 time 0:01:23.694472 train SE 8.673 test SE 8.717\n",
      "Epoch: 133 time 0:01:23.436218 train SE 8.686 test SE 8.780\n",
      "Epoch: 134 time 0:01:23.335459 train SE 8.680 test SE 8.933\n",
      "Epoch: 135 time 0:01:24.041057 train SE 8.693 test SE 8.763\n",
      "Epoch: 136 time 0:01:23.191886 train SE 8.692 test SE 8.935\n",
      "Epoch: 137 time 0:01:23.359962 train SE 8.664 test SE 8.959\n",
      "Epoch: 138 time 0:01:23.379371 train SE 8.628 test SE 8.785\n",
      "Epoch: 139 time 0:01:23.545417 train SE 8.616 test SE 8.777\n",
      "Epoch: 140 time 0:01:22.057809 train SE 8.664 test SE 8.596\n",
      "Epoch: 141 time 0:01:22.461906 train SE 8.715 test SE 8.816\n",
      "Epoch: 142 time 0:01:23.348452 train SE 8.709 test SE 8.955\n",
      "Epoch: 143 time 0:01:23.480569 train SE 8.733 test SE 8.987\n",
      "Epoch: 144 time 0:01:23.215168 train SE 8.763 test SE 8.950\n",
      "Epoch: 145 time 0:01:23.643217 train SE 8.678 test SE 8.831\n",
      "Epoch: 146 time 0:01:23.654651 train SE 8.662 test SE 8.669\n",
      "Epoch: 147 time 0:01:23.516003 train SE 8.671 test SE 8.591\n",
      "Epoch: 148 time 0:01:23.587780 train SE 8.713 test SE 8.599\n",
      "Epoch: 149 time 0:01:23.326616 train SE 8.694 test SE 8.797\n",
      "Epoch: 150 time 0:01:23.028748 train SE 8.695 test SE 8.411\n",
      "Epoch: 151 time 0:01:23.783806 train SE 8.688 test SE 8.611\n",
      "Epoch: 152 time 0:01:24.075522 train SE 8.683 test SE 8.501\n",
      "Epoch: 153 time 0:01:24.609512 train SE 8.647 test SE 8.749\n",
      "Epoch: 154 time 0:01:24.269453 train SE 8.673 test SE 8.810\n",
      "Epoch: 155 time 0:01:23.618209 train SE 8.638 test SE 8.511\n",
      "Epoch: 156 time 0:01:23.232265 train SE 8.668 test SE 8.572\n",
      "Epoch: 157 time 0:01:23.691704 train SE 8.701 test SE 8.740\n",
      "Epoch: 158 time 0:01:22.840386 train SE 8.681 test SE 8.663\n",
      "Epoch: 159 time 0:01:23.274232 train SE 8.687 test SE 8.685\n",
      "Epoch: 160 time 0:01:23.128406 train SE 8.673 test SE 8.429\n",
      "Epoch: 161 time 0:01:23.310517 train SE 8.663 test SE 8.383\n",
      "Epoch: 162 time 0:01:23.668706 train SE 8.649 test SE 8.385\n",
      "Epoch: 163 time 0:01:23.290563 train SE 8.636 test SE 8.405\n",
      "Epoch: 164 time 0:01:23.689091 train SE 8.645 test SE 8.710\n",
      "Epoch: 165 time 0:01:23.992658 train SE 8.665 test SE 8.282\n",
      "Epoch: 166 time 0:01:24.487047 train SE 8.637 test SE 8.332\n",
      "Epoch: 167 time 0:01:23.914460 train SE 8.678 test SE 8.371\n",
      "Epoch: 168 time 0:01:23.575286 train SE 8.676 test SE 8.366\n",
      "Epoch: 169 time 0:01:23.875610 train SE 8.673 test SE 8.634\n",
      "Epoch: 170 time 0:01:23.485019 train SE 8.661 test SE 8.347\n",
      "Epoch: 171 time 0:01:24.670924 train SE 8.637 test SE 8.430\n",
      "Epoch: 172 time 0:01:24.529808 train SE 8.643 test SE 8.343\n",
      "Epoch: 173 time 0:01:26.172445 train SE 8.646 test SE 8.494\n",
      "Epoch: 174 time 0:01:23.361823 train SE 8.633 test SE 8.278\n",
      "Epoch: 175 time 0:01:23.309433 train SE 8.616 test SE 8.241\n",
      "Epoch: 176 time 0:01:24.561283 train SE 8.636 test SE 8.462\n",
      "Epoch: 177 time 0:01:20.165394 train SE 8.615 test SE 8.346\n",
      "Epoch: 178 time 0:01:23.759339 train SE 8.586 test SE 8.298\n",
      "Epoch: 179 time 0:01:23.818684 train SE 8.604 test SE 8.717\n",
      "The best SE is: 9.135\n",
      "[4.50110769 5.57772398 6.89131308 7.05752182 7.11174345 7.24315405\n",
      " 7.21049976 7.64280796 7.81569672 7.91571665 8.00855923 7.9894495\n",
      " 8.10433483 8.23943996 8.30395317 8.28432274 8.49678707 8.44748116\n",
      " 8.51448345 8.50432777 8.33179379 8.55126476 8.47591782 8.57773113\n",
      " 8.55596447 8.58220673 8.49114513 8.57207394 8.62119865 8.69861412\n",
      " 8.69540215 8.67301273 8.55648804 8.67686462 8.65997601 8.61006451\n",
      " 8.55340862 8.68587875 8.5367775  8.62168026 8.6579752  8.67508221\n",
      " 8.62772465 8.61808681 8.70985222 8.7580471  8.57439232 8.67380524\n",
      " 8.6381216  8.79617691 8.71977711 8.72011089 8.76474571 8.77336502\n",
      " 8.67156696 8.69870949 8.78682041 8.87379551 8.95351791 8.95465565\n",
      " 9.05878067 9.03143215 9.05756283 9.1353054  9.12540817 9.134058\n",
      " 9.04372692 9.1249752  9.0295887  8.94966698 9.00694466 9.00313473\n",
      " 9.04053402 8.92320919 9.00085735 9.0586462  8.98074722 8.85866833\n",
      " 8.8981657  8.84985256 8.91469479 8.90100098 8.89781952 8.77411556\n",
      " 8.99547291 8.91333961 8.86873913 8.76503658 8.70830822 8.74622631\n",
      " 8.70755291 8.74962521 8.72456264 8.62868309 8.55681324 8.69838715\n",
      " 8.72044277 8.53833008 8.71389198 8.67534065 8.6974659  8.67111301\n",
      " 8.73853874 8.74255562 8.79182053 8.80629921 8.75921154 8.79314518\n",
      " 8.76043606 8.83880138 8.67599392 8.75873852 8.70319176 8.83902931\n",
      " 8.84179592 8.9243927  8.92109489 8.83173943 8.74694729 8.827034\n",
      " 8.920084   8.93734455 8.97441101 8.95345783 8.88671303 8.97835732\n",
      " 8.94471264 9.00456429 8.7579422  8.92252636 8.7721014  8.82168293\n",
      " 8.7165823  8.77984142 8.93310928 8.76289272 8.93474388 8.95944214\n",
      " 8.78508759 8.77740288 8.59597969 8.81645012 8.95530224 8.98729706\n",
      " 8.95022869 8.83090115 8.66902256 8.59083748 8.59918976 8.79738808\n",
      " 8.41050243 8.61079979 8.50136471 8.74910641 8.80986881 8.51082325\n",
      " 8.57213593 8.73968983 8.66345978 8.68520451 8.42857552 8.38327694\n",
      " 8.385355   8.40537071 8.70972919 8.28163147 8.33198833 8.37116146\n",
      " 8.36551571 8.63433075 8.34684181 8.4298172  8.34345913 8.49401093\n",
      " 8.27844906 8.24082565 8.46190548 8.34635544 8.29826736 8.71693707]\n"
     ]
    }
   ],
   "source": [
    "B = 8\n",
    "train(Nc,N,Nt,B,Nr,L,SNR_dB,K,EPOCH,BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([204800, 2, 32, 128])\n",
      "DNN_BS_hyb_OFDM(\n",
      "  (DQL): DequantizationLayer()\n",
      "  (FC1): Linear(in_features=32, out_features=2048, bias=True)\n",
      "  (bn1): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (mish1): Mish()\n",
      "  (FC2): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "  (bn2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (mish2): Mish()\n",
      "  (FC3): Linear(in_features=1024, out_features=384, bias=True)\n",
      "  (bn3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (mish3): Mish()\n",
      "  (res1): RES_BLOCK(\n",
      "    (conv1): Sequential(\n",
      "      (0): Conv2d(8, 256, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0))\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Mish()\n",
      "    )\n",
      "    (conv2): Sequential(\n",
      "      (0): Conv2d(256, 512, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0))\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Mish()\n",
      "    )\n",
      "    (conv3): Sequential(\n",
      "      (0): Conv2d(512, 8, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0))\n",
      "      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (conv): Conv2d(8, 8, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0))\n",
      ")\n",
      "DNN_US_RF_OFDM(\n",
      "  (pilot): Linear(in_features=64, out_features=8, bias=False)\n",
      "  (res): RES_BLOCK(\n",
      "    (conv1): Sequential(\n",
      "      (0): Conv2d(16, 256, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0))\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Mish()\n",
      "    )\n",
      "    (conv2): Sequential(\n",
      "      (0): Conv2d(256, 512, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0))\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Mish()\n",
      "    )\n",
      "    (conv3): Sequential(\n",
      "      (0): Conv2d(512, 16, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0))\n",
      "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (FC2): Linear(in_features=512, out_features=1024, bias=True)\n",
      "  (bn2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu2): ReLU()\n",
      "  (FC3): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (bn3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu3): ReLU()\n",
      "  (FC4): Linear(in_features=512, out_features=16, bias=True)\n",
      "  (bn4): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (QL): QuantizationLayer()\n",
      ")\n",
      "Epoch: 0 time 0:01:23.697987 train SE 5.250 test SE 6.140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:256: UserWarning: Couldn't retrieve source code for container of type DNN_US_RF_OFDM. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:256: UserWarning: Couldn't retrieve source code for container of type RES_BLOCK. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:256: UserWarning: Couldn't retrieve source code for container of type Mish. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:256: UserWarning: Couldn't retrieve source code for container of type QuantizationLayer. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:256: UserWarning: Couldn't retrieve source code for container of type DNN_BS_hyb_OFDM. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:256: UserWarning: Couldn't retrieve source code for container of type DequantizationLayer. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved!\n",
      "Epoch: 1 time 0:01:25.004606 train SE 6.767 test SE 7.384\n",
      "Model saved!\n",
      "Epoch: 2 time 0:01:24.078100 train SE 7.654 test SE 8.097\n",
      "Model saved!\n",
      "Epoch: 3 time 0:01:22.987244 train SE 8.236 test SE 8.577\n",
      "Model saved!\n",
      "Epoch: 4 time 0:01:23.102617 train SE 8.489 test SE 8.679\n",
      "Model saved!\n",
      "Epoch: 5 time 0:01:24.099013 train SE 8.651 test SE 8.808\n",
      "Model saved!\n",
      "Epoch: 6 time 0:01:23.402430 train SE 8.703 test SE 8.856\n",
      "Model saved!\n",
      "Epoch: 7 time 0:01:26.141999 train SE 8.743 test SE 8.894\n",
      "Model saved!\n",
      "Epoch: 8 time 0:01:25.814849 train SE 8.481 test SE 8.417\n",
      "Epoch: 9 time 0:01:24.879911 train SE 8.386 test SE 8.190\n",
      "Epoch: 10 time 0:01:29.152001 train SE 8.446 test SE 8.548\n",
      "Epoch: 11 time 0:01:23.507045 train SE 8.530 test SE 8.672\n",
      "Epoch: 12 time 0:01:23.372888 train SE 8.684 test SE 8.936\n",
      "Model saved!\n",
      "Epoch: 13 time 0:01:22.994481 train SE 8.945 test SE 9.124\n",
      "Model saved!\n",
      "Epoch: 14 time 0:01:24.315745 train SE 8.988 test SE 9.125\n",
      "Model saved!\n",
      "Epoch: 15 time 0:01:29.048164 train SE 9.035 test SE 9.220\n",
      "Model saved!\n",
      "Epoch: 16 time 0:01:23.584856 train SE 9.082 test SE 9.240\n",
      "Model saved!\n",
      "Epoch: 17 time 0:01:23.747364 train SE 9.066 test SE 9.364\n",
      "Model saved!\n",
      "Epoch: 18 time 0:01:23.567295 train SE 9.129 test SE 9.406\n",
      "Model saved!\n",
      "Epoch: 19 time 0:01:24.095538 train SE 9.125 test SE 9.191\n",
      "Epoch: 20 time 0:01:23.345762 train SE 9.271 test SE 9.377\n",
      "Epoch: 21 time 0:01:23.232762 train SE 9.301 test SE 9.480\n",
      "Model saved!\n",
      "Epoch: 22 time 0:01:23.759783 train SE 9.057 test SE 9.032\n",
      "Epoch: 23 time 0:01:23.578865 train SE 9.109 test SE 9.424\n",
      "Epoch: 24 time 0:01:24.635550 train SE 9.362 test SE 9.312\n",
      "Epoch: 25 time 0:01:24.826435 train SE 8.915 test SE 8.977\n",
      "Epoch: 26 time 0:01:24.420132 train SE 8.859 test SE 8.859\n",
      "Epoch: 27 time 0:01:24.316855 train SE 8.946 test SE 9.332\n",
      "Epoch: 28 time 0:01:23.443181 train SE 9.054 test SE 9.198\n",
      "Epoch: 29 time 0:01:24.864342 train SE 9.223 test SE 9.506\n",
      "Model saved!\n",
      "Epoch: 30 time 0:01:24.655976 train SE 9.414 test SE 9.691\n",
      "Model saved!\n",
      "Epoch: 31 time 0:01:24.737718 train SE 9.489 test SE 9.576\n",
      "Epoch: 32 time 0:01:20.301559 train SE 9.498 test SE 9.667\n",
      "Epoch: 33 time 0:01:24.040635 train SE 9.488 test SE 9.637\n",
      "Epoch: 34 time 0:01:22.987023 train SE 9.395 test SE 9.740\n",
      "Model saved!\n",
      "Epoch: 35 time 0:01:21.783263 train SE 9.574 test SE 9.710\n",
      "Epoch: 36 time 0:01:23.987884 train SE 9.557 test SE 9.517\n",
      "Epoch: 37 time 0:01:25.134189 train SE 9.395 test SE 9.643\n",
      "Epoch: 38 time 0:01:23.848064 train SE 9.460 test SE 9.698\n",
      "Epoch: 39 time 0:01:22.965478 train SE 9.528 test SE 9.670\n",
      "Epoch: 40 time 0:01:24.573254 train SE 9.550 test SE 9.675\n",
      "Epoch: 41 time 0:01:23.971350 train SE 9.383 test SE 9.514\n",
      "Epoch: 42 time 0:01:23.638281 train SE 9.140 test SE 9.112\n",
      "Epoch: 43 time 0:01:24.709890 train SE 9.081 test SE 9.278\n",
      "Epoch: 44 time 0:01:24.823411 train SE 9.228 test SE 9.576\n",
      "Epoch: 45 time 0:01:21.909481 train SE 9.337 test SE 9.636\n",
      "Epoch: 46 time 0:01:22.948653 train SE 9.377 test SE 9.513\n",
      "Epoch: 47 time 0:01:24.738851 train SE 9.344 test SE 9.619\n",
      "Epoch: 48 time 0:01:25.434895 train SE 9.471 test SE 9.562\n",
      "Epoch: 49 time 0:01:23.538979 train SE 9.544 test SE 9.842\n",
      "Model saved!\n",
      "Epoch: 50 time 0:01:24.347801 train SE 9.716 test SE 10.022\n",
      "Model saved!\n",
      "Epoch: 51 time 0:01:24.092937 train SE 9.735 test SE 9.783\n",
      "Epoch: 52 time 0:01:24.539223 train SE 9.748 test SE 9.893\n",
      "Epoch: 53 time 0:01:23.997781 train SE 9.689 test SE 9.973\n",
      "Epoch: 54 time 0:01:23.460661 train SE 9.868 test SE 9.995\n",
      "Epoch: 55 time 0:01:24.038048 train SE 9.775 test SE 9.984\n",
      "Epoch: 56 time 0:01:23.958905 train SE 9.717 test SE 9.913\n",
      "Epoch: 57 time 0:01:24.400264 train SE 9.762 test SE 9.959\n",
      "Epoch: 58 time 0:01:23.770775 train SE 9.800 test SE 9.999\n",
      "Epoch: 59 time 0:01:28.689178 train SE 9.776 test SE 9.919\n",
      "Epoch: 60 time 0:01:23.295630 train SE 9.764 test SE 9.932\n",
      "Epoch: 61 time 0:01:23.399497 train SE 9.799 test SE 10.049\n",
      "Model saved!\n",
      "Epoch: 62 time 0:01:28.793466 train SE 9.916 test SE 10.121\n",
      "Model saved!\n",
      "Epoch: 63 time 0:01:23.577029 train SE 9.967 test SE 10.203\n",
      "Model saved!\n",
      "Epoch: 64 time 0:01:28.279424 train SE 9.994 test SE 10.177\n",
      "Epoch: 65 time 0:01:23.425770 train SE 9.983 test SE 10.215\n",
      "Model saved!\n",
      "Epoch: 66 time 0:01:27.763151 train SE 9.945 test SE 10.084\n",
      "Epoch: 67 time 0:01:27.861636 train SE 9.970 test SE 10.177\n",
      "Epoch: 68 time 0:01:26.546384 train SE 9.943 test SE 10.042\n",
      "Epoch: 69 time 0:01:27.048520 train SE 9.868 test SE 10.030\n",
      "Epoch: 70 time 0:01:24.400679 train SE 9.773 test SE 9.939\n",
      "Epoch: 71 time 0:01:25.724060 train SE 9.803 test SE 10.071\n",
      "Epoch: 72 time 0:01:24.570650 train SE 9.910 test SE 10.093\n",
      "Epoch: 73 time 0:01:23.822173 train SE 9.917 test SE 10.163\n",
      "Epoch: 74 time 0:01:25.292788 train SE 9.878 test SE 10.059\n",
      "Epoch: 75 time 0:01:25.393061 train SE 9.808 test SE 9.917\n",
      "Epoch: 76 time 0:01:24.675372 train SE 9.748 test SE 10.009\n",
      "Epoch: 77 time 0:01:24.698485 train SE 9.685 test SE 9.816\n",
      "Epoch: 78 time 0:01:24.458982 train SE 9.641 test SE 9.867\n",
      "Epoch: 79 time 0:01:25.257886 train SE 9.734 test SE 9.943\n",
      "Epoch: 80 time 0:01:23.286130 train SE 9.767 test SE 9.974\n",
      "Epoch: 81 time 0:01:23.891048 train SE 9.760 test SE 9.969\n",
      "Epoch: 82 time 0:01:23.729894 train SE 9.757 test SE 10.012\n",
      "Epoch: 83 time 0:01:24.211659 train SE 9.754 test SE 9.947\n",
      "Epoch: 84 time 0:01:24.291962 train SE 9.666 test SE 9.815\n",
      "Epoch: 85 time 0:01:23.295691 train SE 9.649 test SE 9.883\n",
      "Epoch: 86 time 0:01:23.152540 train SE 9.757 test SE 9.899\n",
      "Epoch: 87 time 0:01:24.390735 train SE 9.769 test SE 9.930\n",
      "Epoch: 88 time 0:01:24.411702 train SE 9.713 test SE 9.879\n",
      "Epoch: 89 time 0:01:24.541780 train SE 9.490 test SE 9.677\n",
      "Epoch: 90 time 0:01:24.162211 train SE 9.575 test SE 9.819\n",
      "Epoch: 91 time 0:01:24.828139 train SE 9.647 test SE 9.778\n",
      "Epoch: 92 time 0:01:25.491817 train SE 9.683 test SE 9.863\n",
      "Epoch: 93 time 0:01:24.041025 train SE 9.700 test SE 9.898\n",
      "Epoch: 94 time 0:01:23.757946 train SE 9.686 test SE 9.910\n",
      "Epoch: 95 time 0:01:23.022823 train SE 9.732 test SE 10.007\n",
      "Epoch: 96 time 0:01:24.527753 train SE 9.734 test SE 10.061\n",
      "Epoch: 97 time 0:01:23.426887 train SE 9.801 test SE 9.974\n",
      "Epoch: 98 time 0:01:24.872797 train SE 9.816 test SE 10.081\n",
      "Epoch: 99 time 0:01:23.806815 train SE 9.900 test SE 10.065\n",
      "Epoch: 100 time 0:01:23.438180 train SE 10.008 test SE 10.243\n",
      "Model saved!\n",
      "Epoch: 101 time 0:01:23.540968 train SE 10.014 test SE 10.244\n",
      "Model saved!\n",
      "Epoch: 102 time 0:01:25.413327 train SE 10.045 test SE 10.194\n",
      "Epoch: 103 time 0:01:26.292139 train SE 10.012 test SE 10.215\n",
      "Epoch: 104 time 0:01:24.468011 train SE 10.008 test SE 10.208\n",
      "Epoch: 105 time 0:01:23.850129 train SE 9.940 test SE 10.094\n",
      "Epoch: 106 time 0:01:25.695966 train SE 9.915 test SE 10.142\n",
      "Epoch: 107 time 0:01:23.560076 train SE 9.912 test SE 10.050\n",
      "Epoch: 108 time 0:01:25.025500 train SE 9.827 test SE 10.008\n",
      "Epoch: 109 time 0:01:25.283227 train SE 9.843 test SE 10.076\n",
      "Epoch: 110 time 0:01:25.083344 train SE 9.875 test SE 10.076\n",
      "Epoch: 111 time 0:01:25.816407 train SE 9.779 test SE 9.928\n",
      "Epoch: 112 time 0:01:24.563208 train SE 9.754 test SE 9.968\n",
      "Epoch: 113 time 0:01:25.636981 train SE 9.692 test SE 9.909\n",
      "Epoch: 114 time 0:01:24.691967 train SE 9.697 test SE 9.905\n",
      "Epoch: 115 time 0:01:24.189753 train SE 9.697 test SE 9.918\n",
      "Epoch: 116 time 0:01:24.125414 train SE 9.799 test SE 10.075\n",
      "Epoch: 117 time 0:01:23.095214 train SE 9.869 test SE 10.072\n",
      "Epoch: 118 time 0:01:24.236167 train SE 9.873 test SE 10.096\n",
      "Epoch: 119 time 0:01:23.402719 train SE 9.941 test SE 10.179\n",
      "Epoch: 120 time 0:01:24.241222 train SE 9.983 test SE 10.227\n",
      "Epoch: 121 time 0:01:23.967455 train SE 10.012 test SE 10.228\n",
      "Epoch: 122 time 0:01:22.899147 train SE 10.024 test SE 10.201\n",
      "Epoch: 123 time 0:01:24.537294 train SE 10.027 test SE 10.241\n",
      "Epoch: 124 time 0:01:24.526509 train SE 10.007 test SE 10.225\n",
      "Epoch: 125 time 0:01:23.402305 train SE 10.013 test SE 10.229\n",
      "Epoch: 126 time 0:01:23.244676 train SE 9.948 test SE 10.174\n",
      "Epoch: 127 time 0:01:23.455206 train SE 9.933 test SE 10.180\n",
      "Epoch: 128 time 0:01:24.009846 train SE 9.917 test SE 10.106\n",
      "Epoch: 129 time 0:01:24.411060 train SE 9.848 test SE 10.076\n",
      "Epoch: 130 time 0:01:24.112334 train SE 9.928 test SE 10.168\n",
      "Epoch: 131 time 0:01:23.400877 train SE 9.872 test SE 10.064\n",
      "Epoch: 132 time 0:01:24.175650 train SE 9.942 test SE 10.180\n",
      "Epoch: 133 time 0:01:23.315545 train SE 9.937 test SE 10.179\n",
      "Epoch: 134 time 0:01:22.882680 train SE 9.852 test SE 10.058\n",
      "Epoch: 135 time 0:01:23.990274 train SE 9.812 test SE 10.113\n",
      "Epoch: 136 time 0:01:24.527780 train SE 9.877 test SE 10.160\n",
      "Epoch: 137 time 0:01:24.370784 train SE 9.779 test SE 9.942\n",
      "Epoch: 138 time 0:01:23.438198 train SE 9.701 test SE 9.978\n",
      "Epoch: 139 time 0:01:24.194252 train SE 9.757 test SE 9.999\n",
      "Epoch: 140 time 0:01:23.605320 train SE 9.711 test SE 9.884\n",
      "Epoch: 141 time 0:01:24.207704 train SE 9.748 test SE 9.972\n",
      "Epoch: 142 time 0:01:24.402173 train SE 9.807 test SE 10.073\n",
      "Epoch: 143 time 0:01:24.564640 train SE 9.867 test SE 10.113\n",
      "Epoch: 144 time 0:01:23.581323 train SE 9.857 test SE 10.125\n",
      "Epoch: 145 time 0:01:25.085268 train SE 9.912 test SE 10.206\n",
      "Epoch: 146 time 0:01:24.225623 train SE 9.995 test SE 10.297\n",
      "Model saved!\n",
      "Epoch: 147 time 0:01:28.358502 train SE 10.076 test SE 10.322\n",
      "Model saved!\n",
      "Epoch: 148 time 0:01:28.280773 train SE 10.062 test SE 10.285\n",
      "Epoch: 149 time 0:01:27.815726 train SE 10.072 test SE 10.346\n",
      "Model saved!\n",
      "Epoch: 150 time 0:01:23.501639 train SE 10.193 test SE 10.478\n",
      "Model saved!\n",
      "Epoch: 151 time 0:01:27.101055 train SE 10.239 test SE 10.473\n",
      "Epoch: 152 time 0:01:26.727922 train SE 10.236 test SE 10.468\n",
      "Epoch: 153 time 0:01:23.370298 train SE 10.225 test SE 10.467\n",
      "Epoch: 154 time 0:01:26.466149 train SE 10.229 test SE 10.465\n",
      "Epoch: 155 time 0:01:25.657273 train SE 10.246 test SE 10.493\n",
      "Model saved!\n",
      "Epoch: 156 time 0:01:24.045621 train SE 10.254 test SE 10.484\n",
      "Epoch: 157 time 0:01:23.902898 train SE 10.253 test SE 10.468\n",
      "Epoch: 158 time 0:01:23.444790 train SE 10.218 test SE 10.447\n",
      "Epoch: 159 time 0:01:26.503503 train SE 10.199 test SE 10.426\n",
      "Epoch: 160 time 0:01:23.411776 train SE 10.189 test SE 10.427\n",
      "Epoch: 161 time 0:01:23.636351 train SE 10.167 test SE 10.369\n",
      "Epoch: 162 time 0:01:26.619739 train SE 10.081 test SE 10.325\n",
      "Epoch: 163 time 0:01:24.768595 train SE 10.059 test SE 10.258\n",
      "Epoch: 164 time 0:01:27.355259 train SE 10.050 test SE 10.312\n",
      "Epoch: 165 time 0:01:24.481492 train SE 10.067 test SE 10.322\n",
      "Epoch: 166 time 0:01:23.294993 train SE 10.087 test SE 10.369\n",
      "Epoch: 167 time 0:01:25.582130 train SE 10.113 test SE 10.386\n",
      "Epoch: 168 time 0:01:24.309887 train SE 10.143 test SE 10.374\n",
      "Epoch: 169 time 0:01:24.874826 train SE 10.123 test SE 10.366\n",
      "Epoch: 170 time 0:01:24.578179 train SE 10.123 test SE 10.384\n",
      "Epoch: 171 time 0:01:23.725539 train SE 10.161 test SE 10.387\n",
      "Epoch: 172 time 0:01:23.970308 train SE 10.145 test SE 10.400\n",
      "Epoch: 173 time 0:01:24.057461 train SE 10.148 test SE 10.386\n",
      "Epoch: 174 time 0:01:23.665030 train SE 10.203 test SE 10.443\n",
      "Epoch: 175 time 0:01:23.495073 train SE 10.207 test SE 10.430\n",
      "Epoch: 176 time 0:01:23.978176 train SE 10.206 test SE 10.429\n",
      "Epoch: 177 time 0:01:25.793855 train SE 10.199 test SE 10.511\n",
      "Model saved!\n",
      "Epoch: 178 time 0:01:25.356194 train SE 10.238 test SE 10.488\n",
      "Epoch: 179 time 0:01:25.876630 train SE 10.224 test SE 10.506\n",
      "The best SE is: 10.511\n",
      "[ 6.14011145  7.38350773  8.09667492  8.57660103  8.67854977  8.80778122\n",
      "  8.85586452  8.8938427   8.41691303  8.1897974   8.54757977  8.67245388\n",
      "  8.93639088  9.12403584  9.12466717  9.21999264  9.24023628  9.36402512\n",
      "  9.4063406   9.19081879  9.37683392  9.48048878  9.03155994  9.42355919\n",
      "  9.3116293   8.97710991  8.85947323  9.33178616  9.19849205  9.50575542\n",
      "  9.69147015  9.57591534  9.66669655  9.63714981  9.74003124  9.71000767\n",
      "  9.51679611  9.64296436  9.69755459  9.67003441  9.67476654  9.51426888\n",
      "  9.1124649   9.27786827  9.5757885   9.63622952  9.51272583  9.6191988\n",
      "  9.56178856  9.84186649 10.02204418  9.78263664  9.89325333  9.97264385\n",
      "  9.9950695   9.98420143  9.91309261  9.9593153   9.99890423  9.91912174\n",
      "  9.93211651 10.04859924 10.12149715 10.20335007 10.17725468 10.21460152\n",
      " 10.08389664 10.1773634  10.04154301 10.03032684  9.93870068 10.07069016\n",
      " 10.09347153 10.16335011 10.0587101   9.91653728 10.00895977  9.81568241\n",
      "  9.867136    9.94329357  9.97379684  9.96881771 10.01242065  9.94726086\n",
      "  9.81511784  9.88317871  9.89884377  9.93002892  9.87879658  9.67677593\n",
      "  9.81903362  9.77775574  9.86279774  9.89840889  9.9100132  10.00715446\n",
      " 10.0612421   9.97400761 10.08103561 10.06522274 10.24291897 10.2441473\n",
      " 10.19437504 10.21537304 10.2082634  10.09364223 10.14226532 10.04999161\n",
      " 10.0082922  10.0760498  10.07556629  9.92799568  9.9676981   9.90874386\n",
      "  9.90536022  9.9184742  10.07496548 10.07191181 10.09640598 10.17949677\n",
      " 10.22733116 10.22817898 10.20140076 10.24065304 10.22467327 10.22875118\n",
      " 10.17390442 10.17955303 10.10576344 10.07567501 10.16755199 10.06350422\n",
      " 10.18021202 10.17871857 10.05841732 10.11277962 10.16046047  9.94195652\n",
      "  9.97821045  9.99904156  9.88429165  9.9716692  10.07261181 10.11267281\n",
      " 10.12509441 10.20605755 10.29743481 10.32232952 10.28475094 10.34613323\n",
      " 10.47793579 10.47328663 10.46797466 10.46722317 10.46533108 10.49254799\n",
      " 10.48425674 10.46834564 10.44701099 10.42594624 10.42732811 10.36878014\n",
      " 10.32518864 10.25755215 10.3122406  10.32197475 10.36897182 10.38553619\n",
      " 10.37442207 10.36587811 10.38399124 10.387187   10.4000864  10.38633156\n",
      " 10.44250202 10.4301796  10.42897129 10.51085281 10.48825836 10.50609016]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "B = 16\n",
    "train(Nc,N,Nt,B,Nr,L,SNR_dB,K,EPOCH,BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([204800, 2, 32, 128])\n",
      "DNN_BS_hyb_OFDM(\n",
      "  (DQL): DequantizationLayer()\n",
      "  (FC1): Linear(in_features=48, out_features=2048, bias=True)\n",
      "  (bn1): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (mish1): Mish()\n",
      "  (FC2): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "  (bn2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (mish2): Mish()\n",
      "  (FC3): Linear(in_features=1024, out_features=384, bias=True)\n",
      "  (bn3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (mish3): Mish()\n",
      "  (res1): RES_BLOCK(\n",
      "    (conv1): Sequential(\n",
      "      (0): Conv2d(8, 256, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0))\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Mish()\n",
      "    )\n",
      "    (conv2): Sequential(\n",
      "      (0): Conv2d(256, 512, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0))\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Mish()\n",
      "    )\n",
      "    (conv3): Sequential(\n",
      "      (0): Conv2d(512, 8, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0))\n",
      "      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (conv): Conv2d(8, 8, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0))\n",
      ")\n",
      "DNN_US_RF_OFDM(\n",
      "  (pilot): Linear(in_features=64, out_features=8, bias=False)\n",
      "  (res): RES_BLOCK(\n",
      "    (conv1): Sequential(\n",
      "      (0): Conv2d(16, 256, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0))\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Mish()\n",
      "    )\n",
      "    (conv2): Sequential(\n",
      "      (0): Conv2d(256, 512, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0))\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Mish()\n",
      "    )\n",
      "    (conv3): Sequential(\n",
      "      (0): Conv2d(512, 16, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0))\n",
      "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (FC2): Linear(in_features=512, out_features=1024, bias=True)\n",
      "  (bn2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu2): ReLU()\n",
      "  (FC3): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (bn3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu3): ReLU()\n",
      "  (FC4): Linear(in_features=512, out_features=24, bias=True)\n",
      "  (bn4): BatchNorm1d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (QL): QuantizationLayer()\n",
      ")\n",
      "Epoch: 0 time 0:01:23.153435 train SE 4.663 test SE 5.296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:256: UserWarning: Couldn't retrieve source code for container of type DNN_US_RF_OFDM. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:256: UserWarning: Couldn't retrieve source code for container of type RES_BLOCK. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:256: UserWarning: Couldn't retrieve source code for container of type Mish. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:256: UserWarning: Couldn't retrieve source code for container of type QuantizationLayer. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:256: UserWarning: Couldn't retrieve source code for container of type DNN_BS_hyb_OFDM. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:256: UserWarning: Couldn't retrieve source code for container of type DequantizationLayer. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved!\n",
      "Epoch: 1 time 0:01:23.965753 train SE 6.216 test SE 6.897\n",
      "Model saved!\n",
      "Epoch: 2 time 0:01:23.775422 train SE 7.405 test SE 7.900\n",
      "Model saved!\n",
      "Epoch: 3 time 0:01:22.472863 train SE 8.197 test SE 8.468\n",
      "Model saved!\n",
      "Epoch: 4 time 0:01:20.060849 train SE 8.657 test SE 8.933\n",
      "Model saved!\n",
      "Epoch: 5 time 0:01:24.212206 train SE 9.062 test SE 9.213\n",
      "Model saved!\n",
      "Epoch: 6 time 0:01:24.282957 train SE 9.287 test SE 9.378\n",
      "Model saved!\n",
      "Epoch: 7 time 0:01:23.535898 train SE 9.441 test SE 9.481\n",
      "Model saved!\n",
      "Epoch: 8 time 0:01:23.783840 train SE 9.587 test SE 9.666\n",
      "Model saved!\n",
      "Epoch: 9 time 0:01:23.512017 train SE 9.716 test SE 9.792\n",
      "Model saved!\n",
      "Epoch: 10 time 0:01:23.930249 train SE 9.826 test SE 9.845\n",
      "Model saved!\n",
      "Epoch: 11 time 0:01:23.534443 train SE 9.909 test SE 9.963\n",
      "Model saved!\n",
      "Epoch: 12 time 0:01:24.058706 train SE 9.997 test SE 10.062\n",
      "Model saved!\n",
      "Epoch: 13 time 0:01:25.346585 train SE 10.081 test SE 10.093\n",
      "Model saved!\n",
      "Epoch: 14 time 0:01:24.103894 train SE 10.151 test SE 10.172\n",
      "Model saved!\n",
      "Epoch: 15 time 0:01:23.791801 train SE 10.194 test SE 10.243\n",
      "Model saved!\n",
      "Epoch: 16 time 0:01:23.508502 train SE 10.262 test SE 10.262\n",
      "Model saved!\n",
      "Epoch: 17 time 0:01:24.102369 train SE 10.323 test SE 10.351\n",
      "Model saved!\n",
      "Epoch: 18 time 0:01:24.242140 train SE 10.382 test SE 10.383\n",
      "Model saved!\n",
      "Epoch: 19 time 0:01:24.502501 train SE 10.438 test SE 10.449\n",
      "Model saved!\n",
      "Epoch: 20 time 0:01:23.494020 train SE 10.483 test SE 10.484\n",
      "Model saved!\n",
      "Epoch: 21 time 0:01:23.399752 train SE 10.531 test SE 10.528\n",
      "Model saved!\n",
      "Epoch: 22 time 0:01:23.876086 train SE 10.577 test SE 10.577\n",
      "Model saved!\n",
      "Epoch: 23 time 0:01:24.310941 train SE 10.627 test SE 10.640\n",
      "Model saved!\n",
      "Epoch: 24 time 0:01:25.329204 train SE 10.661 test SE 10.674\n",
      "Model saved!\n",
      "Epoch: 25 time 0:01:25.161615 train SE 10.711 test SE 10.675\n",
      "Model saved!\n",
      "Epoch: 26 time 0:01:24.377264 train SE 10.734 test SE 10.704\n",
      "Model saved!\n",
      "Epoch: 27 time 0:01:25.377014 train SE 10.775 test SE 10.748\n",
      "Model saved!\n",
      "Epoch: 28 time 0:01:25.617325 train SE 10.808 test SE 10.786\n",
      "Model saved!\n",
      "Epoch: 29 time 0:01:25.916743 train SE 10.831 test SE 10.791\n",
      "Model saved!\n",
      "Epoch: 30 time 0:01:24.059111 train SE 10.854 test SE 10.816\n",
      "Model saved!\n",
      "Epoch: 31 time 0:01:24.127739 train SE 10.869 test SE 10.830\n",
      "Model saved!\n",
      "Epoch: 32 time 0:01:23.833687 train SE 10.899 test SE 10.858\n",
      "Model saved!\n",
      "Epoch: 33 time 0:01:24.285097 train SE 10.923 test SE 10.882\n",
      "Model saved!\n",
      "Epoch: 34 time 0:01:23.983790 train SE 10.946 test SE 10.902\n",
      "Model saved!\n",
      "Epoch: 35 time 0:01:25.032429 train SE 10.964 test SE 10.915\n",
      "Model saved!\n",
      "Epoch: 36 time 0:01:24.229172 train SE 10.981 test SE 10.922\n",
      "Model saved!\n",
      "Epoch: 37 time 0:01:24.649454 train SE 11.000 test SE 10.962\n",
      "Model saved!\n",
      "Epoch: 38 time 0:01:24.735772 train SE 11.021 test SE 10.958\n",
      "Epoch: 39 time 0:01:23.843692 train SE 11.043 test SE 11.003\n",
      "Model saved!\n",
      "Epoch: 40 time 0:01:19.933487 train SE 11.059 test SE 10.993\n",
      "Epoch: 41 time 0:01:25.745549 train SE 11.058 test SE 11.003\n",
      "Model saved!\n",
      "Epoch: 42 time 0:01:24.519316 train SE 11.075 test SE 11.003\n",
      "Model saved!\n",
      "Epoch: 43 time 0:01:26.825683 train SE 11.094 test SE 11.026\n",
      "Model saved!\n",
      "Epoch: 44 time 0:01:26.064682 train SE 11.107 test SE 11.025\n",
      "Epoch: 45 time 0:01:23.214843 train SE 11.119 test SE 11.045\n",
      "Model saved!\n",
      "Epoch: 46 time 0:01:25.364122 train SE 11.130 test SE 11.028\n",
      "Epoch: 47 time 0:01:22.328458 train SE 11.144 test SE 11.048\n",
      "Model saved!\n",
      "Epoch: 48 time 0:01:25.629407 train SE 11.146 test SE 11.045\n",
      "Epoch: 49 time 0:01:24.004730 train SE 11.157 test SE 11.069\n",
      "Model saved!\n",
      "Epoch: 50 time 0:01:24.500555 train SE 11.160 test SE 11.075\n",
      "Model saved!\n",
      "Epoch: 51 time 0:01:24.143977 train SE 11.180 test SE 11.087\n",
      "Model saved!\n",
      "Epoch: 52 time 0:01:23.998759 train SE 11.190 test SE 11.108\n",
      "Model saved!\n",
      "Epoch: 53 time 0:01:25.652904 train SE 11.200 test SE 11.102\n",
      "Epoch: 54 time 0:01:25.119306 train SE 11.200 test SE 11.091\n",
      "Epoch: 55 time 0:01:23.799426 train SE 11.207 test SE 11.107\n",
      "Epoch: 56 time 0:01:24.575768 train SE 11.225 test SE 11.123\n",
      "Model saved!\n",
      "Epoch: 57 time 0:01:25.054882 train SE 11.233 test SE 11.126\n",
      "Model saved!\n",
      "Epoch: 58 time 0:01:24.246068 train SE 11.242 test SE 11.136\n",
      "Model saved!\n",
      "Epoch: 59 time 0:01:23.994234 train SE 11.254 test SE 11.134\n",
      "Epoch: 60 time 0:01:23.035741 train SE 11.263 test SE 11.141\n",
      "Model saved!\n",
      "Epoch: 61 time 0:01:24.305881 train SE 11.269 test SE 11.131\n",
      "Epoch: 62 time 0:01:22.932549 train SE 11.272 test SE 11.144\n",
      "Model saved!\n",
      "Epoch: 63 time 0:01:24.672463 train SE 11.293 test SE 11.159\n",
      "Model saved!\n",
      "Epoch: 64 time 0:01:24.987508 train SE 11.300 test SE 11.177\n",
      "Model saved!\n",
      "Epoch: 65 time 0:01:24.423650 train SE 11.298 test SE 11.179\n",
      "Model saved!\n",
      "Epoch: 66 time 0:01:24.481506 train SE 11.302 test SE 11.181\n",
      "Model saved!\n",
      "Epoch: 67 time 0:01:24.858413 train SE 11.310 test SE 11.161\n",
      "Epoch: 68 time 0:01:24.476900 train SE 11.315 test SE 11.184\n",
      "Model saved!\n",
      "Epoch: 69 time 0:01:23.838670 train SE 11.323 test SE 11.176\n",
      "Epoch: 70 time 0:01:23.880963 train SE 11.334 test SE 11.196\n",
      "Model saved!\n",
      "Epoch: 71 time 0:01:23.550988 train SE 11.327 test SE 11.197\n",
      "Model saved!\n",
      "Epoch: 72 time 0:01:24.285036 train SE 11.349 test SE 11.169\n",
      "Epoch: 73 time 0:01:24.575186 train SE 11.338 test SE 11.189\n",
      "Epoch: 74 time 0:01:24.302276 train SE 11.346 test SE 11.205\n",
      "Model saved!\n",
      "Epoch: 75 time 0:01:24.144358 train SE 11.356 test SE 11.220\n",
      "Model saved!\n",
      "Epoch: 76 time 0:01:24.026301 train SE 11.365 test SE 11.216\n",
      "Epoch: 77 time 0:01:25.136203 train SE 11.366 test SE 11.211\n",
      "Epoch: 78 time 0:01:23.741335 train SE 11.374 test SE 11.214\n",
      "Epoch: 79 time 0:01:23.462173 train SE 11.371 test SE 11.227\n",
      "Model saved!\n",
      "Epoch: 80 time 0:01:23.852146 train SE 11.382 test SE 11.222\n",
      "Epoch: 81 time 0:01:25.671178 train SE 11.387 test SE 11.223\n",
      "Epoch: 82 time 0:01:25.677167 train SE 11.385 test SE 11.230\n",
      "Model saved!\n",
      "Epoch: 83 time 0:01:25.445928 train SE 11.393 test SE 11.213\n",
      "Epoch: 84 time 0:01:24.932731 train SE 11.396 test SE 11.218\n",
      "Epoch: 85 time 0:01:24.827945 train SE 11.405 test SE 11.221\n",
      "Epoch: 86 time 0:01:24.983655 train SE 11.405 test SE 11.227\n",
      "Epoch: 87 time 0:01:29.316107 train SE 11.409 test SE 11.212\n",
      "Epoch: 88 time 0:01:23.409887 train SE 11.404 test SE 11.242\n",
      "Model saved!\n",
      "Epoch: 89 time 0:01:27.987495 train SE 11.424 test SE 11.236\n",
      "Epoch: 90 time 0:01:23.043936 train SE 11.423 test SE 11.250\n",
      "Model saved!\n",
      "Epoch: 91 time 0:01:27.709707 train SE 11.427 test SE 11.231\n",
      "Epoch: 92 time 0:01:27.163249 train SE 11.430 test SE 11.249\n",
      "Epoch: 93 time 0:01:23.309974 train SE 11.432 test SE 11.244\n",
      "Epoch: 94 time 0:01:27.165819 train SE 11.431 test SE 11.236\n",
      "Epoch: 95 time 0:01:26.141904 train SE 11.437 test SE 11.239\n",
      "Epoch: 96 time 0:01:24.181169 train SE 11.442 test SE 11.232\n",
      "Epoch: 97 time 0:01:26.369136 train SE 11.439 test SE 11.238\n",
      "Epoch: 98 time 0:01:25.696716 train SE 11.445 test SE 11.244\n",
      "Epoch: 99 time 0:01:26.230782 train SE 11.450 test SE 11.245\n",
      "Epoch: 100 time 0:01:25.055498 train SE 11.513 test SE 11.292\n",
      "Model saved!\n",
      "Epoch: 101 time 0:01:24.709333 train SE 11.531 test SE 11.304\n",
      "Model saved!\n",
      "Epoch: 102 time 0:01:24.871373 train SE 11.533 test SE 11.288\n",
      "Epoch: 103 time 0:01:24.829502 train SE 11.534 test SE 11.304\n",
      "Model saved!\n",
      "Epoch: 104 time 0:01:25.024545 train SE 11.538 test SE 11.307\n",
      "Model saved!\n",
      "Epoch: 105 time 0:01:24.760602 train SE 11.546 test SE 11.309\n",
      "Model saved!\n",
      "Epoch: 106 time 0:01:23.874985 train SE 11.546 test SE 11.304\n",
      "Epoch: 107 time 0:01:23.879669 train SE 11.546 test SE 11.317\n",
      "Model saved!\n",
      "Epoch: 108 time 0:01:23.878062 train SE 11.547 test SE 11.297\n",
      "Epoch: 109 time 0:01:23.965320 train SE 11.548 test SE 11.309\n",
      "Epoch: 110 time 0:01:24.464933 train SE 11.554 test SE 11.309\n",
      "Epoch: 111 time 0:01:23.320581 train SE 11.559 test SE 11.309\n",
      "Epoch: 112 time 0:01:23.710899 train SE 11.560 test SE 11.313\n",
      "Epoch: 113 time 0:01:23.698455 train SE 11.559 test SE 11.307\n",
      "Epoch: 114 time 0:01:23.302110 train SE 11.560 test SE 11.296\n",
      "Epoch: 115 time 0:01:23.431298 train SE 11.565 test SE 11.311\n",
      "Epoch: 116 time 0:01:23.563305 train SE 11.568 test SE 11.302\n",
      "Epoch: 117 time 0:01:24.162337 train SE 11.569 test SE 11.319\n",
      "Model saved!\n",
      "Epoch: 118 time 0:01:23.531453 train SE 11.574 test SE 11.305\n",
      "Epoch: 119 time 0:01:24.446559 train SE 11.572 test SE 11.314\n",
      "Epoch: 120 time 0:01:24.714335 train SE 11.565 test SE 11.301\n",
      "Epoch: 121 time 0:01:23.740399 train SE 11.568 test SE 11.303\n",
      "Epoch: 122 time 0:01:23.741399 train SE 11.574 test SE 11.300\n",
      "Epoch: 123 time 0:01:23.816133 train SE 11.577 test SE 11.310\n",
      "Epoch: 124 time 0:01:25.775481 train SE 11.580 test SE 11.314\n",
      "Epoch: 125 time 0:01:24.716699 train SE 11.574 test SE 11.299\n",
      "Epoch: 126 time 0:01:23.598834 train SE 11.580 test SE 11.302\n",
      "Epoch: 127 time 0:01:23.366452 train SE 11.578 test SE 11.292\n",
      "Epoch: 128 time 0:01:23.168918 train SE 11.575 test SE 11.301\n",
      "Epoch: 129 time 0:01:23.588896 train SE 11.582 test SE 11.307\n",
      "Epoch: 130 time 0:01:23.820727 train SE 11.592 test SE 11.306\n",
      "Epoch: 131 time 0:01:24.627064 train SE 11.581 test SE 11.296\n",
      "Epoch: 132 time 0:01:25.093794 train SE 11.590 test SE 11.292\n",
      "Epoch: 133 time 0:01:24.395636 train SE 11.588 test SE 11.294\n",
      "Epoch: 134 time 0:01:24.494379 train SE 11.589 test SE 11.298\n",
      "Epoch: 135 time 0:01:23.306958 train SE 11.591 test SE 11.284\n",
      "Epoch: 136 time 0:01:23.974309 train SE 11.590 test SE 11.303\n",
      "Epoch: 137 time 0:01:23.546604 train SE 11.592 test SE 11.302\n",
      "Epoch: 138 time 0:01:23.362285 train SE 11.589 test SE 11.300\n",
      "Epoch: 139 time 0:01:23.960774 train SE 11.599 test SE 11.304\n",
      "Epoch: 140 time 0:01:23.419464 train SE 11.595 test SE 11.297\n",
      "Epoch: 141 time 0:01:23.519653 train SE 11.593 test SE 11.296\n",
      "Epoch: 142 time 0:01:24.832963 train SE 11.593 test SE 11.285\n",
      "Epoch: 143 time 0:01:24.832532 train SE 11.594 test SE 11.299\n",
      "Epoch: 144 time 0:01:25.251413 train SE 11.603 test SE 11.303\n",
      "Epoch: 145 time 0:01:24.774588 train SE 11.605 test SE 11.301\n",
      "Epoch: 146 time 0:01:24.294470 train SE 11.605 test SE 11.299\n",
      "Epoch: 147 time 0:01:24.266079 train SE 11.606 test SE 11.316\n",
      "Epoch: 148 time 0:01:24.765154 train SE 11.604 test SE 11.311\n",
      "Epoch: 149 time 0:01:25.266772 train SE 11.607 test SE 11.311\n",
      "Epoch: 150 time 0:01:23.385383 train SE 11.647 test SE 11.334\n",
      "Model saved!\n",
      "Epoch: 151 time 0:01:24.332855 train SE 11.652 test SE 11.320\n",
      "Epoch: 152 time 0:01:24.914784 train SE 11.658 test SE 11.328\n",
      "Epoch: 153 time 0:01:24.946243 train SE 11.659 test SE 11.327\n",
      "Epoch: 154 time 0:01:24.918845 train SE 11.662 test SE 11.323\n",
      "Epoch: 155 time 0:01:24.552703 train SE 11.661 test SE 11.325\n",
      "Epoch: 156 time 0:01:24.810442 train SE 11.664 test SE 11.320\n",
      "Epoch: 157 time 0:01:23.622814 train SE 11.664 test SE 11.325\n",
      "Epoch: 158 time 0:01:23.329541 train SE 11.664 test SE 11.336\n",
      "Model saved!\n",
      "Epoch: 159 time 0:01:24.385179 train SE 11.666 test SE 11.329\n",
      "Epoch: 160 time 0:01:24.829066 train SE 11.665 test SE 11.323\n",
      "Epoch: 161 time 0:01:23.742504 train SE 11.669 test SE 11.319\n",
      "Epoch: 162 time 0:01:24.793060 train SE 11.667 test SE 11.327\n",
      "Epoch: 163 time 0:01:23.864057 train SE 11.664 test SE 11.324\n",
      "Epoch: 164 time 0:01:22.478734 train SE 11.665 test SE 11.320\n",
      "Epoch: 165 time 0:01:22.260937 train SE 11.671 test SE 11.319\n",
      "Epoch: 166 time 0:01:26.587754 train SE 11.670 test SE 11.327\n",
      "Epoch: 167 time 0:01:22.743570 train SE 11.675 test SE 11.320\n",
      "Epoch: 168 time 0:01:29.291569 train SE 11.673 test SE 11.328\n",
      "Epoch: 169 time 0:01:23.381450 train SE 11.676 test SE 11.324\n",
      "Epoch: 170 time 0:01:23.865529 train SE 11.673 test SE 11.325\n",
      "Epoch: 171 time 0:01:23.988743 train SE 11.673 test SE 11.327\n",
      "Epoch: 172 time 0:01:24.875459 train SE 11.674 test SE 11.317\n",
      "Epoch: 173 time 0:01:26.477104 train SE 11.676 test SE 11.330\n",
      "Epoch: 174 time 0:01:25.319851 train SE 11.677 test SE 11.323\n",
      "Epoch: 175 time 0:01:26.177477 train SE 11.679 test SE 11.321\n",
      "Epoch: 176 time 0:01:25.306828 train SE 11.681 test SE 11.323\n",
      "Epoch: 177 time 0:01:24.895842 train SE 11.682 test SE 11.327\n",
      "Epoch: 178 time 0:01:27.186451 train SE 11.680 test SE 11.321\n",
      "Epoch: 179 time 0:01:26.538476 train SE 11.682 test SE 11.317\n",
      "The best SE is: 11.336\n",
      "[ 5.29634047  6.89686537  7.89984083  8.4684906   8.93318272  9.21272755\n",
      "  9.37752628  9.48104286  9.66613674  9.79176331  9.84533882  9.96283913\n",
      " 10.06195259 10.09324932 10.17224312 10.24256039 10.26198578 10.35076523\n",
      " 10.38309574 10.4488554  10.48436451 10.52770233 10.57669258 10.63964748\n",
      " 10.67431259 10.67524719 10.70360374 10.74787998 10.78576374 10.79131699\n",
      " 10.81564426 10.83031178 10.85755062 10.8816328  10.90187168 10.9147892\n",
      " 10.92181492 10.96219444 10.95779324 11.00302887 10.99326324 11.00317287\n",
      " 11.00341702 11.02647495 11.02529812 11.045331   11.02827835 11.0482893\n",
      " 11.0453043  11.06914234 11.07477951 11.0867281  11.10769558 11.10181999\n",
      " 11.09083366 11.10749912 11.12342548 11.12617302 11.13572598 11.13382339\n",
      " 11.14084053 11.13125134 11.14424133 11.15861797 11.17652893 11.17929077\n",
      " 11.18140602 11.1611166  11.18375206 11.17596149 11.19614697 11.19662285\n",
      " 11.16921234 11.18859005 11.20535946 11.22031116 11.21567917 11.21109962\n",
      " 11.21386909 11.22680759 11.22206688 11.22281361 11.23030758 11.21262646\n",
      " 11.21832657 11.22099113 11.22744083 11.21236229 11.24234104 11.23616791\n",
      " 11.24954319 11.2311697  11.24851894 11.24405193 11.23648739 11.23852348\n",
      " 11.23216438 11.23825264 11.24371052 11.2447319  11.29225922 11.30428219\n",
      " 11.28757095 11.30432224 11.306674   11.3086195  11.30360413 11.3166914\n",
      " 11.2965889  11.30922413 11.30916882 11.30904865 11.31254864 11.30686855\n",
      " 11.29636669 11.31098175 11.30177975 11.31902218 11.30457211 11.31397533\n",
      " 11.30145359 11.3028698  11.29970837 11.30983067 11.31408024 11.29882908\n",
      " 11.30175591 11.29211521 11.30115414 11.30656719 11.30589867 11.29586792\n",
      " 11.2920084  11.2940464  11.29757214 11.28446102 11.30307388 11.3017807\n",
      " 11.30000877 11.30441666 11.296525   11.29604244 11.28531551 11.29903698\n",
      " 11.30318165 11.30142117 11.29925346 11.31644726 11.31118679 11.31140041\n",
      " 11.33395958 11.31952572 11.32790756 11.32653332 11.32295513 11.32478905\n",
      " 11.32022762 11.32502556 11.33566189 11.32852936 11.32337093 11.31867123\n",
      " 11.32740593 11.32380486 11.31968403 11.31882    11.32659435 11.32032776\n",
      " 11.32814884 11.32373428 11.32518387 11.32745361 11.31737041 11.33002758\n",
      " 11.3234129  11.32137966 11.32301712 11.32709694 11.32117176 11.31699657]\n"
     ]
    }
   ],
   "source": [
    "B = 24\n",
    "train(Nc,N,Nt,B,Nr,L,SNR_dB,K,EPOCH,BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([204800, 2, 32, 128])\n",
      "DNN_BS_hyb_OFDM(\n",
      "  (DQL): DequantizationLayer()\n",
      "  (FC1): Linear(in_features=64, out_features=2048, bias=True)\n",
      "  (bn1): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (mish1): Mish()\n",
      "  (FC2): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "  (bn2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (mish2): Mish()\n",
      "  (FC3): Linear(in_features=1024, out_features=384, bias=True)\n",
      "  (bn3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (mish3): Mish()\n",
      "  (res1): RES_BLOCK(\n",
      "    (conv1): Sequential(\n",
      "      (0): Conv2d(8, 256, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0))\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Mish()\n",
      "    )\n",
      "    (conv2): Sequential(\n",
      "      (0): Conv2d(256, 512, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0))\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Mish()\n",
      "    )\n",
      "    (conv3): Sequential(\n",
      "      (0): Conv2d(512, 8, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0))\n",
      "      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (conv): Conv2d(8, 8, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0))\n",
      ")\n",
      "DNN_US_RF_OFDM(\n",
      "  (pilot): Linear(in_features=64, out_features=8, bias=False)\n",
      "  (res): RES_BLOCK(\n",
      "    (conv1): Sequential(\n",
      "      (0): Conv2d(16, 256, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0))\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Mish()\n",
      "    )\n",
      "    (conv2): Sequential(\n",
      "      (0): Conv2d(256, 512, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0))\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Mish()\n",
      "    )\n",
      "    (conv3): Sequential(\n",
      "      (0): Conv2d(512, 16, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0))\n",
      "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (FC2): Linear(in_features=512, out_features=1024, bias=True)\n",
      "  (bn2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu2): ReLU()\n",
      "  (FC3): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (bn3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu3): ReLU()\n",
      "  (FC4): Linear(in_features=512, out_features=32, bias=True)\n",
      "  (bn4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (QL): QuantizationLayer()\n",
      ")\n",
      "Epoch: 0 time 0:01:21.166948 train SE 4.412 test SE 5.477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:256: UserWarning: Couldn't retrieve source code for container of type DNN_US_RF_OFDM. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:256: UserWarning: Couldn't retrieve source code for container of type RES_BLOCK. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:256: UserWarning: Couldn't retrieve source code for container of type Mish. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:256: UserWarning: Couldn't retrieve source code for container of type QuantizationLayer. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:256: UserWarning: Couldn't retrieve source code for container of type DNN_BS_hyb_OFDM. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:256: UserWarning: Couldn't retrieve source code for container of type DequantizationLayer. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved!\n",
      "Epoch: 1 time 0:01:22.278349 train SE 5.929 test SE 6.345\n",
      "Model saved!\n",
      "Epoch: 2 time 0:01:24.148443 train SE 6.780 test SE 7.319\n",
      "Model saved!\n",
      "Epoch: 3 time 0:01:26.009592 train SE 7.582 test SE 7.841\n",
      "Model saved!\n",
      "Epoch: 4 time 0:01:26.295075 train SE 8.142 test SE 8.378\n",
      "Model saved!\n",
      "Epoch: 5 time 0:01:26.050784 train SE 8.563 test SE 8.659\n",
      "Model saved!\n",
      "Epoch: 6 time 0:01:26.319486 train SE 8.819 test SE 8.950\n",
      "Model saved!\n",
      "Epoch: 7 time 0:01:26.491786 train SE 9.061 test SE 9.217\n",
      "Model saved!\n",
      "Epoch: 8 time 0:01:24.677029 train SE 9.261 test SE 9.358\n",
      "Model saved!\n",
      "Epoch: 9 time 0:01:27.316923 train SE 9.407 test SE 9.461\n",
      "Model saved!\n",
      "Epoch: 10 time 0:01:25.944487 train SE 9.538 test SE 9.592\n",
      "Model saved!\n",
      "Epoch: 11 time 0:01:27.257030 train SE 9.632 test SE 9.694\n",
      "Model saved!\n",
      "Epoch: 12 time 0:01:26.647319 train SE 9.702 test SE 9.785\n",
      "Model saved!\n",
      "Epoch: 13 time 0:01:26.565357 train SE 9.781 test SE 9.830\n",
      "Model saved!\n",
      "Epoch: 14 time 0:01:29.230620 train SE 9.834 test SE 9.922\n",
      "Model saved!\n",
      "Epoch: 15 time 0:01:26.844132 train SE 9.903 test SE 9.974\n",
      "Model saved!\n",
      "Epoch: 16 time 0:01:26.883975 train SE 9.980 test SE 10.011\n",
      "Model saved!\n",
      "Epoch: 17 time 0:01:29.410767 train SE 10.028 test SE 10.027\n",
      "Model saved!\n",
      "Epoch: 18 time 0:01:26.477525 train SE 10.056 test SE 10.132\n",
      "Model saved!\n",
      "Epoch: 19 time 0:01:26.264184 train SE 10.109 test SE 10.166\n",
      "Model saved!\n",
      "Epoch: 20 time 0:01:29.484142 train SE 10.174 test SE 10.196\n",
      "Model saved!\n",
      "Epoch: 21 time 0:01:26.654606 train SE 10.181 test SE 10.201\n",
      "Model saved!\n",
      "Epoch: 22 time 0:01:26.699409 train SE 10.222 test SE 10.244\n",
      "Model saved!\n",
      "Epoch: 23 time 0:01:26.416858 train SE 10.242 test SE 10.255\n",
      "Model saved!\n",
      "Epoch: 24 time 0:01:26.611941 train SE 10.250 test SE 10.263\n",
      "Model saved!\n",
      "Epoch: 25 time 0:01:25.663768 train SE 10.289 test SE 10.286\n",
      "Model saved!\n",
      "Epoch: 26 time 0:01:26.223319 train SE 10.301 test SE 10.281\n",
      "Epoch: 27 time 0:01:26.637793 train SE 10.279 test SE 10.316\n",
      "Model saved!\n",
      "Epoch: 28 time 0:01:26.830623 train SE 10.330 test SE 10.340\n",
      "Model saved!\n",
      "Epoch: 29 time 0:01:26.258657 train SE 10.370 test SE 10.401\n",
      "Model saved!\n",
      "Epoch: 30 time 0:01:26.061240 train SE 10.370 test SE 10.391\n",
      "Epoch: 31 time 0:01:26.872593 train SE 10.394 test SE 10.401\n",
      "Model saved!\n",
      "Epoch: 32 time 0:01:28.363503 train SE 10.388 test SE 10.372\n",
      "Epoch: 33 time 0:01:29.063734 train SE 10.393 test SE 10.396\n",
      "Epoch: 34 time 0:01:28.902201 train SE 10.391 test SE 10.366\n",
      "Epoch: 35 time 0:01:28.014960 train SE 10.401 test SE 10.404\n",
      "Model saved!\n",
      "Epoch: 36 time 0:01:26.768347 train SE 10.404 test SE 10.399\n",
      "Epoch: 37 time 0:01:26.372274 train SE 10.407 test SE 10.450\n",
      "Model saved!\n",
      "Epoch: 38 time 0:01:28.564975 train SE 10.401 test SE 10.408\n",
      "Epoch: 39 time 0:01:28.897143 train SE 10.423 test SE 10.401\n",
      "Epoch: 40 time 0:01:26.955783 train SE 10.400 test SE 10.434\n",
      "Epoch: 41 time 0:01:26.686151 train SE 10.424 test SE 10.464\n",
      "Model saved!\n",
      "Epoch: 42 time 0:01:26.918445 train SE 10.453 test SE 10.471\n",
      "Model saved!\n",
      "Epoch: 43 time 0:01:26.418299 train SE 10.431 test SE 10.415\n",
      "Epoch: 44 time 0:01:27.079568 train SE 10.433 test SE 10.439\n",
      "Epoch: 45 time 0:01:27.391351 train SE 10.453 test SE 10.456\n",
      "Epoch: 46 time 0:01:27.429985 train SE 10.477 test SE 10.483\n",
      "Model saved!\n",
      "Epoch: 47 time 0:01:28.087252 train SE 10.507 test SE 10.495\n",
      "Model saved!\n",
      "Epoch: 48 time 0:01:26.139946 train SE 10.484 test SE 10.493\n",
      "Epoch: 49 time 0:01:26.297104 train SE 10.513 test SE 10.526\n",
      "Model saved!\n",
      "Epoch: 50 time 0:01:27.042494 train SE 10.529 test SE 10.524\n",
      "Epoch: 51 time 0:01:28.779412 train SE 10.527 test SE 10.496\n",
      "Epoch: 52 time 0:01:26.470708 train SE 10.505 test SE 10.509\n",
      "Epoch: 53 time 0:01:28.713105 train SE 10.525 test SE 10.491\n",
      "Epoch: 54 time 0:01:25.850725 train SE 10.511 test SE 10.501\n",
      "Epoch: 55 time 0:01:28.416101 train SE 10.526 test SE 10.498\n",
      "Epoch: 56 time 0:01:27.444697 train SE 10.550 test SE 10.536\n",
      "Model saved!\n",
      "Epoch: 57 time 0:01:26.769481 train SE 10.534 test SE 10.551\n",
      "Model saved!\n",
      "Epoch: 58 time 0:01:27.108370 train SE 10.559 test SE 10.497\n",
      "Epoch: 59 time 0:01:26.488485 train SE 10.550 test SE 10.560\n",
      "Model saved!\n",
      "Epoch: 60 time 0:01:27.050571 train SE 10.574 test SE 10.559\n",
      "Epoch: 61 time 0:01:26.789354 train SE 10.581 test SE 10.566\n",
      "Model saved!\n",
      "Epoch: 62 time 0:01:26.547381 train SE 10.591 test SE 10.576\n",
      "Model saved!\n",
      "Epoch: 63 time 0:01:27.159459 train SE 10.607 test SE 10.601\n",
      "Model saved!\n",
      "Epoch: 64 time 0:01:26.777463 train SE 10.617 test SE 10.593\n",
      "Epoch: 65 time 0:01:27.006705 train SE 10.634 test SE 10.603\n",
      "Model saved!\n",
      "Epoch: 66 time 0:01:25.881325 train SE 10.641 test SE 10.621\n",
      "Model saved!\n",
      "Epoch: 67 time 0:01:27.795548 train SE 10.646 test SE 10.617\n",
      "Epoch: 68 time 0:01:26.406905 train SE 10.642 test SE 10.574\n",
      "Epoch: 69 time 0:01:26.411916 train SE 10.643 test SE 10.624\n",
      "Model saved!\n",
      "Epoch: 70 time 0:01:25.979916 train SE 10.662 test SE 10.638\n",
      "Model saved!\n",
      "Epoch: 71 time 0:01:26.595862 train SE 10.667 test SE 10.630\n",
      "Epoch: 72 time 0:01:26.370000 train SE 10.657 test SE 10.644\n",
      "Model saved!\n",
      "Epoch: 73 time 0:01:27.354843 train SE 10.667 test SE 10.598\n",
      "Epoch: 74 time 0:01:33.318883 train SE 10.661 test SE 10.617\n",
      "Epoch: 75 time 0:01:27.444690 train SE 10.656 test SE 10.638\n",
      "Epoch: 76 time 0:01:27.092111 train SE 10.680 test SE 10.661\n",
      "Model saved!\n",
      "Epoch: 77 time 0:01:27.335968 train SE 10.691 test SE 10.666\n",
      "Model saved!\n",
      "Epoch: 78 time 0:01:27.186332 train SE 10.695 test SE 10.654\n",
      "Epoch: 79 time 0:01:26.604238 train SE 10.691 test SE 10.639\n",
      "Epoch: 80 time 0:01:27.064151 train SE 10.706 test SE 10.676\n",
      "Model saved!\n",
      "Epoch: 81 time 0:01:26.290688 train SE 10.706 test SE 10.668\n",
      "Epoch: 82 time 0:01:26.472147 train SE 10.697 test SE 10.655\n",
      "Epoch: 83 time 0:01:27.230412 train SE 10.713 test SE 10.680\n",
      "Model saved!\n",
      "Epoch: 84 time 0:01:27.101924 train SE 10.715 test SE 10.681\n",
      "Model saved!\n",
      "Epoch: 85 time 0:01:26.816728 train SE 10.724 test SE 10.686\n",
      "Model saved!\n",
      "Epoch: 86 time 0:01:28.258992 train SE 10.714 test SE 10.680\n",
      "Epoch: 87 time 0:01:27.220267 train SE 10.717 test SE 10.642\n",
      "Epoch: 88 time 0:01:27.302531 train SE 10.720 test SE 10.673\n",
      "Epoch: 89 time 0:01:26.695119 train SE 10.743 test SE 10.715\n",
      "Model saved!\n",
      "Epoch: 90 time 0:01:26.828784 train SE 10.761 test SE 10.724\n",
      "Model saved!\n",
      "Epoch: 91 time 0:01:26.772427 train SE 10.758 test SE 10.703\n",
      "Epoch: 92 time 0:01:26.713022 train SE 10.754 test SE 10.706\n",
      "Epoch: 93 time 0:01:27.299419 train SE 10.753 test SE 10.683\n",
      "Epoch: 94 time 0:01:26.630378 train SE 10.762 test SE 10.741\n",
      "Model saved!\n",
      "Epoch: 95 time 0:01:26.453658 train SE 10.766 test SE 10.718\n",
      "Epoch: 96 time 0:01:26.503160 train SE 10.776 test SE 10.724\n",
      "Epoch: 97 time 0:01:26.389786 train SE 10.788 test SE 10.708\n",
      "Epoch: 98 time 0:01:27.354324 train SE 10.767 test SE 10.707\n",
      "Epoch: 99 time 0:01:26.492688 train SE 10.773 test SE 10.749\n",
      "Model saved!\n",
      "Epoch: 100 time 0:01:26.520022 train SE 10.865 test SE 10.804\n",
      "Model saved!\n",
      "Epoch: 101 time 0:01:29.321956 train SE 10.875 test SE 10.811\n",
      "Model saved!\n",
      "Epoch: 102 time 0:01:26.529503 train SE 10.888 test SE 10.801\n",
      "Epoch: 103 time 0:01:26.769291 train SE 10.896 test SE 10.828\n",
      "Model saved!\n",
      "Epoch: 104 time 0:01:26.798764 train SE 10.904 test SE 10.825\n",
      "Epoch: 105 time 0:01:26.342527 train SE 10.903 test SE 10.830\n",
      "Model saved!\n",
      "Epoch: 106 time 0:01:29.522100 train SE 10.907 test SE 10.823\n",
      "Epoch: 107 time 0:01:27.210163 train SE 10.906 test SE 10.829\n",
      "Epoch: 108 time 0:01:26.533937 train SE 10.903 test SE 10.812\n",
      "Epoch: 109 time 0:01:26.526939 train SE 10.909 test SE 10.830\n",
      "Epoch: 110 time 0:01:26.332930 train SE 10.914 test SE 10.837\n",
      "Model saved!\n",
      "Epoch: 111 time 0:01:30.004639 train SE 10.927 test SE 10.847\n",
      "Model saved!\n",
      "Epoch: 112 time 0:01:26.231199 train SE 10.932 test SE 10.833\n",
      "Epoch: 113 time 0:01:26.033785 train SE 10.919 test SE 10.826\n",
      "Epoch: 114 time 0:01:27.029124 train SE 10.919 test SE 10.834\n",
      "Epoch: 115 time 0:01:25.989473 train SE 10.924 test SE 10.847\n",
      "Model saved!\n",
      "Epoch: 116 time 0:01:26.081228 train SE 10.917 test SE 10.829\n",
      "Epoch: 117 time 0:01:26.417870 train SE 10.920 test SE 10.837\n",
      "Epoch: 118 time 0:01:27.372833 train SE 10.932 test SE 10.843\n",
      "Epoch: 119 time 0:01:26.335599 train SE 10.930 test SE 10.856\n",
      "Model saved!\n",
      "Epoch: 120 time 0:01:26.515869 train SE 10.927 test SE 10.851\n",
      "Epoch: 121 time 0:01:26.895485 train SE 10.930 test SE 10.847\n",
      "Epoch: 122 time 0:01:27.337349 train SE 10.925 test SE 10.846\n",
      "Epoch: 123 time 0:01:26.793259 train SE 10.918 test SE 10.840\n",
      "Epoch: 124 time 0:01:26.727857 train SE 10.920 test SE 10.838\n",
      "Epoch: 125 time 0:01:26.904558 train SE 10.904 test SE 10.831\n",
      "Epoch: 126 time 0:01:34.775535 train SE 10.914 test SE 10.812\n",
      "Epoch: 127 time 0:01:26.499071 train SE 10.906 test SE 10.843\n",
      "Epoch: 128 time 0:01:26.989948 train SE 10.925 test SE 10.854\n",
      "Epoch: 129 time 0:01:27.862617 train SE 10.931 test SE 10.859\n",
      "Model saved!\n",
      "Epoch: 130 time 0:01:26.944753 train SE 10.937 test SE 10.856\n",
      "Epoch: 131 time 0:01:26.924852 train SE 10.935 test SE 10.852\n",
      "Epoch: 132 time 0:01:26.833623 train SE 10.923 test SE 10.812\n",
      "Epoch: 133 time 0:01:26.350075 train SE 10.910 test SE 10.828\n",
      "Epoch: 134 time 0:01:26.225157 train SE 10.920 test SE 10.840\n",
      "Epoch: 135 time 0:01:27.873322 train SE 10.913 test SE 10.834\n",
      "Epoch: 136 time 0:01:26.396766 train SE 10.913 test SE 10.817\n",
      "Epoch: 137 time 0:01:26.901571 train SE 10.886 test SE 10.782\n",
      "Epoch: 138 time 0:01:29.083201 train SE 10.862 test SE 10.723\n",
      "Epoch: 139 time 0:01:26.725945 train SE 10.788 test SE 10.751\n",
      "Epoch: 140 time 0:01:26.500712 train SE 10.825 test SE 10.760\n",
      "Epoch: 141 time 0:01:26.731880 train SE 10.817 test SE 10.777\n",
      "Epoch: 142 time 0:01:26.652621 train SE 10.837 test SE 10.777\n",
      "Epoch: 143 time 0:01:27.298507 train SE 10.843 test SE 10.784\n",
      "Epoch: 144 time 0:01:26.672767 train SE 10.856 test SE 10.786\n",
      "Epoch: 145 time 0:01:26.527910 train SE 10.874 test SE 10.821\n",
      "Epoch: 146 time 0:01:26.644218 train SE 10.890 test SE 10.823\n",
      "Epoch: 147 time 0:01:26.527572 train SE 10.913 test SE 10.839\n",
      "Epoch: 148 time 0:01:26.524392 train SE 10.913 test SE 10.821\n",
      "Epoch: 149 time 0:01:26.399834 train SE 10.910 test SE 10.850\n",
      "Epoch: 150 time 0:01:26.134505 train SE 10.973 test SE 10.893\n",
      "Model saved!\n",
      "Epoch: 151 time 0:01:26.356304 train SE 10.986 test SE 10.889\n",
      "Epoch: 152 time 0:01:28.715235 train SE 10.985 test SE 10.892\n",
      "Epoch: 153 time 0:01:29.573694 train SE 10.993 test SE 10.911\n",
      "Model saved!\n",
      "Epoch: 154 time 0:01:27.042166 train SE 10.989 test SE 10.892\n",
      "Epoch: 155 time 0:01:26.508406 train SE 11.013 test SE 10.903\n",
      "Epoch: 156 time 0:01:26.134434 train SE 11.009 test SE 10.908\n",
      "Epoch: 157 time 0:01:27.410170 train SE 11.005 test SE 10.909\n",
      "Epoch: 158 time 0:01:26.811811 train SE 11.019 test SE 10.914\n",
      "Model saved!\n",
      "Epoch: 159 time 0:01:25.592440 train SE 11.020 test SE 10.911\n",
      "Epoch: 160 time 0:01:27.327721 train SE 11.020 test SE 10.909\n",
      "Epoch: 161 time 0:01:26.427750 train SE 11.027 test SE 10.916\n",
      "Model saved!\n",
      "Epoch: 162 time 0:01:26.383798 train SE 11.027 test SE 10.934\n",
      "Model saved!\n",
      "Epoch: 163 time 0:01:26.507992 train SE 11.030 test SE 10.922\n",
      "Epoch: 164 time 0:01:27.064497 train SE 11.031 test SE 10.907\n",
      "Epoch: 165 time 0:01:26.341927 train SE 11.029 test SE 10.905\n",
      "Epoch: 166 time 0:01:26.889005 train SE 11.030 test SE 10.896\n",
      "Epoch: 167 time 0:01:26.658803 train SE 11.035 test SE 10.907\n",
      "Epoch: 168 time 0:01:28.645813 train SE 11.030 test SE 10.908\n",
      "Epoch: 169 time 0:01:28.380780 train SE 11.039 test SE 10.915\n",
      "Epoch: 170 time 0:01:26.720511 train SE 11.049 test SE 10.925\n",
      "Epoch: 171 time 0:01:28.287813 train SE 11.055 test SE 10.919\n",
      "Epoch: 172 time 0:01:26.807178 train SE 11.046 test SE 10.924\n",
      "Epoch: 173 time 0:01:25.544586 train SE 11.061 test SE 10.919\n",
      "Epoch: 174 time 0:01:26.537464 train SE 11.053 test SE 10.921\n",
      "Epoch: 175 time 0:01:26.266196 train SE 11.061 test SE 10.935\n",
      "Model saved!\n",
      "Epoch: 176 time 0:01:27.433180 train SE 11.060 test SE 10.924\n",
      "Epoch: 177 time 0:01:26.781281 train SE 11.066 test SE 10.930\n",
      "Epoch: 178 time 0:01:26.560961 train SE 11.064 test SE 10.920\n",
      "Epoch: 179 time 0:01:26.329616 train SE 11.066 test SE 10.927\n",
      "The best SE is: 10.935\n",
      "[ 5.47690964  6.34489727  7.31894684  7.84145355  8.37780571  8.6592989\n",
      "  8.95043468  9.21721172  9.35816479  9.46088696  9.59162331  9.69397736\n",
      "  9.78538322  9.82969189  9.92187691  9.97371197 10.01118374 10.02728367\n",
      " 10.13174057 10.1658287  10.19623375 10.20116615 10.24410915 10.25524044\n",
      " 10.26293087 10.28637028 10.28063965 10.31571579 10.33952236 10.4005003\n",
      " 10.39070892 10.40050125 10.37162304 10.39640617 10.36633492 10.4040575\n",
      " 10.39923286 10.45018387 10.40843201 10.40085125 10.43389797 10.46417427\n",
      " 10.47059155 10.41547298 10.43920803 10.45598125 10.4833746  10.49521255\n",
      " 10.49260235 10.52587986 10.52356243 10.49619198 10.50915813 10.49080563\n",
      " 10.50057125 10.49773312 10.53583527 10.55125427 10.49661732 10.55967236\n",
      " 10.55930996 10.56582832 10.57618046 10.60140514 10.59275055 10.60310745\n",
      " 10.62101269 10.6174469  10.57420444 10.62394524 10.63834286 10.62970161\n",
      " 10.64419937 10.59830761 10.61748981 10.63825893 10.66107845 10.6656599\n",
      " 10.65426159 10.63938522 10.67604446 10.66831875 10.65471363 10.67961407\n",
      " 10.68149757 10.68606853 10.68011761 10.6417532  10.67279816 10.71548748\n",
      " 10.7238884  10.70340824 10.70609474 10.68296242 10.74071312 10.71823311\n",
      " 10.72357082 10.70837212 10.70715141 10.7493639  10.80382442 10.81139374\n",
      " 10.80117702 10.82840252 10.82530117 10.8302002  10.82279587 10.82915401\n",
      " 10.81185913 10.82983589 10.83737755 10.84657097 10.83261395 10.82633114\n",
      " 10.83386421 10.84716892 10.82913208 10.83671379 10.84324932 10.85637856\n",
      " 10.85112667 10.84659004 10.8461895  10.83985329 10.83808422 10.83127403\n",
      " 10.81228733 10.84285259 10.85366344 10.85852146 10.85587692 10.85188198\n",
      " 10.81247425 10.82757092 10.83991051 10.8342104  10.81690884 10.78181362\n",
      " 10.72272778 10.75074863 10.76023102 10.77687454 10.77677441 10.78399372\n",
      " 10.78604412 10.8211832  10.82281876 10.83897591 10.8207531  10.85007381\n",
      " 10.8930378  10.88855076 10.89230061 10.91087914 10.8920536  10.90342236\n",
      " 10.90817642 10.90928936 10.91402721 10.91070461 10.90891933 10.91584492\n",
      " 10.93410397 10.92156506 10.90651226 10.90486813 10.89584255 10.90677261\n",
      " 10.90757275 10.91524601 10.92464256 10.91879463 10.9242363  10.91897106\n",
      " 10.92145824 10.93513203 10.92391109 10.92969036 10.92045784 10.92696953]\n"
     ]
    }
   ],
   "source": [
    "B = 32\n",
    "train(Nc,N,Nt,B,Nr,L,SNR_dB,K,EPOCH,BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([204800, 2, 32, 128])\n",
      "DNN_BS_hyb_OFDM(\n",
      "  (DQL): DequantizationLayer()\n",
      "  (FC1): Linear(in_features=96, out_features=2048, bias=True)\n",
      "  (bn1): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (mish1): Mish()\n",
      "  (FC2): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "  (bn2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (mish2): Mish()\n",
      "  (FC3): Linear(in_features=1024, out_features=384, bias=True)\n",
      "  (bn3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (mish3): Mish()\n",
      "  (res1): RES_BLOCK(\n",
      "    (conv1): Sequential(\n",
      "      (0): Conv2d(8, 256, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0))\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Mish()\n",
      "    )\n",
      "    (conv2): Sequential(\n",
      "      (0): Conv2d(256, 512, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0))\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Mish()\n",
      "    )\n",
      "    (conv3): Sequential(\n",
      "      (0): Conv2d(512, 8, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0))\n",
      "      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (conv): Conv2d(8, 8, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0))\n",
      ")\n",
      "DNN_US_RF_OFDM(\n",
      "  (pilot): Linear(in_features=64, out_features=8, bias=False)\n",
      "  (res): RES_BLOCK(\n",
      "    (conv1): Sequential(\n",
      "      (0): Conv2d(16, 256, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0))\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Mish()\n",
      "    )\n",
      "    (conv2): Sequential(\n",
      "      (0): Conv2d(256, 512, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0))\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Mish()\n",
      "    )\n",
      "    (conv3): Sequential(\n",
      "      (0): Conv2d(512, 16, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0))\n",
      "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (FC2): Linear(in_features=512, out_features=1024, bias=True)\n",
      "  (bn2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu2): ReLU()\n",
      "  (FC3): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (bn3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu3): ReLU()\n",
      "  (FC4): Linear(in_features=512, out_features=48, bias=True)\n",
      "  (bn4): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (QL): QuantizationLayer()\n",
      ")\n",
      "Epoch: 0 time 0:01:26.913527 train SE 4.609 test SE 5.802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:256: UserWarning: Couldn't retrieve source code for container of type DNN_US_RF_OFDM. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:256: UserWarning: Couldn't retrieve source code for container of type RES_BLOCK. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:256: UserWarning: Couldn't retrieve source code for container of type Mish. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:256: UserWarning: Couldn't retrieve source code for container of type QuantizationLayer. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:256: UserWarning: Couldn't retrieve source code for container of type DNN_BS_hyb_OFDM. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:256: UserWarning: Couldn't retrieve source code for container of type DequantizationLayer. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved!\n",
      "Epoch: 1 time 0:01:25.553138 train SE 6.710 test SE 7.412\n",
      "Model saved!\n",
      "Epoch: 2 time 0:01:25.470861 train SE 7.949 test SE 8.446\n",
      "Model saved!\n",
      "Epoch: 3 time 0:01:26.681168 train SE 8.672 test SE 8.926\n",
      "Model saved!\n",
      "Epoch: 4 time 0:01:26.402294 train SE 9.093 test SE 9.287\n",
      "Model saved!\n",
      "Epoch: 5 time 0:01:28.420341 train SE 9.382 test SE 9.530\n",
      "Model saved!\n",
      "Epoch: 6 time 0:01:26.630614 train SE 9.559 test SE 9.618\n",
      "Model saved!\n",
      "Epoch: 7 time 0:01:26.771787 train SE 9.711 test SE 9.782\n",
      "Model saved!\n",
      "Epoch: 8 time 0:01:26.569963 train SE 9.815 test SE 9.914\n",
      "Model saved!\n",
      "Epoch: 9 time 0:01:26.049148 train SE 9.928 test SE 10.085\n",
      "Model saved!\n",
      "Epoch: 10 time 0:01:27.696845 train SE 10.093 test SE 10.188\n",
      "Model saved!\n",
      "Epoch: 11 time 0:01:26.018314 train SE 10.173 test SE 10.289\n",
      "Model saved!\n",
      "Epoch: 12 time 0:01:26.459651 train SE 10.257 test SE 10.321\n",
      "Model saved!\n",
      "Epoch: 13 time 0:01:26.614324 train SE 10.335 test SE 10.424\n",
      "Model saved!\n",
      "Epoch: 14 time 0:01:26.768327 train SE 10.422 test SE 10.475\n",
      "Model saved!\n",
      "Epoch: 15 time 0:01:26.962193 train SE 10.470 test SE 10.491\n",
      "Model saved!\n",
      "Epoch: 16 time 0:01:27.311288 train SE 10.521 test SE 10.506\n",
      "Model saved!\n",
      "Epoch: 17 time 0:01:26.555359 train SE 10.549 test SE 10.582\n",
      "Model saved!\n",
      "Epoch: 18 time 0:01:27.616693 train SE 10.616 test SE 10.605\n",
      "Model saved!\n",
      "Epoch: 19 time 0:01:26.693999 train SE 10.608 test SE 10.537\n",
      "Epoch: 20 time 0:01:26.680533 train SE 10.626 test SE 10.647\n",
      "Model saved!\n",
      "Epoch: 21 time 0:01:26.987872 train SE 10.679 test SE 10.667\n",
      "Model saved!\n",
      "Epoch: 22 time 0:01:25.927639 train SE 10.726 test SE 10.719\n",
      "Model saved!\n",
      "Epoch: 23 time 0:01:26.991265 train SE 10.750 test SE 10.756\n",
      "Model saved!\n",
      "Epoch: 24 time 0:01:26.770856 train SE 10.742 test SE 10.690\n",
      "Epoch: 25 time 0:01:26.434745 train SE 10.733 test SE 10.756\n",
      "Model saved!\n",
      "Epoch: 26 time 0:01:26.932506 train SE 10.773 test SE 10.724\n",
      "Epoch: 27 time 0:01:28.995365 train SE 10.812 test SE 10.809\n",
      "Model saved!\n",
      "Epoch: 28 time 0:01:27.325885 train SE 10.833 test SE 10.775\n",
      "Epoch: 29 time 0:01:26.673831 train SE 10.851 test SE 10.835\n",
      "Model saved!\n",
      "Epoch: 30 time 0:01:26.502622 train SE 10.880 test SE 10.860\n",
      "Model saved!\n",
      "Epoch: 31 time 0:01:25.436530 train SE 10.897 test SE 10.838\n",
      "Epoch: 32 time 0:01:26.157673 train SE 10.913 test SE 10.863\n",
      "Model saved!\n",
      "Epoch: 33 time 0:01:26.614785 train SE 10.920 test SE 10.862\n",
      "Epoch: 34 time 0:01:23.776818 train SE 10.942 test SE 10.920\n",
      "Model saved!\n",
      "Epoch: 35 time 0:01:33.936845 train SE 10.954 test SE 10.907\n",
      "Epoch: 36 time 0:01:26.866181 train SE 10.963 test SE 10.898\n",
      "Epoch: 37 time 0:01:25.960516 train SE 10.970 test SE 10.925\n",
      "Model saved!\n",
      "Epoch: 38 time 0:01:25.809263 train SE 10.942 test SE 10.891\n",
      "Epoch: 39 time 0:01:26.350427 train SE 10.974 test SE 10.944\n",
      "Model saved!\n",
      "Epoch: 40 time 0:01:28.110725 train SE 10.975 test SE 10.912\n",
      "Epoch: 41 time 0:01:26.877481 train SE 10.986 test SE 10.958\n",
      "Model saved!\n",
      "Epoch: 42 time 0:01:26.556664 train SE 11.011 test SE 10.941\n",
      "Epoch: 43 time 0:01:26.123123 train SE 11.020 test SE 10.980\n",
      "Model saved!\n",
      "Epoch: 44 time 0:01:32.172289 train SE 11.013 test SE 10.957\n",
      "Epoch: 45 time 0:01:26.615695 train SE 11.037 test SE 10.973\n",
      "Epoch: 46 time 0:01:26.315091 train SE 11.032 test SE 10.994\n",
      "Model saved!\n",
      "Epoch: 47 time 0:01:30.487888 train SE 11.059 test SE 11.000\n",
      "Model saved!\n",
      "Epoch: 48 time 0:01:30.644414 train SE 11.038 test SE 10.989\n",
      "Epoch: 49 time 0:01:26.532091 train SE 11.070 test SE 10.994\n",
      "Epoch: 50 time 0:01:26.618810 train SE 11.080 test SE 10.999\n",
      "Epoch: 51 time 0:01:26.321547 train SE 11.069 test SE 11.009\n",
      "Model saved!\n",
      "Epoch: 52 time 0:01:33.342308 train SE 11.083 test SE 11.032\n",
      "Model saved!\n",
      "Epoch: 53 time 0:01:27.065954 train SE 11.093 test SE 11.044\n",
      "Model saved!\n",
      "Epoch: 54 time 0:01:26.529907 train SE 11.120 test SE 11.038\n",
      "Epoch: 55 time 0:01:27.131364 train SE 11.113 test SE 11.059\n",
      "Model saved!\n",
      "Epoch: 56 time 0:01:28.833297 train SE 11.119 test SE 11.043\n",
      "Epoch: 57 time 0:01:26.221206 train SE 11.098 test SE 11.030\n",
      "Epoch: 58 time 0:01:26.691948 train SE 11.126 test SE 11.056\n",
      "Epoch: 59 time 0:01:26.449719 train SE 11.133 test SE 11.050\n",
      "Epoch: 60 time 0:01:28.378563 train SE 11.104 test SE 11.044\n",
      "Epoch: 61 time 0:01:26.390375 train SE 11.129 test SE 11.080\n",
      "Model saved!\n",
      "Epoch: 62 time 0:01:27.010759 train SE 11.127 test SE 11.020\n",
      "Epoch: 63 time 0:01:26.868105 train SE 11.127 test SE 11.041\n",
      "Epoch: 64 time 0:01:27.112248 train SE 11.113 test SE 11.009\n",
      "Epoch: 65 time 0:01:26.398818 train SE 11.084 test SE 10.987\n",
      "Epoch: 66 time 0:01:26.572881 train SE 11.104 test SE 11.055\n",
      "Epoch: 67 time 0:01:26.696412 train SE 11.113 test SE 11.020\n",
      "Epoch: 68 time 0:01:33.473404 train SE 11.109 test SE 11.038\n",
      "Epoch: 69 time 0:01:25.852240 train SE 11.086 test SE 10.998\n",
      "Epoch: 70 time 0:01:26.549565 train SE 11.112 test SE 11.055\n",
      "Epoch: 71 time 0:01:26.354030 train SE 11.146 test SE 11.041\n",
      "Epoch: 72 time 0:01:26.001422 train SE 11.139 test SE 11.043\n",
      "Epoch: 73 time 0:01:26.107345 train SE 11.138 test SE 11.043\n",
      "Epoch: 74 time 0:01:25.249417 train SE 11.139 test SE 11.029\n",
      "Epoch: 75 time 0:01:25.146194 train SE 11.144 test SE 11.035\n",
      "Epoch: 76 time 0:01:25.956932 train SE 11.134 test SE 11.059\n",
      "Epoch: 77 time 0:01:25.753066 train SE 11.164 test SE 11.056\n",
      "Epoch: 78 time 0:01:27.479919 train SE 11.153 test SE 11.016\n",
      "Epoch: 79 time 0:01:26.145483 train SE 11.136 test SE 11.053\n",
      "Epoch: 80 time 0:01:26.560554 train SE 11.155 test SE 11.040\n",
      "Epoch: 81 time 0:01:26.587913 train SE 11.172 test SE 11.062\n",
      "Epoch: 82 time 0:01:25.615439 train SE 11.141 test SE 11.050\n",
      "Epoch: 83 time 0:01:25.438273 train SE 11.173 test SE 11.075\n",
      "Epoch: 84 time 0:01:26.376903 train SE 11.193 test SE 11.069\n",
      "Epoch: 85 time 0:01:26.301557 train SE 11.193 test SE 11.090\n",
      "Model saved!\n",
      "Epoch: 86 time 0:01:26.542835 train SE 11.196 test SE 11.097\n",
      "Model saved!\n",
      "Epoch: 87 time 0:01:25.726212 train SE 11.209 test SE 11.098\n",
      "Model saved!\n",
      "Epoch: 88 time 0:01:26.264627 train SE 11.210 test SE 11.099\n",
      "Model saved!\n",
      "Epoch: 89 time 0:01:31.565911 train SE 11.218 test SE 11.138\n",
      "Model saved!\n",
      "Epoch: 90 time 0:01:26.096006 train SE 11.230 test SE 11.114\n",
      "Epoch: 91 time 0:01:26.637763 train SE 11.242 test SE 11.122\n",
      "Epoch: 92 time 0:01:27.056637 train SE 11.240 test SE 11.116\n",
      "Epoch: 93 time 0:01:26.715375 train SE 11.252 test SE 11.136\n",
      "Epoch: 94 time 0:01:26.284520 train SE 11.242 test SE 11.124\n",
      "Epoch: 95 time 0:01:22.527710 train SE 11.252 test SE 11.130\n",
      "Epoch: 96 time 0:01:25.366201 train SE 11.257 test SE 11.163\n",
      "Model saved!\n",
      "Epoch: 97 time 0:01:25.791584 train SE 11.268 test SE 11.134\n",
      "Epoch: 98 time 0:01:26.911024 train SE 11.265 test SE 11.161\n",
      "Epoch: 99 time 0:01:26.174524 train SE 11.267 test SE 11.159\n",
      "Epoch: 100 time 0:01:26.223273 train SE 11.344 test SE 11.216\n",
      "Model saved!\n",
      "Epoch: 101 time 0:01:26.862633 train SE 11.353 test SE 11.226\n",
      "Model saved!\n",
      "Epoch: 102 time 0:01:25.536725 train SE 11.368 test SE 11.225\n",
      "Epoch: 103 time 0:01:26.247155 train SE 11.365 test SE 11.230\n",
      "Model saved!\n",
      "Epoch: 104 time 0:01:25.333633 train SE 11.364 test SE 11.203\n",
      "Epoch: 105 time 0:01:26.731968 train SE 11.365 test SE 11.230\n",
      "Epoch: 106 time 0:01:26.229744 train SE 11.376 test SE 11.222\n",
      "Epoch: 107 time 0:01:27.564679 train SE 11.377 test SE 11.243\n",
      "Model saved!\n",
      "Epoch: 108 time 0:01:26.850645 train SE 11.385 test SE 11.241\n",
      "Epoch: 109 time 0:01:26.269799 train SE 11.381 test SE 11.237\n",
      "Epoch: 110 time 0:01:26.974767 train SE 11.380 test SE 11.225\n",
      "Epoch: 111 time 0:01:25.741556 train SE 11.380 test SE 11.222\n",
      "Epoch: 112 time 0:01:26.284835 train SE 11.386 test SE 11.230\n",
      "Epoch: 113 time 0:01:26.684112 train SE 11.390 test SE 11.244\n",
      "Model saved!\n",
      "Epoch: 114 time 0:01:26.301163 train SE 11.392 test SE 11.223\n",
      "Epoch: 115 time 0:01:26.434755 train SE 11.386 test SE 11.235\n",
      "Epoch: 116 time 0:01:26.673587 train SE 11.391 test SE 11.235\n",
      "Epoch: 117 time 0:01:27.186568 train SE 11.391 test SE 11.220\n",
      "Epoch: 118 time 0:01:26.993356 train SE 11.387 test SE 11.226\n",
      "Epoch: 119 time 0:01:26.515697 train SE 11.397 test SE 11.242\n",
      "Epoch: 120 time 0:01:26.231814 train SE 11.400 test SE 11.234\n",
      "Epoch: 121 time 0:01:26.801260 train SE 11.403 test SE 11.225\n",
      "Epoch: 122 time 0:01:26.193912 train SE 11.402 test SE 11.247\n",
      "Model saved!\n",
      "Epoch: 123 time 0:01:26.941111 train SE 11.399 test SE 11.228\n",
      "Epoch: 124 time 0:01:26.703503 train SE 11.406 test SE 11.224\n",
      "Epoch: 125 time 0:01:26.695581 train SE 11.408 test SE 11.228\n",
      "Epoch: 126 time 0:01:27.352932 train SE 11.410 test SE 11.246\n",
      "Epoch: 127 time 0:01:26.208301 train SE 11.411 test SE 11.237\n",
      "Epoch: 128 time 0:01:26.816119 train SE 11.407 test SE 11.231\n",
      "Epoch: 129 time 0:01:26.382537 train SE 11.415 test SE 11.253\n",
      "Model saved!\n",
      "Epoch: 130 time 0:01:26.522960 train SE 11.414 test SE 11.244\n",
      "Epoch: 131 time 0:01:25.872780 train SE 11.423 test SE 11.244\n",
      "Epoch: 132 time 0:01:26.530959 train SE 11.419 test SE 11.241\n",
      "Epoch: 133 time 0:01:26.152068 train SE 11.418 test SE 11.241\n",
      "Epoch: 134 time 0:01:26.673160 train SE 11.416 test SE 11.244\n",
      "Epoch: 135 time 0:01:27.130879 train SE 11.423 test SE 11.236\n",
      "Epoch: 136 time 0:01:26.761866 train SE 11.429 test SE 11.253\n",
      "Model saved!\n",
      "Epoch: 137 time 0:01:26.632293 train SE 11.430 test SE 11.250\n",
      "Epoch: 138 time 0:01:26.290207 train SE 11.433 test SE 11.245\n",
      "Epoch: 139 time 0:01:26.947331 train SE 11.421 test SE 11.263\n",
      "Model saved!\n",
      "Epoch: 140 time 0:01:27.274973 train SE 11.429 test SE 11.255\n",
      "Epoch: 141 time 0:01:26.793410 train SE 11.433 test SE 11.263\n",
      "Model saved!\n",
      "Epoch: 142 time 0:01:26.858596 train SE 11.429 test SE 11.242\n",
      "Epoch: 143 time 0:01:26.376908 train SE 11.438 test SE 11.266\n",
      "Model saved!\n",
      "Epoch: 144 time 0:01:27.126953 train SE 11.447 test SE 11.275\n",
      "Model saved!\n",
      "Epoch: 145 time 0:01:26.573415 train SE 11.455 test SE 11.278\n",
      "Model saved!\n",
      "Epoch: 146 time 0:01:27.453676 train SE 11.443 test SE 11.240\n",
      "Epoch: 147 time 0:01:26.636533 train SE 11.445 test SE 11.273\n",
      "Epoch: 148 time 0:01:30.480054 train SE 11.456 test SE 11.254\n",
      "Epoch: 149 time 0:01:25.747081 train SE 11.442 test SE 11.252\n",
      "Epoch: 150 time 0:01:26.954946 train SE 11.477 test SE 11.287\n",
      "Model saved!\n",
      "Epoch: 151 time 0:01:26.675128 train SE 11.488 test SE 11.291\n",
      "Model saved!\n",
      "Epoch: 152 time 0:01:27.059548 train SE 11.488 test SE 11.298\n",
      "Model saved!\n",
      "Epoch: 153 time 0:01:26.557058 train SE 11.499 test SE 11.290\n",
      "Epoch: 154 time 0:01:26.405983 train SE 11.500 test SE 11.291\n",
      "Epoch: 155 time 0:01:26.653198 train SE 11.501 test SE 11.299\n",
      "Model saved!\n",
      "Epoch: 156 time 0:01:27.671602 train SE 11.504 test SE 11.286\n",
      "Epoch: 157 time 0:01:25.295314 train SE 11.508 test SE 11.285\n",
      "Epoch: 158 time 0:01:26.769342 train SE 11.506 test SE 11.285\n",
      "Epoch: 159 time 0:01:25.802051 train SE 11.512 test SE 11.286\n",
      "Epoch: 160 time 0:01:26.791823 train SE 11.508 test SE 11.293\n",
      "Epoch: 161 time 0:01:26.834294 train SE 11.506 test SE 11.297\n",
      "Epoch: 162 time 0:01:26.751878 train SE 11.512 test SE 11.285\n",
      "Epoch: 163 time 0:01:26.854246 train SE 11.513 test SE 11.292\n",
      "Epoch: 164 time 0:01:25.789029 train SE 11.503 test SE 11.286\n",
      "Epoch: 165 time 0:01:27.011308 train SE 11.511 test SE 11.292\n",
      "Epoch: 166 time 0:01:26.095725 train SE 11.509 test SE 11.291\n",
      "Epoch: 167 time 0:01:26.527106 train SE 11.506 test SE 11.283\n",
      "Epoch: 168 time 0:01:26.208367 train SE 11.513 test SE 11.288\n",
      "Epoch: 169 time 0:01:26.738884 train SE 11.504 test SE 11.266\n",
      "Epoch: 170 time 0:01:26.683168 train SE 11.495 test SE 11.268\n",
      "Epoch: 171 time 0:01:26.612344 train SE 11.492 test SE 11.273\n",
      "Epoch: 172 time 0:01:26.428201 train SE 11.504 test SE 11.289\n",
      "Epoch: 173 time 0:01:26.557528 train SE 11.506 test SE 11.280\n",
      "Epoch: 174 time 0:01:26.664219 train SE 11.508 test SE 11.279\n",
      "Epoch: 175 time 0:01:26.990321 train SE 11.515 test SE 11.270\n",
      "Epoch: 176 time 0:01:26.664102 train SE 11.508 test SE 11.284\n",
      "Epoch: 177 time 0:01:26.454401 train SE 11.508 test SE 11.271\n",
      "Epoch: 178 time 0:01:26.396524 train SE 11.503 test SE 11.273\n",
      "Epoch: 179 time 0:01:25.044836 train SE 11.508 test SE 11.273\n",
      "The best SE is: 11.299\n",
      "[ 5.80238438  7.41201496  8.4458952   8.92633915  9.28735733  9.5297575\n",
      "  9.61777115  9.78186417  9.91397476 10.08524895 10.18765163 10.28919601\n",
      " 10.3206625  10.42426205 10.47511959 10.49070072 10.50608444 10.58243465\n",
      " 10.60538864 10.53711987 10.6467514  10.6670475  10.71885777 10.75553322\n",
      " 10.68964767 10.7558918  10.72354603 10.80916023 10.77510357 10.83518982\n",
      " 10.8600359  10.83803463 10.86262703 10.86166382 10.91960335 10.90698719\n",
      " 10.89811897 10.92518806 10.89086914 10.94390392 10.91222668 10.95768166\n",
      " 10.941432   10.9799757  10.95728016 10.97278214 10.99429321 10.99970818\n",
      " 10.98918533 10.99384975 10.99876595 11.00938892 11.0316391  11.04353046\n",
      " 11.03753948 11.05865002 11.04307079 11.02976131 11.05649757 11.04964447\n",
      " 11.04448223 11.07950115 11.01974869 11.04090595 11.00901794 10.9871912\n",
      " 11.0551405  11.01986027 11.03817272 10.99777699 11.05467415 11.04091549\n",
      " 11.04260254 11.04298687 11.02869034 11.0351038  11.05856037 11.0560751\n",
      " 11.01563931 11.05276012 11.03975868 11.06159019 11.04982471 11.07531643\n",
      " 11.06939602 11.0904665  11.09680653 11.09766674 11.09922791 11.13777351\n",
      " 11.11388397 11.12242413 11.11569595 11.1357832  11.12370777 11.12966919\n",
      " 11.16329765 11.13430309 11.16140747 11.15855598 11.21603966 11.22640896\n",
      " 11.2253952  11.22986031 11.20310116 11.22973537 11.22212791 11.24263\n",
      " 11.24054718 11.23666573 11.2250824  11.22240734 11.22968769 11.24355793\n",
      " 11.22334671 11.23488331 11.23502636 11.22027493 11.22593975 11.24170494\n",
      " 11.23382854 11.22513103 11.2473402  11.22808647 11.22435665 11.2278471\n",
      " 11.24579334 11.2365675  11.23123837 11.2527771  11.24422455 11.24385071\n",
      " 11.24106979 11.24114609 11.24389744 11.23570538 11.25333405 11.25023365\n",
      " 11.24480915 11.26250648 11.25478649 11.26309109 11.2417593  11.26581287\n",
      " 11.27465439 11.27812481 11.24019718 11.27346325 11.25367165 11.25234032\n",
      " 11.28651714 11.29110813 11.29783154 11.29011822 11.29072475 11.29905796\n",
      " 11.28555107 11.28538418 11.28530407 11.2856245  11.29255199 11.29685688\n",
      " 11.28506756 11.29243851 11.28569317 11.2922821  11.29070377 11.28333569\n",
      " 11.28795242 11.26640606 11.26849079 11.27260876 11.28942204 11.27981091\n",
      " 11.27905273 11.27019405 11.28369999 11.27113056 11.27282143 11.27264786]\n"
     ]
    }
   ],
   "source": [
    "B = 48\n",
    "train(Nc,N,Nt,B,Nr,L,SNR_dB,K,EPOCH,BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([204800, 2, 32, 128])\n",
      "DNN_BS_hyb_OFDM(\n",
      "  (DQL): DequantizationLayer()\n",
      "  (FC1): Linear(in_features=128, out_features=2048, bias=True)\n",
      "  (bn1): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (mish1): Mish()\n",
      "  (FC2): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "  (bn2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (mish2): Mish()\n",
      "  (FC3): Linear(in_features=1024, out_features=384, bias=True)\n",
      "  (bn3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (mish3): Mish()\n",
      "  (res1): RES_BLOCK(\n",
      "    (conv1): Sequential(\n",
      "      (0): Conv2d(8, 256, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0))\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Mish()\n",
      "    )\n",
      "    (conv2): Sequential(\n",
      "      (0): Conv2d(256, 512, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0))\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Mish()\n",
      "    )\n",
      "    (conv3): Sequential(\n",
      "      (0): Conv2d(512, 8, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0))\n",
      "      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (conv): Conv2d(8, 8, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0))\n",
      ")\n",
      "DNN_US_RF_OFDM(\n",
      "  (pilot): Linear(in_features=64, out_features=8, bias=False)\n",
      "  (res): RES_BLOCK(\n",
      "    (conv1): Sequential(\n",
      "      (0): Conv2d(16, 256, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0))\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Mish()\n",
      "    )\n",
      "    (conv2): Sequential(\n",
      "      (0): Conv2d(256, 512, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0))\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Mish()\n",
      "    )\n",
      "    (conv3): Sequential(\n",
      "      (0): Conv2d(512, 16, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0))\n",
      "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (FC2): Linear(in_features=512, out_features=1024, bias=True)\n",
      "  (bn2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu2): ReLU()\n",
      "  (FC3): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (bn3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu3): ReLU()\n",
      "  (FC4): Linear(in_features=512, out_features=64, bias=True)\n",
      "  (bn4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (QL): QuantizationLayer()\n",
      ")\n",
      "Epoch: 0 time 0:01:28.214438 train SE 5.375 test SE 5.985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:256: UserWarning: Couldn't retrieve source code for container of type DNN_US_RF_OFDM. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:256: UserWarning: Couldn't retrieve source code for container of type RES_BLOCK. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:256: UserWarning: Couldn't retrieve source code for container of type Mish. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:256: UserWarning: Couldn't retrieve source code for container of type QuantizationLayer. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:256: UserWarning: Couldn't retrieve source code for container of type DNN_BS_hyb_OFDM. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:256: UserWarning: Couldn't retrieve source code for container of type DequantizationLayer. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved!\n",
      "Epoch: 1 time 0:01:25.761685 train SE 6.064 test SE 6.077\n",
      "Model saved!\n",
      "Epoch: 2 time 0:01:25.359670 train SE 6.131 test SE 6.218\n",
      "Model saved!\n",
      "Epoch: 3 time 0:01:26.371868 train SE 6.494 test SE 6.785\n",
      "Model saved!\n",
      "Epoch: 4 time 0:01:26.427826 train SE 7.092 test SE 7.401\n",
      "Model saved!\n",
      "Epoch: 5 time 0:01:27.222767 train SE 7.777 test SE 8.105\n",
      "Model saved!\n",
      "Epoch: 6 time 0:01:26.459098 train SE 8.264 test SE 8.471\n",
      "Model saved!\n",
      "Epoch: 7 time 0:01:26.562913 train SE 8.614 test SE 8.801\n",
      "Model saved!\n",
      "Epoch: 8 time 0:01:26.621204 train SE 8.923 test SE 9.084\n",
      "Model saved!\n",
      "Epoch: 9 time 0:01:27.212648 train SE 9.170 test SE 9.327\n",
      "Model saved!\n",
      "Epoch: 10 time 0:01:25.432812 train SE 9.402 test SE 9.524\n",
      "Model saved!\n",
      "Epoch: 11 time 0:01:26.694608 train SE 9.595 test SE 9.709\n",
      "Model saved!\n",
      "Epoch: 12 time 0:01:26.794248 train SE 9.785 test SE 9.844\n",
      "Model saved!\n",
      "Epoch: 13 time 0:01:26.860508 train SE 9.901 test SE 9.944\n",
      "Model saved!\n",
      "Epoch: 14 time 0:01:26.641289 train SE 10.013 test SE 10.010\n",
      "Model saved!\n",
      "Epoch: 15 time 0:01:27.162797 train SE 10.086 test SE 10.111\n",
      "Model saved!\n",
      "Epoch: 16 time 0:01:29.238781 train SE 10.158 test SE 10.201\n",
      "Model saved!\n",
      "Epoch: 17 time 0:01:29.166653 train SE 10.232 test SE 10.256\n",
      "Model saved!\n",
      "Epoch: 18 time 0:01:26.981805 train SE 10.294 test SE 10.286\n",
      "Model saved!\n",
      "Epoch: 19 time 0:01:27.672939 train SE 10.338 test SE 10.368\n",
      "Model saved!\n",
      "Epoch: 20 time 0:01:26.540338 train SE 10.401 test SE 10.393\n",
      "Model saved!\n",
      "Epoch: 21 time 0:01:28.811858 train SE 10.435 test SE 10.436\n",
      "Model saved!\n",
      "Epoch: 22 time 0:01:26.804727 train SE 10.484 test SE 10.499\n",
      "Model saved!\n",
      "Epoch: 23 time 0:01:26.615751 train SE 10.526 test SE 10.499\n",
      "Epoch: 24 time 0:01:26.680478 train SE 10.548 test SE 10.570\n",
      "Model saved!\n",
      "Epoch: 25 time 0:01:26.756394 train SE 10.606 test SE 10.597\n",
      "Model saved!\n",
      "Epoch: 26 time 0:01:27.164319 train SE 10.631 test SE 10.629\n",
      "Model saved!\n",
      "Epoch: 27 time 0:01:27.489006 train SE 10.660 test SE 10.616\n",
      "Epoch: 28 time 0:01:26.605243 train SE 10.669 test SE 10.709\n",
      "Model saved!\n",
      "Epoch: 29 time 0:01:26.874074 train SE 10.751 test SE 10.717\n",
      "Model saved!\n",
      "Epoch: 30 time 0:01:26.637738 train SE 10.780 test SE 10.762\n",
      "Model saved!\n",
      "Epoch: 31 time 0:01:26.958772 train SE 10.809 test SE 10.740\n",
      "Epoch: 32 time 0:01:25.973551 train SE 10.828 test SE 10.789\n",
      "Model saved!\n",
      "Epoch: 33 time 0:01:26.721903 train SE 10.839 test SE 10.795\n",
      "Model saved!\n",
      "Epoch: 34 time 0:01:26.989237 train SE 10.882 test SE 10.875\n",
      "Model saved!\n",
      "Epoch: 35 time 0:01:26.791367 train SE 10.928 test SE 10.867\n",
      "Epoch: 36 time 0:01:27.657541 train SE 10.921 test SE 10.898\n",
      "Model saved!\n",
      "Epoch: 37 time 0:01:26.866122 train SE 10.987 test SE 10.938\n",
      "Model saved!\n",
      "Epoch: 38 time 0:01:26.842221 train SE 11.006 test SE 10.964\n",
      "Model saved!\n",
      "Epoch: 39 time 0:01:30.054081 train SE 11.054 test SE 11.005\n",
      "Model saved!\n",
      "Epoch: 40 time 0:01:26.751379 train SE 11.091 test SE 11.067\n",
      "Model saved!\n",
      "Epoch: 41 time 0:01:29.768805 train SE 11.115 test SE 11.059\n",
      "Epoch: 42 time 0:01:26.737339 train SE 11.132 test SE 11.068\n",
      "Model saved!\n",
      "Epoch: 43 time 0:01:26.674083 train SE 11.123 test SE 11.073\n",
      "Model saved!\n",
      "Epoch: 44 time 0:01:26.514148 train SE 11.151 test SE 11.081\n",
      "Model saved!\n",
      "Epoch: 45 time 0:01:26.841710 train SE 11.176 test SE 11.137\n",
      "Model saved!\n",
      "Epoch: 46 time 0:01:27.225673 train SE 11.209 test SE 11.131\n",
      "Epoch: 47 time 0:01:26.823742 train SE 11.210 test SE 11.160\n",
      "Model saved!\n",
      "Epoch: 48 time 0:01:29.597832 train SE 11.237 test SE 11.156\n",
      "Epoch: 49 time 0:01:27.169854 train SE 11.255 test SE 11.206\n",
      "Model saved!\n",
      "Epoch: 50 time 0:01:26.267648 train SE 11.283 test SE 11.212\n",
      "Model saved!\n",
      "Epoch: 51 time 0:01:24.609194 train SE 11.289 test SE 11.189\n",
      "Epoch: 52 time 0:01:25.842464 train SE 11.290 test SE 11.216\n",
      "Model saved!\n",
      "Epoch: 53 time 0:01:27.272034 train SE 11.327 test SE 11.194\n",
      "Epoch: 54 time 0:01:26.564866 train SE 11.335 test SE 11.235\n",
      "Model saved!\n",
      "Epoch: 55 time 0:01:27.392661 train SE 11.353 test SE 11.269\n",
      "Model saved!\n",
      "Epoch: 56 time 0:01:26.788395 train SE 11.368 test SE 11.278\n",
      "Model saved!\n",
      "Epoch: 57 time 0:01:27.247061 train SE 11.382 test SE 11.294\n",
      "Model saved!\n",
      "Epoch: 58 time 0:01:26.893068 train SE 11.400 test SE 11.307\n",
      "Model saved!\n",
      "Epoch: 59 time 0:01:27.812647 train SE 11.413 test SE 11.313\n",
      "Model saved!\n",
      "Epoch: 60 time 0:01:26.594774 train SE 11.413 test SE 11.304\n",
      "Epoch: 61 time 0:01:30.001101 train SE 11.408 test SE 11.297\n",
      "Epoch: 62 time 0:01:26.708042 train SE 11.439 test SE 11.352\n",
      "Model saved!\n",
      "Epoch: 63 time 0:01:26.547031 train SE 11.458 test SE 11.345\n",
      "Epoch: 64 time 0:01:26.668160 train SE 11.457 test SE 11.356\n",
      "Model saved!\n",
      "Epoch: 65 time 0:01:27.588159 train SE 11.470 test SE 11.361\n",
      "Model saved!\n",
      "Epoch: 66 time 0:01:26.672198 train SE 11.481 test SE 11.365\n",
      "Model saved!\n",
      "Epoch: 67 time 0:01:26.634338 train SE 11.493 test SE 11.389\n",
      "Model saved!\n",
      "Epoch: 68 time 0:01:30.150932 train SE 11.495 test SE 11.366\n",
      "Epoch: 69 time 0:01:27.085915 train SE 11.496 test SE 11.389\n",
      "Epoch: 70 time 0:01:26.587484 train SE 11.500 test SE 11.371\n",
      "Epoch: 71 time 0:01:26.869168 train SE 11.515 test SE 11.408\n",
      "Model saved!\n",
      "Epoch: 72 time 0:01:25.975938 train SE 11.531 test SE 11.411\n",
      "Model saved!\n",
      "Epoch: 73 time 0:01:28.931152 train SE 11.535 test SE 11.408\n",
      "Epoch: 74 time 0:01:26.499228 train SE 11.543 test SE 11.422\n",
      "Model saved!\n",
      "Epoch: 75 time 0:01:26.037876 train SE 11.554 test SE 11.431\n",
      "Model saved!\n",
      "Epoch: 76 time 0:01:26.484704 train SE 11.563 test SE 11.421\n",
      "Epoch: 77 time 0:01:26.740928 train SE 11.563 test SE 11.421\n",
      "Epoch: 78 time 0:01:25.951298 train SE 11.563 test SE 11.425\n",
      "Epoch: 79 time 0:01:25.358766 train SE 11.566 test SE 11.421\n",
      "Epoch: 80 time 0:01:23.229377 train SE 11.577 test SE 11.436\n",
      "Model saved!\n",
      "Epoch: 81 time 0:01:23.124299 train SE 11.580 test SE 11.433\n",
      "Epoch: 82 time 0:01:23.422834 train SE 11.592 test SE 11.440\n",
      "Model saved!\n",
      "Epoch: 83 time 0:01:24.394211 train SE 11.595 test SE 11.442\n",
      "Model saved!\n",
      "Epoch: 84 time 0:01:25.185643 train SE 11.588 test SE 11.447\n",
      "Model saved!\n",
      "Epoch: 85 time 0:01:26.627807 train SE 11.605 test SE 11.463\n",
      "Model saved!\n",
      "Epoch: 86 time 0:01:26.484073 train SE 11.610 test SE 11.452\n",
      "Epoch: 87 time 0:01:27.277465 train SE 11.613 test SE 11.450\n",
      "Epoch: 88 time 0:01:25.866866 train SE 11.627 test SE 11.459\n",
      "Epoch: 89 time 0:01:25.882652 train SE 11.616 test SE 11.469\n",
      "Model saved!\n",
      "Epoch: 90 time 0:01:25.208539 train SE 11.631 test SE 11.469\n",
      "Epoch: 91 time 0:01:25.583978 train SE 11.629 test SE 11.478\n",
      "Model saved!\n",
      "Epoch: 92 time 0:01:26.407883 train SE 11.651 test SE 11.491\n",
      "Model saved!\n",
      "Epoch: 93 time 0:01:26.532384 train SE 11.642 test SE 11.468\n",
      "Epoch: 94 time 0:01:32.125075 train SE 11.656 test SE 11.480\n",
      "Epoch: 95 time 0:01:26.856304 train SE 11.655 test SE 11.489\n",
      "Epoch: 96 time 0:01:25.241581 train SE 11.654 test SE 11.489\n",
      "Epoch: 97 time 0:01:25.568584 train SE 11.663 test SE 11.504\n",
      "Model saved!\n",
      "Epoch: 98 time 0:01:32.481238 train SE 11.666 test SE 11.500\n",
      "Epoch: 99 time 0:01:26.829767 train SE 11.669 test SE 11.483\n",
      "Epoch: 100 time 0:01:25.677267 train SE 11.728 test SE 11.557\n",
      "Model saved!\n",
      "Epoch: 101 time 0:01:26.204558 train SE 11.743 test SE 11.558\n",
      "Model saved!\n",
      "Epoch: 102 time 0:01:26.311088 train SE 11.753 test SE 11.557\n",
      "Epoch: 103 time 0:01:25.453729 train SE 11.757 test SE 11.550\n",
      "Epoch: 104 time 0:01:25.963455 train SE 11.760 test SE 11.550\n",
      "Epoch: 105 time 0:01:28.875746 train SE 11.765 test SE 11.568\n",
      "Model saved!\n",
      "Epoch: 106 time 0:01:31.235462 train SE 11.773 test SE 11.575\n",
      "Model saved!\n",
      "Epoch: 107 time 0:01:25.541575 train SE 11.772 test SE 11.573\n",
      "Epoch: 108 time 0:01:25.775549 train SE 11.780 test SE 11.562\n",
      "Epoch: 109 time 0:01:27.972759 train SE 11.772 test SE 11.557\n",
      "Epoch: 110 time 0:01:27.004267 train SE 11.778 test SE 11.572\n",
      "Epoch: 111 time 0:01:27.009727 train SE 11.778 test SE 11.568\n",
      "Epoch: 112 time 0:01:26.508152 train SE 11.781 test SE 11.579\n",
      "Model saved!\n",
      "Epoch: 113 time 0:01:27.442112 train SE 11.790 test SE 11.577\n",
      "Epoch: 114 time 0:01:26.191905 train SE 11.788 test SE 11.570\n",
      "Epoch: 115 time 0:01:26.871133 train SE 11.797 test SE 11.578\n",
      "Epoch: 116 time 0:01:28.079405 train SE 11.793 test SE 11.572\n",
      "Epoch: 117 time 0:01:27.417316 train SE 11.799 test SE 11.578\n",
      "Epoch: 118 time 0:01:27.031272 train SE 11.804 test SE 11.578\n",
      "Epoch: 119 time 0:01:22.604098 train SE 11.804 test SE 11.578\n",
      "Epoch: 120 time 0:01:27.649068 train SE 11.807 test SE 11.576\n",
      "Epoch: 121 time 0:01:25.530711 train SE 11.804 test SE 11.575\n",
      "Epoch: 122 time 0:01:25.271740 train SE 11.805 test SE 11.581\n",
      "Model saved!\n",
      "Epoch: 123 time 0:01:26.594230 train SE 11.808 test SE 11.577\n",
      "Epoch: 124 time 0:01:26.635753 train SE 11.814 test SE 11.583\n",
      "Model saved!\n",
      "Epoch: 125 time 0:01:26.880521 train SE 11.806 test SE 11.569\n",
      "Epoch: 126 time 0:01:25.123741 train SE 11.814 test SE 11.585\n",
      "Model saved!\n",
      "Epoch: 127 time 0:01:27.105875 train SE 11.820 test SE 11.592\n",
      "Model saved!\n",
      "Epoch: 128 time 0:01:27.035726 train SE 11.828 test SE 11.590\n",
      "Epoch: 129 time 0:01:26.383385 train SE 11.819 test SE 11.590\n",
      "Epoch: 130 time 0:01:27.015773 train SE 11.821 test SE 11.578\n",
      "Epoch: 131 time 0:01:26.213840 train SE 11.825 test SE 11.587\n",
      "Epoch: 132 time 0:01:26.948371 train SE 11.831 test SE 11.587\n",
      "Epoch: 133 time 0:01:26.573876 train SE 11.822 test SE 11.589\n",
      "Epoch: 134 time 0:01:26.693556 train SE 11.830 test SE 11.589\n",
      "Epoch: 135 time 0:01:25.910246 train SE 11.834 test SE 11.592\n",
      "Epoch: 136 time 0:01:26.950375 train SE 11.839 test SE 11.586\n",
      "Epoch: 137 time 0:01:26.129119 train SE 11.835 test SE 11.600\n",
      "Model saved!\n",
      "Epoch: 138 time 0:01:27.060576 train SE 11.839 test SE 11.581\n",
      "Epoch: 139 time 0:01:27.311906 train SE 11.839 test SE 11.599\n",
      "Epoch: 140 time 0:01:27.127013 train SE 11.843 test SE 11.603\n",
      "Model saved!\n",
      "Epoch: 141 time 0:01:26.697064 train SE 11.837 test SE 11.598\n",
      "Epoch: 142 time 0:01:27.121914 train SE 11.843 test SE 11.602\n",
      "Epoch: 143 time 0:01:26.790569 train SE 11.848 test SE 11.608\n",
      "Model saved!\n",
      "Epoch: 144 time 0:01:27.362381 train SE 11.851 test SE 11.602\n",
      "Epoch: 145 time 0:01:26.881606 train SE 11.845 test SE 11.586\n",
      "Epoch: 146 time 0:01:26.019421 train SE 11.842 test SE 11.590\n",
      "Epoch: 147 time 0:01:26.493592 train SE 11.854 test SE 11.578\n",
      "Epoch: 148 time 0:01:26.456207 train SE 11.843 test SE 11.594\n",
      "Epoch: 149 time 0:01:26.103164 train SE 11.851 test SE 11.603\n",
      "Epoch: 150 time 0:01:26.931513 train SE 11.885 test SE 11.622\n",
      "Model saved!\n",
      "Epoch: 151 time 0:01:25.847845 train SE 11.892 test SE 11.624\n",
      "Model saved!\n",
      "Epoch: 152 time 0:01:26.451134 train SE 11.899 test SE 11.620\n",
      "Epoch: 153 time 0:01:27.062090 train SE 11.898 test SE 11.632\n",
      "Model saved!\n",
      "Epoch: 154 time 0:01:26.737476 train SE 11.903 test SE 11.634\n",
      "Model saved!\n",
      "Epoch: 155 time 0:01:26.771228 train SE 11.900 test SE 11.620\n",
      "Epoch: 156 time 0:01:24.686533 train SE 11.909 test SE 11.623\n",
      "Epoch: 157 time 0:01:25.325808 train SE 11.905 test SE 11.627\n",
      "Epoch: 158 time 0:01:26.040820 train SE 11.912 test SE 11.628\n",
      "Epoch: 159 time 0:01:25.157271 train SE 11.909 test SE 11.613\n",
      "Epoch: 160 time 0:01:26.777917 train SE 11.908 test SE 11.634\n",
      "Epoch: 161 time 0:01:25.772114 train SE 11.919 test SE 11.630\n",
      "Epoch: 162 time 0:01:25.289047 train SE 11.916 test SE 11.634\n",
      "Model saved!\n",
      "Epoch: 163 time 0:01:23.154154 train SE 11.908 test SE 11.632\n",
      "Epoch: 164 time 0:01:24.408899 train SE 11.917 test SE 11.637\n",
      "Model saved!\n",
      "Epoch: 165 time 0:01:25.958196 train SE 11.910 test SE 11.622\n",
      "Epoch: 166 time 0:01:25.934484 train SE 11.925 test SE 11.638\n",
      "Model saved!\n",
      "Epoch: 167 time 0:01:22.447947 train SE 11.917 test SE 11.626\n",
      "Epoch: 168 time 0:01:26.730059 train SE 11.917 test SE 11.631\n",
      "Epoch: 169 time 0:01:26.870131 train SE 11.921 test SE 11.621\n",
      "Epoch: 170 time 0:01:26.800331 train SE 11.925 test SE 11.627\n",
      "Epoch: 171 time 0:01:25.417606 train SE 11.922 test SE 11.627\n",
      "Epoch: 172 time 0:01:25.688778 train SE 11.929 test SE 11.637\n",
      "Epoch: 173 time 0:01:26.406927 train SE 11.919 test SE 11.621\n",
      "Epoch: 174 time 0:01:26.247805 train SE 11.925 test SE 11.628\n",
      "Epoch: 175 time 0:01:27.417067 train SE 11.927 test SE 11.639\n",
      "Model saved!\n",
      "Epoch: 176 time 0:01:25.161697 train SE 11.921 test SE 11.622\n",
      "Epoch: 177 time 0:01:26.290099 train SE 11.927 test SE 11.638\n",
      "Epoch: 178 time 0:01:27.181245 train SE 11.930 test SE 11.625\n",
      "Epoch: 179 time 0:01:26.932972 train SE 11.929 test SE 11.621\n",
      "The best SE is: 11.639\n",
      "[ 5.98462677  6.07738066  6.21795607  6.78468084  7.40146494  8.10541821\n",
      "  8.47124004  8.80146408  9.08354187  9.32668495  9.52358341  9.70938301\n",
      "  9.84375     9.94394207 10.01025009 10.11119556 10.20096493 10.25627041\n",
      " 10.28584957 10.3678627  10.39292812 10.43600368 10.49872684 10.49868202\n",
      " 10.5704565  10.59667015 10.62881088 10.61550903 10.70933723 10.71739864\n",
      " 10.7624855  10.74004269 10.78879547 10.79532909 10.87456036 10.8673172\n",
      " 10.89827919 10.93787003 10.96354675 11.00486279 11.06716728 11.05876446\n",
      " 11.06836796 11.07292843 11.08063793 11.1371603  11.13122082 11.15997601\n",
      " 11.15621471 11.2056284  11.21190262 11.18933201 11.2159977  11.1944437\n",
      " 11.2354126  11.26930904 11.27834129 11.29432297 11.30734158 11.3125639\n",
      " 11.30401039 11.29749584 11.35224056 11.34524059 11.3564291  11.3614378\n",
      " 11.36470509 11.38898754 11.36574268 11.38882065 11.37125492 11.40793419\n",
      " 11.41106892 11.40774632 11.42208481 11.4308033  11.42106915 11.42123699\n",
      " 11.42470455 11.4208889  11.43586063 11.43336582 11.44010925 11.44150448\n",
      " 11.44715214 11.46326828 11.45240688 11.44964027 11.45867825 11.46899128\n",
      " 11.46860409 11.47771454 11.49083424 11.46803093 11.48043919 11.48939037\n",
      " 11.48914146 11.50376511 11.49993324 11.48257732 11.55736065 11.55841827\n",
      " 11.55669117 11.55002117 11.54977608 11.56808281 11.57493496 11.57254601\n",
      " 11.56225109 11.55749702 11.57157135 11.56800938 11.57903576 11.57674026\n",
      " 11.56996155 11.57794476 11.57189369 11.57789516 11.57788849 11.57829762\n",
      " 11.57593536 11.57510281 11.58054924 11.57741642 11.58338451 11.5691061\n",
      " 11.58547974 11.59177971 11.58971214 11.59024811 11.57803059 11.58713436\n",
      " 11.58714294 11.58884048 11.58939838 11.59163189 11.58640575 11.59962559\n",
      " 11.58098888 11.59939671 11.60288715 11.5979414  11.60177326 11.60805702\n",
      " 11.60207081 11.58607101 11.58979702 11.57837105 11.59386158 11.60290432\n",
      " 11.62220573 11.62412739 11.62038708 11.63238621 11.63362694 11.62037468\n",
      " 11.62278461 11.62702847 11.62822819 11.61288929 11.63352108 11.63000679\n",
      " 11.63417721 11.63152409 11.63739777 11.62209606 11.63805485 11.62567234\n",
      " 11.6307478  11.62052631 11.62694073 11.62730694 11.63746548 11.62140369\n",
      " 11.62761593 11.63945484 11.62191296 11.63775539 11.62543011 11.62121105]\n"
     ]
    }
   ],
   "source": [
    "B = 64\n",
    "train(Nc,N,Nt,B,Nr,L,SNR_dB,K,EPOCH,BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Self_installed_software\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:649: SourceChangeWarning: source code of class 'torch.nn.modules.linear.Linear' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "D:\\Self_installed_software\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:649: SourceChangeWarning: source code of class 'torch.nn.modules.container.Sequential' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "D:\\Self_installed_software\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:649: SourceChangeWarning: source code of class 'torch.nn.modules.conv.Conv2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "D:\\Self_installed_software\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:649: SourceChangeWarning: source code of class 'torch.nn.modules.batchnorm.BatchNorm2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "D:\\Self_installed_software\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:649: SourceChangeWarning: source code of class 'torch.nn.modules.batchnorm.BatchNorm1d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "D:\\Self_installed_software\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:649: SourceChangeWarning: source code of class 'torch.nn.modules.activation.ReLU' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test SE 4.677\n",
      "test SE 6.060\n",
      "test SE 7.230\n",
      "test SE 9.961\n",
      "test SE 10.745\n",
      "test SE 11.334\n",
      "test SE 11.521\n",
      "test SE 11.304\n",
      "test SE 11.645\n"
     ]
    }
   ],
   "source": [
    "for B in [1,3,5,8,16,24,32,48,64]:\n",
    "    test(Nc,N,N,Nt,B,Nr,L,SNR_dB,K,BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 504x468 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "B_set = [1,3,5,8,16,24,32,48,64]\n",
    "SE_list_DNN = [ 4.677,  6.060,  7.230,  9.961, 10.745,11.334 , 11.521 , 11.304, 11.645]\n",
    "\n",
    "SE_list_percsi_MO = np.ones(8)*14.19122653\n",
    "B_set2 = [0,10,20,30,40,50,60,70]\n",
    "SE_OMP_set_MO = np.ones(8)*8.55466791\n",
    "B_set3 = [20,30,40,50,60,70]\n",
    "SE_OMP_QUAN_set_MO = [3.15252835,5.79277314,7.66800098,8.04520523,8.25775114,8.40329873]\n",
    "\n",
    "SE_list_percsi_PCA = np.ones(8)*14.04247462 \n",
    "SE_OMP_set_PCA = np.ones(8)*8.4357455 \n",
    "SE_OMP_QUAN_set_PCA = [3.17091409,5.82117721,7.69306492,8.05957222,8.29079286,8.42041856]\n",
    "# %matplotlib notebook\n",
    "\n",
    "plt.figure(figsize=(7, 6.5))\n",
    "ax = plt.subplot(111)\n",
    "# 设置刻度字体大小\n",
    "fs = 11\n",
    "plt.xticks(fontsize=fs)\n",
    "plt.yticks(fontsize=fs)\n",
    "# 设置坐标标签字体大小\n",
    "ax.set_xlabel(..., fontsize=fs+2)\n",
    "ax.set_ylabel(..., fontsize=fs+2)\n",
    "\n",
    "plt.plot(B_set,SE_list_DNN,label = \"Proposed scheme\",marker='v',color = 'blue',linewidth='1.5')\n",
    "plt.plot(B_set2,SE_list_percsi_MO,label = \"MO-HB (perfect CSI at the BS)\",color = 'red',linewidth='1.5')\n",
    "plt.plot(B_set2,SE_list_percsi_PCA,label = \"PCA-HB (perfect CSI at the BS)\",color = 'purple',linewidth='1.5')\n",
    "plt.plot(B_set2,SE_OMP_set_MO,label = \"MO-HB (estimated CSI, perfect CSI feedback)\",color = 'brown',linewidth='1.5')\n",
    "plt.plot(B_set3,SE_OMP_QUAN_set_MO,label = \"MO-HB (estimated CSI, limited feedback)\",marker='v',color = 'brown',linewidth='1.5',linestyle = \"--\")\n",
    "\n",
    "\n",
    "plt.plot(B_set2,SE_OMP_set_PCA,label = \"PCA-HB (estimated CSI, perfect CSI feedback)\",color = 'green',linewidth='1.5')\n",
    "plt.plot(B_set3,SE_OMP_QUAN_set_PCA,label = \"PCA-HB (estimated CSI, limited feedback)\",marker='v',color = 'green',linewidth='1.5',linestyle = \"--\")\n",
    "\n",
    "plt.grid()\n",
    "# plt.title('K=2 Lp=2 L=8 UPA8*8')\n",
    "plt.xlabel('Feedback Overhead $B$ (bits)') \n",
    "plt.ylabel('Sum Rate (bps/Hz)')\n",
    "plt.ylim((-5,15))\n",
    "plt.xlim((0,70))\n",
    "plt.yticks([0, 5, 10,15],['0', '5', '10','15'])\n",
    "plt.legend(loc=\"best\")\n",
    "plt.legend(fontsize=fs)\n",
    "#plt.set_yticks = [0, 5, 10,15]\n",
    "#plt.set_yticklabels = ['0', '5', '10','15']\n",
    "plt.savefig('fig11_a.pdf')\n",
    "plt.savefig('fig11_a.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
